{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3544acfe",
   "metadata": {},
   "source": [
    "# Benchmark Metric Table\n",
    "- Default Reward Settings, across different Algorithm (3 dataset): 3 Tables\n",
    "- Different Reward Settings (3 dataset * 7 algo): 21 Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30340ff2",
   "metadata": {},
   "source": [
    "# Benchmark Metric Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"models/training_log/with_action_penalty_with_action_stable_penalty_weaning_reward_10_lr_1e-5\"\n",
    "import os\n",
    "import sys\n",
    "os.listdir(\"../models/training_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53645e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "preprocess_prefix = \"preprocess_aggregate_update_all_use_unseen_\"\n",
    "\n",
    "# Define experiment setup\n",
    "EXP_FOLDER_PREFIX_list = [\n",
    "    f'{preprocess_prefix}without_action_penalty_with_action_stable_penalty_weaning_reward_10_lr_1e-5',\n",
    "    f'{preprocess_prefix}without_action_penalty_without_action_stable_penalty_weaning_reward_10_lr_1e-5',\n",
    "    f'{preprocess_prefix}without_internal_reward_weaning_reward_10_lr_1e-5',\n",
    "    f'{preprocess_prefix}without_action_penalty_with_action_stable_penalty_weaning_reward_100_lr_1e-5',\n",
    "    f'{preprocess_prefix}with_internal_reward_without_weaning_reward_lr_1e-5',\n",
    "    # f'{preprocess_prefix}without_action_penalty_with_action_stable_penalty_weaning_reward_10_lr_1e-6',\n",
    "    # f'{preprocess_prefix}BC_1e-6_other_1e-5'\n",
    "]\n",
    "\n",
    "FOLDER_NAME_MAPPING = {\n",
    "    f'{preprocess_prefix}without_action_penalty_with_action_stable_penalty_weaning_reward_10_lr_1e-5': 'default',\n",
    "    f'{preprocess_prefix}without_action_penalty_without_action_stable_penalty_weaning_reward_10_lr_1e-5': 'no_action_stable_penalty',\n",
    "    f'{preprocess_prefix}without_internal_reward_weaning_reward_10_lr_1e-5': 'no_internal_reward',\n",
    "    f'{preprocess_prefix}without_action_penalty_with_action_stable_penalty_weaning_reward_100_lr_1e-5': 'large_extub_reward',\n",
    "    f'{preprocess_prefix}with_internal_reward_without_weaning_reward_lr_1e-5': 'no_extub_reward',\n",
    "    f'{preprocess_prefix}without_action_penalty_with_action_stable_penalty_weaning_reward_10_lr_1e-6': 'small_lr', \n",
    "    f'{preprocess_prefix}BC_1e-6_other_1e-5': 'BC_1e-6_other_1e-5'\n",
    "}\n",
    "\n",
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "reward_types = [\"small_reward\", \"median_reward\", \"large_reward\"]\n",
    "\n",
    "# Add mapping for reward types to severity labels for display\n",
    "REWARD_TYPE_DISPLAY_MAPPING = {\n",
    "    \"small_reward\": \"high_severity\",\n",
    "    \"median_reward\": \"median_severity\", \n",
    "    \"large_reward\": \"low_severity\"\n",
    "}\n",
    "\n",
    "# Metrics to extract\n",
    "# metric_names = [\n",
    "#     (\"mean_reward\", \"Total Cumulative Reward\"),\n",
    "#     (\"meet_weaning_percentage\", \"Extubation Success Rate (\\%)\"),\n",
    "#     (\"mean_traj_len\", \"Avg. Episode Length (hrs)\"),\n",
    "#     (\"mean_traj_len_to_meet_weaning\", \"Avg. Time to Meet (hrs)\"),\n",
    "#     (\"action_diversity\", \"Action Diversity\"),\n",
    "#     (\"mean_nonsense_actions\", \"Anomalous Actions (\\%)\")\n",
    "# ]\n",
    "\n",
    "metric_names = [\n",
    "    (\"mean_reward\", \"Total Cumulative Reward\"),\n",
    "    (\"meet_weaning_percentage\", \"Extubation Meet Rate (\\%)\"),\n",
    "    (\"mean_traj_len\", \"Avg. Trajectory Length (hrs)\"),\n",
    "    (\"mean_traj_len_to_meet_weaning\", \"Avg. Time to Meet (hrs)\"),\n",
    "    (\"action_diversity\", \"Action Diversity\"),\n",
    "    (\"mean_nonsense_actions\", \"Anomalous Actions (\\%)\")\n",
    "]\n",
    "\n",
    "def extract_best_epoch(df, reward_category):\n",
    "    subset = df[df['category'] == reward_category]\n",
    "    if subset.empty:\n",
    "        return None\n",
    "    return subset.loc[subset['mean_reward'].idxmax()]\n",
    "\n",
    "def build_table_for_algo(EXP_FOLDER, data_source):\n",
    "    result = {rtype: defaultdict(dict) for rtype in reward_types}\n",
    "    for algo in algo_list:\n",
    "        csv_path = f'../models/training_log/{EXP_FOLDER}/rewards_summary_{algo}_{data_source}.csv'\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        for rtype in reward_types:\n",
    "            best = extract_best_epoch(df, rtype)\n",
    "            for key, label in metric_names:\n",
    "                if best is not None:\n",
    "                    val = best[key]\n",
    "                    val = round(val * 100, 2) if \"%\" in label else round(val, 2)\n",
    "                else:\n",
    "                    val = \"N/A\"\n",
    "                result[rtype][label][algo] = val\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_table_for_algo_with_physician_policy(EXP_FOLDER, data_source):\n",
    "    result = {rtype: defaultdict(dict) for rtype in reward_types}\n",
    "\n",
    "    # Include RL algorithms\n",
    "    for algo in algo_list + ['naive_agent']:\n",
    "        if algo == 'naive_agent':\n",
    "            csv_path = f'../models/training_log/naive_agent/rewards_summary_naive_agent_{data_source}.csv'\n",
    "        else:\n",
    "            csv_path = f'../models/training_log/{EXP_FOLDER}/rewards_summary_{algo}_{data_source}.csv'\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        for rtype in reward_types:\n",
    "            if algo == 'naive_agent':\n",
    "                best = df[df['category'] == rtype]\n",
    "                best = best.iloc[0] if not best.empty else None\n",
    "            else:\n",
    "                best = extract_best_epoch(df, rtype)\n",
    "\n",
    "            for key, label in metric_names:\n",
    "                if best is not None:\n",
    "                    val = best[key]\n",
    "                    val = round(val * 100, 2) if \"%\" in label else round(val, 2)\n",
    "                else:\n",
    "                    val = \"N/A\"\n",
    "                result[rtype][label][algo] = val\n",
    "\n",
    "    return result\n",
    "\n",
    "def build_table_for_reward_design(algo, data_source):\n",
    "    result = {rtype: defaultdict(dict) for rtype in reward_types}\n",
    "    for EXP_FOLDER in EXP_FOLDER_PREFIX_list:\n",
    "        csv_path = f'../models/training_log/{EXP_FOLDER}/rewards_summary_{algo}_{data_source}.csv'\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        for rtype in reward_types:\n",
    "            best = extract_best_epoch(df, rtype)\n",
    "            for key, label in metric_names:\n",
    "                if best is not None:\n",
    "                    val = best[key]\n",
    "                    val = round(val * 100, 2) if \"%\" in label else round(val, 2)\n",
    "                else:\n",
    "                    val = \"N/A\"\n",
    "                result[rtype][label][EXP_FOLDER] = val\n",
    "    return result\n",
    "\n",
    "def escape_latex(text):\n",
    "    \"\"\"\n",
    "    Escape underscores for LaTeX compatibility.\n",
    "    \"\"\"\n",
    "    return text.replace('_', '\\\\_')\n",
    "\n",
    "def print_table_latex(table_dict, columns, caption_suffix=\"\"):\n",
    "    for rtype in reward_types:\n",
    "        severity_display = REWARD_TYPE_DISPLAY_MAPPING[rtype].replace('_', ' ').title()\n",
    "        \n",
    "        print(\"\\\\begin{table}[htbp]\")\n",
    "        print(\"\\\\centering\")\n",
    "        print(f\"\\\\caption{{Performance Metrics ({severity_display.replace('_', ' ').title()}) {caption_suffix}}}\")\n",
    "        print(\"\\\\small\")\n",
    "        print(\"\\\\resizebox{\\\\textwidth}{!}{%\")  # Start resizebox\n",
    "        print(\"\\\\begin{tabular}{\" + \"l\" + \"c\" * len(columns) + \"}\")\n",
    "        print(\"\\\\toprule\")\n",
    "\n",
    "        # Header row\n",
    "        header_cells = [\"\\\\textbf{Metric}\"]\n",
    "        for col in columns:\n",
    "            if col == \"naive_agent\":\n",
    "                escaped_label = \"Physician Policy\"\n",
    "            else:\n",
    "                short_label = FOLDER_NAME_MAPPING.get(col, col)\n",
    "                escaped_label = escape_latex(short_label)\n",
    "            header_cells.append(f\"\\\\texttt{{{escaped_label}}}\")\n",
    "        print(\" & \".join(header_cells) + \" \\\\\\\\\")\n",
    "\n",
    "        print(\"\\\\midrule\")\n",
    "\n",
    "        # Data rows\n",
    "        for key, label in metric_names:\n",
    "            row = [label]\n",
    "            for col in columns:\n",
    "                val = table_dict[rtype][label].get(col, \"N/A\")\n",
    "                row.append(str(val))\n",
    "            print(\" & \".join(row) + \" \\\\\\\\\")\n",
    "\n",
    "        print(\"\\\\bottomrule\")\n",
    "        print(\"\\\\end{tabular}\")\n",
    "        print(\"}%\")  # End resizebox\n",
    "        safe_label = caption_suffix.replace(' ', '_').replace('(', '').replace(')', '').replace('%', '')\n",
    "        print(f\"\\\\label{{tab:rl_metrics_{rtype}_{safe_label}}}\")\n",
    "        print(\"\\\\end{table}\")\n",
    "        print()\n",
    "\n",
    "# def print_table_latex(table_dict, columns, caption_suffix=\"\"):\n",
    "#     for rtype in reward_types:\n",
    "#         # Convert reward type to severity display name\n",
    "#         severity_display = REWARD_TYPE_DISPLAY_MAPPING[rtype].replace('_', ' ').title()\n",
    "        \n",
    "#         print(\"\\\\begin{table}[htbp]\")\n",
    "#         print(\"\\\\centering\")\n",
    "#         print(f\"\\\\caption{{Performance Metrics ({severity_display}) {caption_suffix}}}\")\n",
    "#         print(\"\\\\small\")\n",
    "#         print(\"\\\\resizebox{\\\\textwidth}{!}{%\")  # Start resizebox\n",
    "#         print(\"\\\\begin{tabular}{\" + \"l\" + \"c\" * len(columns) + \"}\")\n",
    "#         print(\"\\\\toprule\")\n",
    "\n",
    "#         # Header row\n",
    "#         header_cells = [\"\\\\textbf{Metric}\"]\n",
    "#         for col in columns:\n",
    "#             short_label = FOLDER_NAME_MAPPING.get(col, col)\n",
    "#             escaped_label = escape_latex(short_label)\n",
    "#             header_cells.append(f\"\\\\texttt{{{escaped_label}}}\")\n",
    "#         print(\" & \".join(header_cells) + \" \\\\\\\\\")\n",
    "\n",
    "#         print(\"\\\\midrule\")\n",
    "\n",
    "#         # Data rows\n",
    "#         for key, label in metric_names:\n",
    "#             row = [label]\n",
    "#             for col in columns:\n",
    "#                 val = table_dict[rtype][label].get(col, \"N/A\")\n",
    "#                 row.append(str(val))\n",
    "#             print(\" & \".join(row) + \" \\\\\\\\\")\n",
    "\n",
    "#         print(\"\\\\bottomrule\")\n",
    "#         print(\"\\\\end{tabular}\")\n",
    "#         print(\"}%\")  # End resizebox\n",
    "        \n",
    "#         # Use severity name for label\n",
    "#         severity_label = REWARD_TYPE_DISPLAY_MAPPING[rtype]\n",
    "#         safe_label = caption_suffix.replace(' ', '_').replace('(', '').replace(')', '').replace('%', '')\n",
    "#         print(f\"\\\\label{{tab:rl_metrics_{severity_label}_{safe_label}}}\")\n",
    "#         print(\"\\\\end{table}\")\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Default Reward Settings with Different Algo\n",
    "# 1. Compare algorithms within a reward setting\n",
    "exp_setting = EXP_FOLDER_PREFIX_list[0]  # \"default\" setting\n",
    "data_source = \"eICU\"\n",
    "table_dict = build_table_for_algo(exp_setting, data_source)\n",
    "print_table_latex(table_dict, algo_list, caption_suffix=f\"for {data_source}\")\n",
    "# print_table_latex(table_dict, algo_list, caption_suffix=f\"for {data_source} ({exp_setting})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8aae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compare algorithms within a reward setting\n",
    "exp_setting = EXP_FOLDER_PREFIX_list[0]  # \"default\" setting\n",
    "for data_source in data_source_list:\n",
    "    table_dict = build_table_for_algo(exp_setting, data_source)\n",
    "    print_table_latex(table_dict, algo_list, caption_suffix=f\"for {data_source}\")\n",
    "    # print_table_latex(table_dict, algo_list, caption_suffix=f\"for {data_source} ({exp_setting})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289c29b",
   "metadata": {},
   "source": [
    "With Pyhsician Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ef590",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_list_with_physician_policy = ['naive_agent'] +  algo_list\n",
    "exp_setting = EXP_FOLDER_PREFIX_list[0]  # \"default\" setting\n",
    "for data_source in data_source_list:\n",
    "    table = build_table_for_algo_with_physician_policy(EXP_FOLDER=exp_setting, data_source=data_source)\n",
    "    print_table_latex(table, algo_list_with_physician_policy, caption_suffix=f\"for {data_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2559cb2d",
   "metadata": {},
   "source": [
    "## Merge Severity for Default Reward Settings with Different Algo (with Physician Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335330c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table_for_algo_with_physician_policy_severity_merged(EXP_FOLDER, data_source):\n",
    "    \"\"\"\n",
    "    Build a merged table that combines all severity categories for algorithm comparison including physician policy.\n",
    "    Returns a single table with metrics for all severity levels.\n",
    "    \"\"\"\n",
    "    result = defaultdict(dict)\n",
    "    \n",
    "    # Include RL algorithms + naive_agent\n",
    "    for algo in algo_list + ['naive_agent']:\n",
    "        if algo == 'naive_agent':\n",
    "            csv_path = f'../models/training_log/naive_agent/rewards_summary_naive_agent_{data_source}.csv'\n",
    "        else:\n",
    "            csv_path = f'../models/training_log/{EXP_FOLDER}/rewards_summary_{algo}_{data_source}.csv'\n",
    "        \n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        for rtype in reward_types:\n",
    "            if algo == 'naive_agent':\n",
    "                best = df[df['category'] == rtype]\n",
    "                best = best.iloc[0] if not best.empty else None\n",
    "            else:\n",
    "                best = extract_best_epoch(df, rtype)\n",
    "            \n",
    "            severity_display = REWARD_TYPE_DISPLAY_MAPPING[rtype].replace('_', ' ').title()\n",
    "            \n",
    "            for key, label in metric_names:\n",
    "                # Create a combined metric name with severity level\n",
    "                combined_metric = f\"{label} ({severity_display})\"\n",
    "                \n",
    "                if best is not None:\n",
    "                    val = best[key]\n",
    "                    val = round(val * 100, 2) if \"%\" in label else round(val, 2)\n",
    "                else:\n",
    "                    val = \"N/A\"\n",
    "                result[combined_metric][algo] = val\n",
    "    \n",
    "    return result\n",
    "\n",
    "def print_compact_merged_table_latex_with_physician(table_dict, columns, exp_folder, data_source):\n",
    "    \"\"\"\n",
    "    Print a compact merged LaTeX table with severity as columns and physician policy included.\n",
    "    \"\"\"\n",
    "    print(\"\\\\begin{table}[htbp]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(f\"\\\\caption{{Algorithm Comparison Across All Severity Categories for {data_source}}}\")\n",
    "    print(\"\\\\small\")\n",
    "    print(\"\\\\resizebox{\\\\textwidth}{!}{%\")\n",
    "    \n",
    "    # Create a structure where each metric has columns for each severity and each algorithm\n",
    "    num_cols = len(columns) * len(REWARD_TYPE_DISPLAY_MAPPING)\n",
    "    print(\"\\\\begin{tabular}{l\" + \"c\" * num_cols + \"}\")\n",
    "    print(\"\\\\toprule\")\n",
    "    \n",
    "    # Multi-level header\n",
    "    header_row1 = [\"\\\\textbf{Metric}\"]\n",
    "    header_row2 = [\"\"]\n",
    "    \n",
    "    for col in columns:\n",
    "        if col == \"naive_agent\":\n",
    "            escaped_col = \"Physician Policy\"\n",
    "        else:\n",
    "            escaped_col = escape_latex(col)\n",
    "        header_row1.extend([f\"\\\\multicolumn{{3}}{{c}}{{\\\\texttt{{{escaped_col}}}}}\", \"\", \"\"])\n",
    "        header_row2.extend([\"\\\\textbf{High}\", \"\\\\textbf{Med}\", \"\\\\textbf{Low}\"])\n",
    "    \n",
    "    # Remove extra empty strings\n",
    "    header_row1 = [h for h in header_row1 if h != \"\"]\n",
    "    \n",
    "    print(\" & \".join(header_row1) + \" \\\\\\\\\")\n",
    "    print(\" & \".join(header_row2) + \" \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    \n",
    "    # Data rows\n",
    "    base_metrics = [label for key, label in metric_names]\n",
    "    \n",
    "    for base_metric in base_metrics:\n",
    "        row = [base_metric]\n",
    "        \n",
    "        # For each algorithm (column)\n",
    "        for col in columns:\n",
    "            # For each severity category within this algorithm column\n",
    "            for severity_key in [\"high_severity\", \"median_severity\", \"low_severity\"]:\n",
    "                severity_display = severity_key.replace('_', ' ').title()\n",
    "                combined_metric = f\"{base_metric} ({severity_display})\"\n",
    "                \n",
    "                # Get the value for this specific algorithm and severity\n",
    "                val = table_dict.get(combined_metric, {}).get(col, \"N/A\")\n",
    "                row.append(str(val))\n",
    "        \n",
    "        print(\" & \".join(row) + \" \\\\\\\\\")\n",
    "    \n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"}%\")\n",
    "    \n",
    "    exp_folder_safe = escape_latex(exp_folder)\n",
    "    print(f\"\\\\label{{tab:algo_comparison_with_physician_{exp_folder_safe}_{data_source}}}\")\n",
    "    print(\"\\\\end{table}\")\n",
    "    print()\n",
    "\n",
    "# Usage function\n",
    "def generate_compact_merged_tables_with_physician():\n",
    "    \"\"\"Generate only compact merged algorithm comparison tables with physician policy.\"\"\"\n",
    "    exp_setting = EXP_FOLDER_PREFIX_list[0]  # Use default setting\n",
    "    columns = ['naive_agent'] + algo_list\n",
    "    \n",
    "    for data_source in data_source_list:\n",
    "        table_dict = build_table_for_algo_with_physician_policy_severity_merged(exp_setting, data_source)\n",
    "        print(f\"% Compact algorithm comparison table with physician for {exp_setting} on {data_source}\")\n",
    "        print_compact_merged_table_latex_with_physician(table_dict, columns, exp_setting, data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a34d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use it:\n",
    "# [Part 1] Default reward settings with different algo\n",
    "generate_compact_merged_tables_with_physician()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18951f53",
   "metadata": {},
   "source": [
    "## Highlight and Merge Severity for Default Reward Settings with Different Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef9709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC_OPTIMIZATION = {\n",
    "#     \"Total Cumulative Reward\": \"higher\",\n",
    "#     \"Extubation Success Rate (%)\": \"higher\",\n",
    "#     \"Avg. Episode Length (hrs)\": \"lower\",\n",
    "#     \"Avg. Time to Meet (hrs)\": \"lower\",\n",
    "#     \"Action Diversity\": \"higher\",\n",
    "#     \"Anomalous Actions (%)\": \"lower\"\n",
    "# }\n",
    "METRIC_OPTIMIZATION = {\n",
    "    \"Total Cumulative Reward\": \"higher\",\n",
    "    \"Extubation Meet Rate (\\%)\": \"higher\",\n",
    "    \"Avg. Trajectory Length (hrs)\": \"lower\",\n",
    "    \"Avg. Time to Meet (hrs)\": \"lower\",\n",
    "    \"Action Diversity\": \"higher\",\n",
    "    \"Anomalous Actions (\\%)\": \"lower\"\n",
    "}\n",
    "\n",
    "def build_table_for_algo_merged_with_physician_highlight(EXP_FOLDER, data_source):\n",
    "    result = defaultdict(dict)\n",
    "    for algo in algo_list + ['naive_agent']:\n",
    "        if algo == 'naive_agent':\n",
    "            csv_path = f'../models/training_log/naive_agent/rewards_summary_naive_agent_{data_source}.csv'\n",
    "        else:\n",
    "            csv_path = f'../models/training_log/{EXP_FOLDER}/rewards_summary_{algo}_{data_source}.csv'\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        for rtype in reward_types:\n",
    "            best = None\n",
    "            if algo == 'naive_agent':\n",
    "                subset = df[df['category'] == rtype]\n",
    "                if not subset.empty:\n",
    "                    best = subset.iloc[0]\n",
    "            else:\n",
    "                best = extract_best_epoch(df, rtype)\n",
    "            sev_disp = REWARD_TYPE_DISPLAY_MAPPING[rtype].replace('_',' ').title()\n",
    "            for key, label in metric_names:\n",
    "                combined = f\"{label} ({sev_disp})\"\n",
    "                if best is not None:\n",
    "                    val = best[key]\n",
    "                    val = round(val*100,2) if \"%\" in label else round(val,2)\n",
    "                else:\n",
    "                    val = \"N/A\"\n",
    "                result[combined][algo] = val\n",
    "    return result\n",
    "\n",
    "def _to_num(x):\n",
    "    \"\"\"Try to coerce to float; return None if not possible.\"\"\"\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def compute_best_per_metric(table_dict):\n",
    "    best_vals = {sev: {} for sev in [\"High Severity\",\"Median Severity\",\"Low Severity\"]}\n",
    "    for combined, algo_vals in table_dict.items():\n",
    "        metric, sev = combined.rsplit(\" (\",1)\n",
    "        sev = sev[:-1]\n",
    "        direction = METRIC_OPTIMIZATION.get(metric)\n",
    "        if not direction:\n",
    "            continue\n",
    "        nums = [_to_num(v) for v in algo_vals.values()]\n",
    "        nums = [v for v in nums if v is not None]\n",
    "        if not nums:\n",
    "            continue\n",
    "        best = max(nums) if direction==\"higher\" else min(nums)\n",
    "        best_vals[sev][metric] = best\n",
    "    return best_vals\n",
    "\n",
    "def is_best(raw, best):\n",
    "    \"\"\"Return True if raw equals best after float coercion.\"\"\"\n",
    "    r = _to_num(raw)\n",
    "    # print(f\"debug message: raw={raw}, best={best}, r={r}\")\n",
    "    # print(f\"type of r: {type(r)}, type of best: {type(best)}\")\n",
    "    return (r is not None) and (best is not None) and (r == best)\n",
    "\n",
    "def print_compact_merged_table_latex_with_physician_highlighted(table_dict, columns, exp_folder, data_source):\n",
    "    \"\"\"\n",
    "    Print a compact merged LaTeX table with severity as columns and physician policy included.\n",
    "    Best values are highlighted in bold for each severity category within each metric row.\n",
    "    \"\"\"\n",
    "    best_vals = compute_best_per_metric(table_dict)\n",
    "\n",
    "    print(\"\\\\begin{table}[htbp]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(f\"\\\\caption{{Algorithm Comparison Across All Severity Categories for {data_source}}}\")\n",
    "    print(\"\\\\small\")\n",
    "    print(\"\\\\resizebox{\\\\textwidth}{!}{%\")\n",
    "    num_cols = len(columns)*3\n",
    "    print(\"\\\\begin{tabular}{l\" + \"c\"*num_cols + \"}\")\n",
    "    print(\"\\\\toprule\")\n",
    "\n",
    "    # header\n",
    "    hdr1 = [\"\\\\textbf{Metric}\"]\n",
    "    hdr2 = [\"\"]\n",
    "    for col in columns:\n",
    "        name = \"Physician Policy\" if col==\"naive_agent\" else escape_latex(col)\n",
    "        hdr1.append(f\"\\\\multicolumn{{3}}{{c}}{{\\\\texttt{{{name}}}}}\")\n",
    "        hdr2 += [\"\\\\textbf{High}\", \"\\\\textbf{Med}\", \"\\\\textbf{Low}\"]\n",
    "    print(\" & \".join(hdr1) + \" \\\\\\\\\")\n",
    "    print(\" & \".join(hdr2) + \" \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "\n",
    "    base_metrics = [label for _,label in metric_names]\n",
    "    for metric in base_metrics:\n",
    "        row = [metric]\n",
    "        for col in columns:\n",
    "            for sev_key in [\"high_severity\",\"median_severity\",\"low_severity\"]:\n",
    "                sev_disp = sev_key.replace(\"_\",\" \").title()\n",
    "                combined = f\"{metric} ({sev_disp})\"\n",
    "                raw = table_dict.get(combined,{}).get(col,\"N/A\")\n",
    "                if is_best(raw, best_vals[sev_disp].get(metric)):\n",
    "                    cell = f\"\\\\textbf{{{raw}}}\"\n",
    "                else:\n",
    "                    cell = str(raw)\n",
    "                row.append(cell)\n",
    "        print(\" & \".join(row) + \" \\\\\\\\\")\n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"}%\")\n",
    "    safe = escape_latex(exp_folder)\n",
    "    print(f\"\\\\label{{tab:algo_comparison_with_physician_{safe}_{data_source}}}\")\n",
    "    print(\"\\\\end{table}\\n\")\n",
    "\n",
    "def print_vertical_merged_table_latex_with_physician_highlighted(table_dict, columns, exp_folder, data_source):\n",
    "    \"\"\"\n",
    "    Print a vertical merged LaTeX table with severity sections and physician policy included.\n",
    "    Best values are highlighted in bold within each severity section.\n",
    "    \"\"\"\n",
    "    best_vals = compute_best_per_metric(table_dict)\n",
    "\n",
    "    print(\"\\\\begin{table}[htbp]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(f\"\\\\caption{{Algorithm Performance Across All Severity Categories for {data_source}}}\")\n",
    "    print(\"\\\\small\")\n",
    "    print(\"\\\\resizebox{\\\\textwidth}{!}{%\")\n",
    "    print(\"\\\\begin{tabular}{l\" + \"c\"*len(columns) + \"}\")\n",
    "    print(\"\\\\toprule\")\n",
    "\n",
    "    header = [\"\\\\textbf{Metric}\"] + [\n",
    "        f\"\\\\texttt{{'Physician Policy' if c=='naive_agent' else escape_latex(c)}}\"\n",
    "        for c in columns\n",
    "    ]\n",
    "    print(\" & \".join(header) + \" \\\\\\\\\\n\\\\midrule\")\n",
    "\n",
    "    for sev in [\"High Severity\",\"Median Severity\",\"Low Severity\"]:\n",
    "        print(f\"\\\\multicolumn{{{len(columns)+1}}}{{c}}{{\\\\textbf{{{sev}}}}} \\\\\\\\\\n\\\\midrule\")\n",
    "        for combined in sorted(table_dict.keys()):\n",
    "            if combined.endswith(f\"({sev})\"):\n",
    "                metric = combined.replace(f\" ({sev})\",\"\")\n",
    "                row = [metric]\n",
    "                for col in columns:\n",
    "                    raw = table_dict[combined].get(col,\"N/A\")\n",
    "                    if is_best(raw, best_vals[sev].get(metric)):\n",
    "                        cell = f\"\\\\textbf{{{raw}}}\"\n",
    "                    else:\n",
    "                        cell = str(raw)\n",
    "                    row.append(cell)\n",
    "                print(\" & \".join(row) + \" \\\\\\\\\")\n",
    "        if sev!=\"Low Severity\":\n",
    "            print(\"\\\\addlinespace\")\n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"}%\")\n",
    "    safe = escape_latex(exp_folder)\n",
    "    print(f\"\\\\label{{tab:vertical_algo_comparison_with_physician_{safe}_{data_source}}}\")\n",
    "    print(\"\\\\end{table}\\n\")\n",
    "\n",
    "# Updated usage functions with highlighting\n",
    "def generate_compact_merged_tables_with_physician_highlighted():\n",
    "    \"\"\"Generate compact merged algorithm comparison tables with physician policy and highlighting.\"\"\"\n",
    "    exp_setting = EXP_FOLDER_PREFIX_list[0]\n",
    "    cols = ['naive_agent'] + algo_list\n",
    "    for ds in data_source_list:\n",
    "        tbl = build_table_for_algo_merged_with_physician_highlight(exp_setting, ds)\n",
    "        print(f\"% Compact algorithm comparison table with physician and highlighting for {exp_setting} on {ds}\")\n",
    "        print_compact_merged_table_latex_with_physician_highlighted(tbl, cols, exp_setting, ds)\n",
    "\n",
    "def generate_vertical_merged_tables_with_physician_highlighted():\n",
    "    \"\"\"Generate vertical merged algorithm comparison tables with physician policy and highlighting.\"\"\"\n",
    "    exp_setting = EXP_FOLDER_PREFIX_list[0]\n",
    "    cols = ['naive_agent'] + algo_list\n",
    "    for ds in data_source_list:\n",
    "        tbl = build_table_for_algo_merged_with_physician_highlight(exp_setting, ds)\n",
    "        print(f\"% Vertical algorithm comparison table with physician and highlighting for {exp_setting} on {ds}\")\n",
    "        print_vertical_merged_table_latex_with_physician_highlighted(tbl, cols, exp_setting, ds)\n",
    "\n",
    "def generate_both_merged_tables_with_physician_highlighted():\n",
    "    \"\"\"Generate both compact and vertical merged tables with physician policy and highlighting.\"\"\"\n",
    "    exp_setting = EXP_FOLDER_PREFIX_list[0]\n",
    "    cols = ['naive_agent'] + algo_list\n",
    "    for ds in data_source_list:\n",
    "        tbl = build_table_for_algo_merged_with_physician_highlight(exp_setting, ds)\n",
    "        \n",
    "        # Compact version\n",
    "        print(f\"% Compact algorithm comparison table with physician and highlighting for {exp_setting} on {ds}\")\n",
    "        print_compact_merged_table_latex_with_physician_highlighted(tbl, cols, exp_setting, ds)\n",
    "        \n",
    "        # Vertical version\n",
    "        print(f\"% Vertical algorithm comparison table with physician and highlighting for {exp_setting} on {ds}\")\n",
    "        print_vertical_merged_table_latex_with_physician_highlighted(tbl, cols, exp_setting, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage examples:\n",
    "# For compact tables only:\n",
    "# [Part 1] With Highlighted Best Values\n",
    "generate_compact_merged_tables_with_physician_highlighted()\n",
    "\n",
    "# For vertical tables only:\n",
    "# generate_vertical_merged_tables_with_physician_highlighted()\n",
    "\n",
    "# For both layouts:\n",
    "# generate_both_merged_tables_with_physician_highlighted()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc03b95a",
   "metadata": {},
   "source": [
    "## Merge severity for Different Reward Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f3b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table_for_reward_design_severity_merged(algo, data_source):\n",
    "    \"\"\"\n",
    "    Build a merged table that combines all severity categories for reward design comparison for a specific algorithm.\n",
    "    Returns a single table with metrics for all severity levels across different reward designs.\n",
    "    \"\"\"\n",
    "    result = defaultdict(dict)\n",
    "    \n",
    "    for EXP_FOLDER in EXP_FOLDER_PREFIX_list:\n",
    "        csv_path = f'../models/training_log/{EXP_FOLDER}/rewards_summary_{algo}_{data_source}.csv'\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        for rtype in reward_types:\n",
    "            best = extract_best_epoch(df, rtype)\n",
    "            severity_display = REWARD_TYPE_DISPLAY_MAPPING[rtype].replace('_', ' ').title()\n",
    "            \n",
    "            for key, label in metric_names:\n",
    "                # Create a combined metric name with severity level\n",
    "                combined_metric = f\"{label} ({severity_display})\"\n",
    "                \n",
    "                if best is not None:\n",
    "                    val = best[key]\n",
    "                    val = round(val * 100, 2) if \"%\" in label else round(val, 2)\n",
    "                else:\n",
    "                    val = \"N/A\"\n",
    "                result[combined_metric][EXP_FOLDER] = val\n",
    "    \n",
    "    return result\n",
    "\n",
    "def print_compact_reward_design_table_latex(table_dict, columns, algo, data_source):\n",
    "    \"\"\"\n",
    "    Print a compact merged LaTeX table with severity as columns for reward design comparison.\n",
    "    \"\"\"\n",
    "    print(\"\\\\begin{table}[htbp]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(f\"\\\\caption{{Reward Design Comparison Across All Severity Categories for {data_source} ({algo})}}\")\n",
    "    print(\"\\\\small\")\n",
    "    print(\"\\\\resizebox{\\\\textwidth}{!}{%\")\n",
    "    \n",
    "    # Create a structure where each metric has columns for each severity and each reward design\n",
    "    num_cols = len(columns) * len(REWARD_TYPE_DISPLAY_MAPPING)\n",
    "    print(\"\\\\begin{tabular}{l\" + \"c\" * num_cols + \"}\")\n",
    "    print(\"\\\\toprule\")\n",
    "    \n",
    "    # Multi-level header\n",
    "    header_row1 = [\"\\\\textbf{Metric}\"]\n",
    "    header_row2 = [\"\"]\n",
    "    \n",
    "    for col in columns:\n",
    "        short_label = FOLDER_NAME_MAPPING.get(col, col)\n",
    "        escaped_label = escape_latex(short_label)\n",
    "        header_row1.extend([f\"\\\\multicolumn{{3}}{{c}}{{\\\\texttt{{{escaped_label}}}}}\", \"\", \"\"])\n",
    "        header_row2.extend([\"\\\\textbf{High}\", \"\\\\textbf{Med}\", \"\\\\textbf{Low}\"])\n",
    "    \n",
    "    # Remove extra empty strings\n",
    "    header_row1 = [h for h in header_row1 if h != \"\"]\n",
    "    \n",
    "    print(\" & \".join(header_row1) + \" \\\\\\\\\")\n",
    "    print(\" & \".join(header_row2) + \" \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    \n",
    "    # Data rows\n",
    "    base_metrics = [label for key, label in metric_names]\n",
    "    \n",
    "    for base_metric in base_metrics:\n",
    "        row = [base_metric]\n",
    "        \n",
    "        # For each reward design (column)\n",
    "        for col in columns:\n",
    "            # For each severity category within this reward design column\n",
    "            for severity_key in [\"high_severity\", \"median_severity\", \"low_severity\"]:\n",
    "                severity_display = severity_key.replace('_', ' ').title()\n",
    "                combined_metric = f\"{base_metric} ({severity_display})\"\n",
    "                \n",
    "                # Get the value for this specific reward design and severity\n",
    "                val = table_dict.get(combined_metric, {}).get(col, \"N/A\")\n",
    "                row.append(str(val))\n",
    "        \n",
    "        print(\" & \".join(row) + \" \\\\\\\\\")\n",
    "    \n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"}%\")\n",
    "    \n",
    "    safe_algo = escape_latex(algo)\n",
    "    print(f\"\\\\label{{tab:reward_design_comparison_{safe_algo}_{data_source}}}\")\n",
    "    print(\"\\\\end{table}\")\n",
    "    print()\n",
    "\n",
    "def print_vertical_reward_design_table_latex(table_dict, columns, algo, data_source):\n",
    "    \"\"\"\n",
    "    Print a vertical merged LaTeX table with severity sections for reward design comparison.\n",
    "    \"\"\"\n",
    "    print(\"\\\\begin{table}[htbp]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(f\"\\\\caption{{Reward Design Performance Across All Severity Categories for {data_source} ({algo})}}\")\n",
    "    print(\"\\\\small\")\n",
    "    print(\"\\\\resizebox{\\\\textwidth}{!}{%\")\n",
    "    print(\"\\\\begin{tabular}{\" + \"l\" + \"c\" * len(columns) + \"}\")\n",
    "    print(\"\\\\toprule\")\n",
    "\n",
    "    # Header row\n",
    "    header_cells = [\"\\\\textbf{Metric}\"]\n",
    "    for col in columns:\n",
    "        short_label = FOLDER_NAME_MAPPING.get(col, col)\n",
    "        escaped_label = escape_latex(short_label)\n",
    "        header_cells.append(f\"\\\\texttt{{{escaped_label}}}\")\n",
    "    print(\" & \".join(header_cells) + \" \\\\\\\\\")\n",
    "\n",
    "    print(\"\\\\midrule\")\n",
    "\n",
    "    # Group metrics by severity for better organization\n",
    "    severity_order = [\"High Severity\", \"Median Severity\", \"Low Severity\"]\n",
    "    \n",
    "    for severity in severity_order:\n",
    "        # Add a section header for each severity level\n",
    "        print(f\"\\\\multicolumn{{{len(columns) + 1}}}{{c}}{{\\\\textbf{{{severity}}}}} \\\\\\\\\")\n",
    "        print(\"\\\\midrule\")\n",
    "        \n",
    "        # Find all metrics for this severity level\n",
    "        severity_metrics = [metric for metric in table_dict.keys() if f\"({severity})\" in metric]\n",
    "        \n",
    "        for metric in severity_metrics:\n",
    "            # Clean up the metric name by removing the severity suffix\n",
    "            clean_metric = metric.replace(f\" ({severity})\", \"\")\n",
    "            \n",
    "            row = [clean_metric]\n",
    "            for col in columns:\n",
    "                val = table_dict[metric].get(col, \"N/A\")\n",
    "                row.append(str(val))\n",
    "            \n",
    "            print(\" & \".join(row) + \" \\\\\\\\\")\n",
    "        \n",
    "        if severity != severity_order[-1]:\n",
    "            print(\"\\\\addlinespace\")\n",
    "\n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"}%\")\n",
    "    \n",
    "    safe_algo = escape_latex(algo)\n",
    "    print(f\"\\\\label{{tab:vertical_reward_design_{safe_algo}_{data_source}}}\")\n",
    "    print(\"\\\\end{table}\")\n",
    "    print()\n",
    "\n",
    "# Usage functions\n",
    "def generate_reward_design_tables_severity_merged(algo, data_source):\n",
    "    \"\"\"Generate merged reward design comparison tables for a specific algorithm and data source.\"\"\"\n",
    "    table_dict = build_table_for_reward_design_severity_merged(algo, data_source)\n",
    "    \n",
    "    # Option 1: Vertical layout with severity sections\n",
    "    # print(f\"% Vertical reward design table for {algo} on {data_source}\")\n",
    "    # print_vertical_reward_design_table_latex(table_dict, EXP_FOLDER_PREFIX_list, algo, data_source)\n",
    "    \n",
    "    # Option 2: Compact horizontal layout\n",
    "    print(f\"% Compact reward design table for {algo} on {data_source}\")\n",
    "    print_compact_reward_design_table_latex(table_dict, EXP_FOLDER_PREFIX_list, algo, data_source)\n",
    "\n",
    "def generate_all_reward_design_tables_severity_merged():\n",
    "    \"\"\"Generate merged reward design tables for all algorithms and data sources.\"\"\"\n",
    "    for data_source in data_source_list:\n",
    "        for algo in algo_list:\n",
    "            generate_reward_design_tables_severity_merged(algo, data_source)\n",
    "\n",
    "# Usage example with your original code pattern:\n",
    "def generate_reward_design_tables_for_data_source_severity_merged(data_source):\n",
    "    \"\"\"Generate reward design tables for all algorithms on a specific data source with severity merged.\"\"\"\n",
    "    for algo in algo_list:\n",
    "        table_dict = build_table_for_reward_design_severity_merged(algo, data_source)\n",
    "        print(f\"% Compact reward design table for {algo} on {data_source}\")\n",
    "        print_compact_reward_design_table_latex(table_dict, EXP_FOLDER_PREFIX_list, algo, data_source)\n",
    "\n",
    "# To replicate your original usage pattern:\n",
    "# data_source = \"eICU\"\n",
    "# generate_reward_design_tables_for_data_source_severity_merged(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82412aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Part 2] Reward Design Comparison with Severity Merged\n",
    "generate_all_reward_design_tables_severity_merged()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc9d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compare reward designs for a specific algorithm\n",
    "algo = \"DiscreteCQL\"\n",
    "table_dict = build_table_for_reward_design(algo, data_source)\n",
    "print_table_latex(table_dict, EXP_FOLDER_PREFIX_list, caption_suffix=f\"for {data_source} ({algo})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = \"eICU\"\n",
    "for algo in algo_list:\n",
    "    # algo = \"DiscreteCQL\"\n",
    "    table_dict = build_table_for_reward_design(algo, data_source)\n",
    "    print_table_latex(table_dict, EXP_FOLDER_PREFIX_list, caption_suffix=f\"for {data_source} ({algo})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compare reward designs for a specific algorithm\n",
    "for data_source in data_source_list:\n",
    "    for algo in algo_list:\n",
    "        # algo = \"DiscreteCQL\"\n",
    "        table_dict = build_table_for_reward_design(algo, data_source)\n",
    "        print_table_latex(table_dict, EXP_FOLDER_PREFIX_list, caption_suffix=f\"for {data_source} ({algo})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57969b7e",
   "metadata": {},
   "source": [
    "## Highlight and Merge Severity for Different Reward Design (single algo * dataset have one table) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428746c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to your existing notebook after the existing reward design functions\n",
    "\n",
    "def build_table_for_reward_design_severity_merged_with_highlight(algo, data_source):\n",
    "    \"\"\"\n",
    "    Build a merged table that combines all severity categories for reward design comparison for a specific algorithm.\n",
    "    Returns a single table with metrics for all severity levels across different reward designs.\n",
    "    \"\"\"\n",
    "    result = defaultdict(dict)\n",
    "    \n",
    "    for EXP_FOLDER in EXP_FOLDER_PREFIX_list:\n",
    "        csv_path = f'../models/training_log/{EXP_FOLDER}/rewards_summary_{algo}_{data_source}.csv'\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        for rtype in reward_types:\n",
    "            best = extract_best_epoch(df, rtype)\n",
    "            severity_display = REWARD_TYPE_DISPLAY_MAPPING[rtype].replace('_', ' ').title()\n",
    "            \n",
    "            for key, label in metric_names:\n",
    "                # Create a combined metric name with severity level\n",
    "                combined_metric = f\"{label} ({severity_display})\"\n",
    "                \n",
    "                if best is not None:\n",
    "                    val = best[key]\n",
    "                    val = round(val * 100, 2) if \"%\" in label else round(val, 2)\n",
    "                else:\n",
    "                    val = \"N/A\"\n",
    "                result[combined_metric][EXP_FOLDER] = val\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compute_best_per_metric_reward_design(table_dict):\n",
    "    \"\"\"Compute best values for each metric within each severity category for reward design tables.\"\"\"\n",
    "    best_vals = {sev: {} for sev in [\"High Severity\", \"Median Severity\", \"Low Severity\"]}\n",
    "    \n",
    "    for combined, design_vals in table_dict.items():\n",
    "        # Parse the combined metric name to extract metric and severity\n",
    "        metric, sev = combined.rsplit(\" (\", 1)\n",
    "        sev = sev[:-1]  # Remove the closing parenthesis\n",
    "        \n",
    "        direction = METRIC_OPTIMIZATION.get(metric)\n",
    "        if not direction:\n",
    "            continue\n",
    "            \n",
    "        # Get all numeric values for this metric-severity combination\n",
    "        nums = [_to_num(v) for v in design_vals.values()]\n",
    "        nums = [v for v in nums if v is not None]\n",
    "        \n",
    "        if not nums:\n",
    "            continue\n",
    "            \n",
    "        # Find the best value based on optimization direction\n",
    "        best = max(nums) if direction == \"higher\" else min(nums)\n",
    "        best_vals[sev][metric] = best\n",
    "    \n",
    "    return best_vals\n",
    "\n",
    "def print_compact_reward_design_table_latex_with_highlight(table_dict, columns, algo, data_source):\n",
    "    \"\"\"\n",
    "    Print a compact merged LaTeX table with severity as columns for reward design comparison.\n",
    "    Best values are highlighted in bold for each severity category within each metric row.\n",
    "    \"\"\"\n",
    "    best_vals = compute_best_per_metric_reward_design(table_dict)\n",
    "    \n",
    "    print(\"\\\\begin{table}[htbp]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(f\"\\\\caption{{Reward Design Comparison Across All Severity Categories for {data_source} ({algo})}}\")\n",
    "    print(\"\\\\small\")\n",
    "    print(\"\\\\resizebox{\\\\textwidth}{!}{%\")\n",
    "    \n",
    "    # Create a structure where each metric has columns for each severity and each reward design\n",
    "    num_cols = len(columns) * len(REWARD_TYPE_DISPLAY_MAPPING)\n",
    "    print(\"\\\\begin{tabular}{l\" + \"c\" * num_cols + \"}\")\n",
    "    print(\"\\\\toprule\")\n",
    "    \n",
    "    # Multi-level header\n",
    "    header_row1 = [\"\\\\textbf{Metric}\"]\n",
    "    header_row2 = [\"\"]\n",
    "    \n",
    "    for col in columns:\n",
    "        short_label = FOLDER_NAME_MAPPING.get(col, col)\n",
    "        escaped_label = escape_latex(short_label)\n",
    "        header_row1.extend([f\"\\\\multicolumn{{3}}{{c}}{{\\\\texttt{{{escaped_label}}}}}\", \"\", \"\"])\n",
    "        header_row2.extend([\"\\\\textbf{High}\", \"\\\\textbf{Med}\", \"\\\\textbf{Low}\"])\n",
    "    \n",
    "    # Remove extra empty strings\n",
    "    header_row1 = [h for h in header_row1 if h != \"\"]\n",
    "    \n",
    "    print(\" & \".join(header_row1) + \" \\\\\\\\\")\n",
    "    print(\" & \".join(header_row2) + \" \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    \n",
    "    # Data rows\n",
    "    base_metrics = [label for key, label in metric_names]\n",
    "    \n",
    "    for base_metric in base_metrics:\n",
    "        row = [base_metric]\n",
    "        \n",
    "        # For each reward design (column)\n",
    "        for col in columns:\n",
    "            # For each severity category within this reward design column\n",
    "            for severity_key in [\"high_severity\", \"median_severity\", \"low_severity\"]:\n",
    "                severity_display = severity_key.replace('_', ' ').title()\n",
    "                combined_metric = f\"{base_metric} ({severity_display})\"\n",
    "                \n",
    "                # Get the value for this specific reward design and severity\n",
    "                raw = table_dict.get(combined_metric, {}).get(col, \"N/A\")\n",
    "                \n",
    "                # Check if this is the best value for this metric-severity combination\n",
    "                if is_best(raw, best_vals[severity_display].get(base_metric)):\n",
    "                    cell = f\"\\\\textbf{{{raw}}}\"\n",
    "                else:\n",
    "                    cell = str(raw)\n",
    "                \n",
    "                row.append(cell)\n",
    "        \n",
    "        print(\" & \".join(row) + \" \\\\\\\\\")\n",
    "    \n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"}%\")\n",
    "    \n",
    "    safe_algo = escape_latex(algo)\n",
    "    print(f\"\\\\label{{tab:reward_design_comparison_highlighted_{safe_algo}_{data_source}}}\")\n",
    "    print(\"\\\\end{table}\")\n",
    "    print()\n",
    "\n",
    "def print_vertical_reward_design_table_latex_with_highlight(table_dict, columns, algo, data_source):\n",
    "    \"\"\"\n",
    "    Print a vertical merged LaTeX table with severity sections for reward design comparison.\n",
    "    Best values are highlighted in bold within each severity section.\n",
    "    \"\"\"\n",
    "    best_vals = compute_best_per_metric_reward_design(table_dict)\n",
    "    \n",
    "    print(\"\\\\begin{table}[htbp]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(f\"\\\\caption{{Reward Design Performance Across All Severity Categories for {data_source} ({algo})}}\")\n",
    "    print(\"\\\\small\")\n",
    "    print(\"\\\\resizebox{\\\\textwidth}{!}{%\")\n",
    "    print(\"\\\\begin{tabular}{\" + \"l\" + \"c\" * len(columns) + \"}\")\n",
    "    print(\"\\\\toprule\")\n",
    "\n",
    "    # Header row\n",
    "    header_cells = [\"\\\\textbf{Metric}\"]\n",
    "    for col in columns:\n",
    "        short_label = FOLDER_NAME_MAPPING.get(col, col)\n",
    "        escaped_label = escape_latex(short_label)\n",
    "        header_cells.append(f\"\\\\texttt{{{escaped_label}}}\")\n",
    "    print(\" & \".join(header_cells) + \" \\\\\\\\\")\n",
    "\n",
    "    print(\"\\\\midrule\")\n",
    "\n",
    "    # Group metrics by severity for better organization\n",
    "    severity_order = [\"High Severity\", \"Median Severity\", \"Low Severity\"]\n",
    "    \n",
    "    for severity in severity_order:\n",
    "        # Add a section header for each severity level\n",
    "        print(f\"\\\\multicolumn{{{len(columns) + 1}}}{{c}}{{\\\\textbf{{{severity}}}}} \\\\\\\\\")\n",
    "        print(\"\\\\midrule\")\n",
    "        \n",
    "        # Find all metrics for this severity level\n",
    "        severity_metrics = [metric for metric in table_dict.keys() if f\"({severity})\" in metric]\n",
    "        \n",
    "        for metric in severity_metrics:\n",
    "            # Clean up the metric name by removing the severity suffix\n",
    "            clean_metric = metric.replace(f\" ({severity})\", \"\")\n",
    "            \n",
    "            row = [clean_metric]\n",
    "            for col in columns:\n",
    "                raw = table_dict[metric].get(col, \"N/A\")\n",
    "                \n",
    "                # Check if this is the best value for this metric in this severity category\n",
    "                if is_best(raw, best_vals[severity].get(clean_metric)):\n",
    "                    cell = f\"\\\\textbf{{{raw}}}\"\n",
    "                else:\n",
    "                    cell = str(raw)\n",
    "                \n",
    "                row.append(cell)\n",
    "            \n",
    "            print(\" & \".join(row) + \" \\\\\\\\\")\n",
    "        \n",
    "        if severity != severity_order[-1]:\n",
    "            print(\"\\\\addlinespace\")\n",
    "\n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"}%\")\n",
    "    \n",
    "    safe_algo = escape_latex(algo)\n",
    "    print(f\"\\\\label{{tab:vertical_reward_design_highlighted_{safe_algo}_{data_source}}}\")\n",
    "    print(\"\\\\end{table}\")\n",
    "    print()\n",
    "\n",
    "# Updated usage functions with highlighting\n",
    "def generate_reward_design_tables_severity_merged_with_highlight(algo, data_source):\n",
    "    \"\"\"Generate merged reward design comparison tables with highlighting for a specific algorithm and data source.\"\"\"\n",
    "    table_dict = build_table_for_reward_design_severity_merged_with_highlight(algo, data_source)\n",
    "    \n",
    "    # Option 1: Compact horizontal layout with highlighting\n",
    "    print(f\"% Compact reward design table with highlighting for {algo} on {data_source}\")\n",
    "    print_compact_reward_design_table_latex_with_highlight(table_dict, EXP_FOLDER_PREFIX_list, algo, data_source)\n",
    "    \n",
    "    # Option 2: Vertical layout with severity sections and highlighting\n",
    "    # print(f\"% Vertical reward design table with highlighting for {algo} on {data_source}\")\n",
    "    # print_vertical_reward_design_table_latex_with_highlight(table_dict, EXP_FOLDER_PREFIX_list, algo, data_source)\n",
    "\n",
    "def generate_all_reward_design_tables_severity_merged_with_highlight():\n",
    "    \"\"\"Generate merged reward design tables with highlighting for all algorithms and data sources.\"\"\"\n",
    "    for data_source in data_source_list:\n",
    "        for algo in algo_list:\n",
    "            generate_reward_design_tables_severity_merged_with_highlight(algo, data_source)\n",
    "\n",
    "def generate_reward_design_tables_for_data_source_severity_merged_with_highlight(data_source):\n",
    "    \"\"\"Generate reward design tables with highlighting for all algorithms on a specific data source with severity merged.\"\"\"\n",
    "    for algo in algo_list:\n",
    "        table_dict = build_table_for_reward_design_severity_merged_with_highlight(algo, data_source)\n",
    "        print(f\"% Compact reward design table with highlighting for {algo} on {data_source}\")\n",
    "        print_compact_reward_design_table_latex_with_highlight(table_dict, EXP_FOLDER_PREFIX_list, algo, data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage examples:\n",
    "# For all algorithms and data sources with highlighting:\n",
    "# [Part 2] Reward Design Comparison with Severity Merged and Highlighted\n",
    "generate_all_reward_design_tables_severity_merged_with_highlight()\n",
    "\n",
    "# For a specific data source with highlighting:\n",
    "# data_source = \"eICU\"\n",
    "# generate_reward_design_tables_for_data_source_severity_merged_with_highlight(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955e705",
   "metadata": {},
   "source": [
    "## Physician Policy (Naive Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_naive_agent_latex_tables():\n",
    "    data_sources = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "    for data_source in data_sources:\n",
    "        csv_path = f'../models/training_log/naive_agent/rewards_summary_naive_agent_{data_source}.csv'\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"% Missing file for {data_source}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        print(\"\\\\begin{table}[htbp]\")\n",
    "        print(\"\\\\centering\")\n",
    "        print(f\"\\\\caption{{Physician Policy Benchmark ({data_source})}}\")\n",
    "        # print(f\"\\\\caption{{Performance Metrics of Physician Policy ({data_source})}}\")\n",
    "        print(\"\\\\small\")\n",
    "        print(\"\\\\begin{tabular}{lccc}\")\n",
    "        print(\"\\\\toprule\")\n",
    "        print(\"\\\\textbf{Metric} & \\\\textbf{High Severity} & \\\\textbf{Median Severity} & \\\\textbf{Low Severity} \\\\\\\\\")\n",
    "        # print(\"\\\\textbf{Metric} & \\\\textbf{Small Reward} & \\\\textbf{Median Reward} & \\\\textbf{Large Reward} \\\\\\\\\")\n",
    "        print(\"\\\\midrule\")\n",
    "\n",
    "        for key, label in metric_names:\n",
    "            row = [label]\n",
    "            for reward_type in reward_types:\n",
    "                subset = df[df['category'] == reward_type]\n",
    "                if subset.empty:\n",
    "                    row.append(\"N/A\")\n",
    "                else:\n",
    "                    val = subset.iloc[0][key]\n",
    "                    val = round(val * 100, 2) if \"%\" in label else round(val, 2)\n",
    "                    row.append(str(val))\n",
    "            print(\" & \".join(row) + \" \\\\\\\\\")\n",
    "\n",
    "        print(\"\\\\bottomrule\")\n",
    "        print(\"\\\\end{tabular}\")\n",
    "        print(f\"\\\\label{{tab:naive_agent_{data_source}_metrics}}\")\n",
    "        print(\"\\\\end{table}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf0dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_naive_agent_latex_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e89a47",
   "metadata": {},
   "source": [
    "## Default Reward Design Settings with Physician Policy (single algo * dataset have one table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ea1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table_for_algo(EXP_FOLDER, data_source):\n",
    "    result = {rtype: defaultdict(dict) for rtype in reward_types}\n",
    "\n",
    "    # Include RL algorithms\n",
    "    for algo in algo_list + ['naive_agent']:\n",
    "        if algo == 'naive_agent':\n",
    "            csv_path = f'../models/training_log/naive_agent/rewards_summary_naive_agent_{data_source}.csv'\n",
    "        else:\n",
    "            csv_path = f'../models/training_log/{EXP_FOLDER}/rewards_summary_{algo}_{data_source}.csv'\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        for rtype in reward_types:\n",
    "            if algo == 'naive_agent':\n",
    "                best = df[df['category'] == rtype]\n",
    "                best = best.iloc[0] if not best.empty else None\n",
    "            else:\n",
    "                best = extract_best_epoch(df, rtype)\n",
    "\n",
    "            for key, label in metric_names:\n",
    "                if best is not None:\n",
    "                    val = best[key]\n",
    "                    val = round(val * 100, 2) if \"%\" in label else round(val, 2)\n",
    "                else:\n",
    "                    val = \"N/A\"\n",
    "                result[rtype][label][algo] = val\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def print_table_latex(table_dict, columns, caption_suffix=\"\"):\n",
    "    for rtype in reward_types:\n",
    "        severity_display = REWARD_TYPE_DISPLAY_MAPPING[rtype].replace('_', ' ').title()\n",
    "        \n",
    "        print(\"\\\\begin{table}[htbp]\")\n",
    "        print(\"\\\\centering\")\n",
    "        print(f\"\\\\caption{{Performance Metrics ({severity_display.replace('_', ' ').title()}) {caption_suffix}}}\")\n",
    "        print(\"\\\\small\")\n",
    "        print(\"\\\\resizebox{\\\\textwidth}{!}{%\")  # Start resizebox\n",
    "        print(\"\\\\begin{tabular}{\" + \"l\" + \"c\" * len(columns) + \"}\")\n",
    "        print(\"\\\\toprule\")\n",
    "\n",
    "        # Header row\n",
    "        header_cells = [\"\\\\textbf{Metric}\"]\n",
    "        for col in columns:\n",
    "            if col == \"naive_agent\":\n",
    "                escaped_label = \"Physician Policy\"\n",
    "            else:\n",
    "                short_label = FOLDER_NAME_MAPPING.get(col, col)\n",
    "                escaped_label = escape_latex(short_label)\n",
    "            header_cells.append(f\"\\\\texttt{{{escaped_label}}}\")\n",
    "        print(\" & \".join(header_cells) + \" \\\\\\\\\")\n",
    "\n",
    "        print(\"\\\\midrule\")\n",
    "\n",
    "        # Data rows\n",
    "        for key, label in metric_names:\n",
    "            row = [label]\n",
    "            for col in columns:\n",
    "                val = table_dict[rtype][label].get(col, \"N/A\")\n",
    "                row.append(str(val))\n",
    "            print(\" & \".join(row) + \" \\\\\\\\\")\n",
    "\n",
    "        print(\"\\\\bottomrule\")\n",
    "        print(\"\\\\end{tabular}\")\n",
    "        print(\"}%\")  # End resizebox\n",
    "        safe_label = caption_suffix.replace(' ', '_').replace('(', '').replace(')', '').replace('%', '')\n",
    "        print(f\"\\\\label{{tab:rl_metrics_{rtype}_{safe_label}}}\")\n",
    "        print(\"\\\\end{table}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c47b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['naive_agent'] +  algo_list\n",
    "exp_setting = EXP_FOLDER_PREFIX_list[0]  # \"default\" setting\n",
    "for data_source in data_source_list:\n",
    "    table = build_table_for_algo(EXP_FOLDER=exp_setting, data_source=data_source)\n",
    "    print_table_latex(table, columns, caption_suffix=f\"for {data_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c555a66",
   "metadata": {},
   "source": [
    "# Training Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a9614",
   "metadata": {},
   "source": [
    "## Reward curve with Physician Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab73af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER_PREFIX = EXP_FOLDER_PREFIX_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ebde65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "# Load naive_agent data\n",
    "naive_agent_train_rewards = pd.read_csv('../models/training_log/naive_agent/rewards_summary_naive_agent_train.csv')\n",
    "naive_agent_test_rewards = pd.read_csv('../models/training_log/naive_agent/rewards_summary_naive_agent_test.csv')\n",
    "naive_agent_eICU_rewards = pd.read_csv('../models/training_log/naive_agent/rewards_summary_naive_agent_eICU.csv')\n",
    "\n",
    "# Map data sources to naive_agent data\n",
    "naive_agent_data_map = {\n",
    "    \"train\": naive_agent_train_rewards,\n",
    "    \"test\": naive_agent_test_rewards,\n",
    "    \"eICU\": naive_agent_eICU_rewards\n",
    "}\n",
    "\n",
    "# Iterate over each data source\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Get naive_agent data for the current data source\n",
    "    naive_agent_data = naive_agent_data_map[data_source]\n",
    "\n",
    "    # Plot data for each category\n",
    "    for category, algo_data_list in category_data_dict.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        # Plot data for each algorithm\n",
    "        for algo, category_data in algo_data_list:\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            plt.plot(epochs, mean_rewards, label=f\"{algo}\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            plt.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2\n",
    "            )\n",
    "\n",
    "        # Add naive_agent constant curve\n",
    "        if category in naive_agent_data['category'].values:\n",
    "            naive_mean_reward = naive_agent_data[naive_agent_data['category'] == category]['mean_reward'].values[0]\n",
    "            plt.axhline(y=naive_mean_reward, color='r', linestyle='--', label='Physician Policy')\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Training Progress for {category} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Reward\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fe24c",
   "metadata": {},
   "source": [
    "### Merge into single figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be8a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "reward_categories = [\"small_reward\", \"median_reward\", \"large_reward\"]\n",
    "\n",
    "# Load naive_agent data\n",
    "naive_agent_data_map = {}\n",
    "for ds in data_source_list:\n",
    "    naive_agent_data_map[ds] = pd.read_csv(f'../models/training_log/naive_agent/rewards_summary_naive_agent_{ds}.csv')\n",
    "\n",
    "# Create a single figure with subplots (3x3 grid)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle('Training Progress Across Different Data Sources and Severity Categories', fontsize=20, y=0.98)\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "plot_idx = 0\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Get naive_agent data for the current data source\n",
    "    naive_agent_data = naive_agent_data_map[data_source]\n",
    "\n",
    "    # Plot data for each category in the current data source\n",
    "    for category in reward_categories:\n",
    "        if category in category_data_dict:\n",
    "            ax = axes_flat[plot_idx]\n",
    "            \n",
    "            # Plot data for each algorithm\n",
    "            for algo, category_data in category_data_dict[category]:\n",
    "                epochs = category_data['epoch']\n",
    "                mean_rewards = category_data['mean_reward']\n",
    "                std_rewards = category_data['std_reward']\n",
    "\n",
    "                # Plot mean reward curve\n",
    "                ax.plot(epochs, mean_rewards, label=f\"{algo}\", marker='o', markersize=4, linewidth=1.5)\n",
    "\n",
    "                # Plot standard deviation as shaded area\n",
    "                ax.fill_between(\n",
    "                    epochs,\n",
    "                    mean_rewards - std_rewards,\n",
    "                    mean_rewards + std_rewards,\n",
    "                    alpha=0.15\n",
    "                )\n",
    "\n",
    "            # Add naive_agent constant curve\n",
    "            if category in naive_agent_data['category'].values:\n",
    "                naive_mean_reward = naive_agent_data[naive_agent_data['category'] == category]['mean_reward'].values[0]\n",
    "                ax.axhline(y=naive_mean_reward, color='red', linestyle='--', linewidth=2, label='Physician Policy')\n",
    "\n",
    "            # Customize subplot\n",
    "            severity_display = REWARD_TYPE_DISPLAY_MAPPING[category].replace('_', ' ').title()\n",
    "            \n",
    "            ax.set_title(f\"{data_source.upper()} - {severity_display.replace('_', ' ').title()}\", fontsize=12, fontweight='bold')\n",
    "            ax.set_xlabel(\"Epoch\", fontsize=10)\n",
    "            ax.set_ylabel(\"Reward\", fontsize=10)\n",
    "            ax.set_xticks(np.arange(0, 20, 2))\n",
    "            ax.grid(alpha=0.3)\n",
    "            ax.tick_params(labelsize=9)\n",
    "            \n",
    "            # Add legend only to the first subplot to avoid clutter\n",
    "            if plot_idx == 0:\n",
    "                ax.legend(loc=\"upper left\", fontsize=8, ncol=2)\n",
    "        \n",
    "        plot_idx += 1\n",
    "\n",
    "# Adjust layout and save\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.93, hspace=0.3, wspace=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Save the figure for your thesis\n",
    "# plt.savefig('training_progress_combined.pdf', dpi=300, bbox_inches='tight')\n",
    "# plt.savefig('../models/training_log/training_progress_combined.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca552ef1",
   "metadata": {},
   "source": [
    "#### Change Legend without \"Discrete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512dbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "reward_categories = [\"small_reward\", \"median_reward\", \"large_reward\"]\n",
    "algo_display_name = {\n",
    "    \"DiscreteBC\": \"BC\",\n",
    "    \"NFQ\": \"NFQ\",\n",
    "    \"DQN\": \"DQN\",\n",
    "    \"DoubleDQN\": \"DDQN\",\n",
    "    \"DiscreteSAC\": \"SAC\",\n",
    "    \"DiscreteBCQ\": \"BCQ\",\n",
    "    \"DiscreteCQL\": \"CQL\"\n",
    "}\n",
    "# Load naive_agent data\n",
    "naive_agent_data_map = {}\n",
    "for ds in data_source_list:\n",
    "    naive_agent_data_map[ds] = pd.read_csv(f'../models/training_log/naive_agent/rewards_summary_naive_agent_{ds}.csv')\n",
    "\n",
    "# Create a single figure with subplots (3x3 grid)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle('Training Progress Across Different Data Sources and Severity Categories', fontsize=20, y=0.98)\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "plot_idx = 0\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Get naive_agent data for the current data source\n",
    "    naive_agent_data = naive_agent_data_map[data_source]\n",
    "\n",
    "    # Plot data for each category in the current data source\n",
    "    for category in reward_categories:\n",
    "        if category in category_data_dict:\n",
    "            ax = axes_flat[plot_idx]\n",
    "            \n",
    "            # Plot data for each algorithm\n",
    "            for algo, category_data in category_data_dict[category]:\n",
    "                epochs = category_data['epoch']\n",
    "                mean_rewards = category_data['mean_reward']\n",
    "                std_rewards = category_data['std_reward']\n",
    "\n",
    "                # Use display name for legend\n",
    "                display_name = algo_display_name.get(algo, algo)\n",
    "                ax.plot(epochs, mean_rewards, label=display_name, marker='o', markersize=4, linewidth=1.5)\n",
    "\n",
    "                # Plot standard deviation as shaded area\n",
    "                ax.fill_between(\n",
    "                    epochs,\n",
    "                    mean_rewards - std_rewards,\n",
    "                    mean_rewards + std_rewards,\n",
    "                    alpha=0.15\n",
    "                )\n",
    "\n",
    "            # Add naive_agent constant curve\n",
    "            if category in naive_agent_data['category'].values:\n",
    "                naive_mean_reward = naive_agent_data[naive_agent_data['category'] == category]['mean_reward'].values[0]\n",
    "                ax.axhline(y=naive_mean_reward, color='red', linestyle='--', linewidth=2, label='Physician Policy')\n",
    "\n",
    "            # Customize subplot\n",
    "            severity_display = REWARD_TYPE_DISPLAY_MAPPING[category].replace('_', ' ').title()\n",
    "            \n",
    "            ax.set_title(f\"{data_source.upper()} - {severity_display.replace('_', ' ').title()}\", fontsize=12, fontweight='bold')\n",
    "            ax.set_xlabel(\"Epoch\", fontsize=10)\n",
    "            ax.set_ylabel(\"Reward\", fontsize=10)\n",
    "            ax.set_xticks(np.arange(0, 20, 2))\n",
    "            ax.grid(alpha=0.3)\n",
    "            ax.tick_params(labelsize=9)\n",
    "            \n",
    "            # Add legend only to the first subplot to avoid clutter\n",
    "            if plot_idx == 0:\n",
    "                ax.legend(loc=\"upper left\", fontsize=8, ncol=2)\n",
    "        \n",
    "        plot_idx += 1\n",
    "\n",
    "# Adjust layout and save\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.93, hspace=0.3, wspace=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Save the figure for your thesis\n",
    "# plt.savefig('training_progress_combined.pdf', dpi=300, bbox_inches='tight')\n",
    "# plt.savefig('../models/training_log/training_progress_combined.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd67b3c",
   "metadata": {},
   "source": [
    "### Training Progress Curve: Reward, Shared y-axis on the same dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc1268",
   "metadata": {},
   "source": [
    "### Training Progress Curve: Anomalous Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1eaa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "# Iterate over each data source\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Plot data for each category\n",
    "    for category, algo_data_list in category_data_dict.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for algo, category_data in algo_data_list:\n",
    "            epochs = category_data['epoch']\n",
    "            mean_nonsense_actions = category_data['mean_nonsense_actions']\n",
    "            # std_nonsense_actions = category_data['std_nonsense_actions']\n",
    "\n",
    "            # Plot mean nonsense actions curve\n",
    "            plt.plot(epochs, mean_nonsense_actions, label=f\"{algo}\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            # plt.fill_between(\n",
    "            #     epochs,\n",
    "            #     mean_nonsense_actions - std_nonsense_actions,\n",
    "            #     mean_nonsense_actions + std_nonsense_actions,\n",
    "            #     alpha=0.2\n",
    "            # )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Mean Anomaly Actions for {category} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Mean Anomaly Actions\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79506d",
   "metadata": {},
   "source": [
    "### Merge Anomalous Action into 1 Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c83d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "reward_categories = [\"small_reward\", \"median_reward\", \"large_reward\"]\n",
    "\n",
    "# Load naive_agent data for anomalous actions comparison\n",
    "naive_agent_data_map = {}\n",
    "for ds in data_source_list:\n",
    "    naive_agent_data_map[ds] = pd.read_csv(f'../models/training_log/naive_agent/rewards_summary_naive_agent_{ds}.csv')\n",
    "\n",
    "# Create a single figure with subplots (3x3 grid)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle('Anomalous Actions Training Progress Across Data Sources and Severity Categories', fontsize=20, y=0.98)\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "plot_idx = 0\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Get naive_agent data for the current data source\n",
    "    naive_agent_data = naive_agent_data_map[data_source]\n",
    "\n",
    "    # Plot data for each category in the current data source\n",
    "    for category in reward_categories:\n",
    "        if category in category_data_dict:\n",
    "            ax = axes_flat[plot_idx]\n",
    "            \n",
    "            # Plot data for each algorithm\n",
    "            for algo, category_data in category_data_dict[category]:\n",
    "                epochs = category_data['epoch']\n",
    "                mean_nonsense_actions = category_data['mean_nonsense_actions']\n",
    "                \n",
    "                # Plot mean anomalous actions curve\n",
    "                ax.plot(epochs, mean_nonsense_actions, label=f\"{algo}\", marker='o', markersize=4, linewidth=1.5)\n",
    "\n",
    "            # Add naive_agent constant line for anomalous actions\n",
    "            if category in naive_agent_data['category'].values:\n",
    "                naive_nonsense_actions = naive_agent_data[naive_agent_data['category'] == category]['mean_nonsense_actions'].values[0]\n",
    "                ax.axhline(y=naive_nonsense_actions, color='red', linestyle='--', linewidth=2, label='Physician Policy')\n",
    "\n",
    "            # Customize subplot\n",
    "            severity_display = REWARD_TYPE_DISPLAY_MAPPING[category].replace('_', ' ').title()\n",
    "            ax.set_title(f\"{data_source.upper()} - {severity_display.replace('_', ' ').title()}\", fontsize=12, fontweight='bold')\n",
    "            ax.set_xlabel(\"Epoch\", fontsize=10)\n",
    "            ax.set_ylabel(\"Mean Anomalous Actions\", fontsize=10)\n",
    "            # ax.set_ylabel(\"Mean Anomalous Actions (%)\", fontsize=10)\n",
    "            ax.set_xticks(np.arange(0, 20, 2))\n",
    "            ax.grid(alpha=0.3)\n",
    "            ax.tick_params(labelsize=9)\n",
    "            \n",
    "            # Set y-axis limits for better visualization (0-100% for anomalous actions)\n",
    "            ax.set_ylim(0, max(1, ax.get_ylim()[1]))\n",
    "            # ax.set_ylim(0, max(100, ax.get_ylim()[1]))\n",
    "            \n",
    "            # Add legend only to the first subplot to avoid clutter\n",
    "            if plot_idx == 0:\n",
    "                ax.legend(loc=\"upper left\", fontsize=8, ncol=2)\n",
    "        \n",
    "        plot_idx += 1\n",
    "\n",
    "# Adjust layout and save\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.93, hspace=0.3, wspace=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Save the figure for your thesis\n",
    "# plt.savefig('anomalous_actions_training_progress.pdf', dpi=300, bbox_inches='tight')\n",
    "# plt.savefig('anomalous_actions_training_progress.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab63b3",
   "metadata": {},
   "source": [
    "#### Change Legend without \"Discrete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e18b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "reward_categories = [\"small_reward\", \"median_reward\", \"large_reward\"]\n",
    "\n",
    "algo_display_name = {\n",
    "    \"DiscreteBC\": \"BC\",\n",
    "    \"NFQ\": \"NFQ\",\n",
    "    \"DQN\": \"DQN\",\n",
    "    \"DoubleDQN\": \"DDQN\",\n",
    "    \"DiscreteSAC\": \"SAC\",\n",
    "    \"DiscreteBCQ\": \"BCQ\",\n",
    "    \"DiscreteCQL\": \"CQL\"\n",
    "}\n",
    "\n",
    "# Load naive_agent data for anomalous actions comparison\n",
    "naive_agent_data_map = {}\n",
    "for ds in data_source_list:\n",
    "    naive_agent_data_map[ds] = pd.read_csv(f'../models/training_log/naive_agent/rewards_summary_naive_agent_{ds}.csv')\n",
    "\n",
    "# Create a single figure with subplots (3x3 grid)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle('Anomalous Actions Training Progress Across Data Sources and Severity Categories', fontsize=20, y=0.98)\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "plot_idx = 0\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Get naive_agent data for the current data source\n",
    "    naive_agent_data = naive_agent_data_map[data_source]\n",
    "\n",
    "    # Plot data for each category in the current data source\n",
    "    for category in reward_categories:\n",
    "        if category in category_data_dict:\n",
    "            ax = axes_flat[plot_idx]\n",
    "            \n",
    "            # Plot data for each algorithm\n",
    "            for algo, category_data in category_data_dict[category]:\n",
    "                epochs = category_data['epoch']\n",
    "                mean_nonsense_actions = category_data['mean_nonsense_actions']\n",
    "                \n",
    "                # Use display name for legend\n",
    "                display_name = algo_display_name.get(algo, algo)\n",
    "                ax.plot(epochs, mean_nonsense_actions, label=display_name, marker='o', markersize=4, linewidth=1.5)\n",
    "\n",
    "            # Add naive_agent constant line for anomalous actions\n",
    "            if category in naive_agent_data['category'].values:\n",
    "                naive_nonsense_actions = naive_agent_data[naive_agent_data['category'] == category]['mean_nonsense_actions'].values[0]\n",
    "                ax.axhline(y=naive_nonsense_actions, color='red', linestyle='--', linewidth=2, label='Physician Policy')\n",
    "\n",
    "            # Customize subplot\n",
    "            severity_display = REWARD_TYPE_DISPLAY_MAPPING[category].replace('_', ' ').title()\n",
    "            ax.set_title(f\"{data_source.upper()} - {severity_display.replace('_', ' ').title()}\", fontsize=12, fontweight='bold')\n",
    "            ax.set_xlabel(\"Epoch\", fontsize=10)\n",
    "            ax.set_ylabel(\"Mean Anomalous Actions\", fontsize=10)\n",
    "            # ax.set_ylabel(\"Mean Anomalous Actions (%)\", fontsize=10)\n",
    "            ax.set_xticks(np.arange(0, 20, 2))\n",
    "            ax.grid(alpha=0.3)\n",
    "            ax.tick_params(labelsize=9)\n",
    "            \n",
    "            # Set y-axis limits for better visualization (0-100% for anomalous actions)\n",
    "            ax.set_ylim(0, max(1, ax.get_ylim()[1]))\n",
    "            # ax.set_ylim(0, max(100, ax.get_ylim()[1]))\n",
    "            \n",
    "            # Add legend only to the first subplot to avoid clutter\n",
    "            if plot_idx == 0:\n",
    "                ax.legend(loc=\"upper left\", fontsize=8, ncol=2)\n",
    "        \n",
    "        plot_idx += 1\n",
    "\n",
    "# Adjust layout and save\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.93, hspace=0.3, wspace=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Save the figure for your thesis\n",
    "# plt.savefig('anomalous_actions_training_progress.pdf', dpi=300, bbox_inches='tight')\n",
    "# plt.savefig('anomalous_actions_training_progress.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f84934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ventilation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
