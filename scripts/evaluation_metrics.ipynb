{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics in ehrMGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_24_FLAG = False\n",
    "DATA_SOURCE = \"mimic_iv\" # \"eICU\" \"mimic_iv\" \"MIMIC_EXTRACT_mimiciv\"\n",
    "DATA_GEN = \"GAN\" # \"TABLE\" \"GAN\"\n",
    "bins = 20                          # number of bins for discretization\n",
    "obs_hrs = 3                        # number of observation hours (e.g., current + previous)\n",
    "subset_size = 10000                # number of distinct stay_id to use (sample a subset)\n",
    "test_cases_num = 1000              # number of test cases to generate\n",
    "id_cols = ['stay_id', 'before_weaning_hr']\n",
    "feature_cols = ['heart_rate', 'resp_rate', 'spo2', 'fio2', 'respiratory_rate_set', 'gender_M', 'age'] # TODO: add gender, age, peep, (tidal_volume_set, tidal_volume_observed, sbp, dbp, mbp)\n",
    "selected_cols = id_cols + feature_cols\n",
    "state_cols = ['heart_rate', 'resp_rate', 'spo2']  # State variables\n",
    "action_cols = ['fio2', 'respiratory_rate_set']  # Action variables\n",
    "state_units = ['bpm', 'bpm', '%']  # Units for state variables\n",
    "action_units = ['%', 'bpm']  # Units for action variables\n",
    "baseline_cols = ['gender_M', 'age']  # Baseline variables\n",
    "traj_hrs = \"24\" if GAN_24_FLAG else \"all\"\n",
    "prefix_path = f\"../data/{DATA_SOURCE}\"\n",
    "output_dir = f\"{prefix_path}/init_{obs_hrs}_hrs_{bins}_bins_traj_{traj_hrs}_hrs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real data\n",
    "- only select out the first 24 hr of each stay_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Data reading\n",
    "# ================================\n",
    "# Define data paths (update if needed)\n",
    "eICU_prefix_path = \"../data/eICU\"\n",
    "mimic_iii_prefix_path = \"../data/mimic_iii\"\n",
    "mimic_iv_prefix_path = \"../data/mimic_iv\"\n",
    "\n",
    "eICU_file = os.path.join(eICU_prefix_path, \"baseline_charttime_ground_truth.csv\")\n",
    "mimic_iv_file = os.path.join(mimic_iv_prefix_path, \"baseline_charttime_ground_truth.csv\")\n",
    "mimic_iii_file = os.path.join(mimic_iii_prefix_path, \"baseline_charttime_ground_truth.csv\")\n",
    "\n",
    "# Read dataframes\n",
    "eICU_df = pd.read_csv(eICU_file)\n",
    "mimic_iv_df = pd.read_csv(mimic_iv_file)\n",
    "mimic_iii_df = pd.read_csv(mimic_iii_file)\n",
    "\n",
    "# ================================\n",
    "# Add \"hours_in\" column (if missing)\n",
    "# ================================\n",
    "def add_hours_in(baseline_df):\n",
    "    # if hours_in not present, create it based on before_weaning_hr\n",
    "    if 'hours_in' not in list(baseline_df.columns):\n",
    "        vital_sign_df = baseline_df[selected_cols]\n",
    "        vital_sign_df_with_hours = vital_sign_df.copy()\n",
    "        # For each stay_id, use the maximum before_weaning_hr as baseline\n",
    "        max_before_weaning = vital_sign_df.groupby('stay_id')['before_weaning_hr'].transform('max')\n",
    "        vital_sign_df_with_hours['hours_in'] = max_before_weaning - vital_sign_df['before_weaning_hr']\n",
    "        vital_sign_df_with_hours = vital_sign_df_with_hours.sort_values(['stay_id', 'hours_in'])\n",
    "        vital_sign_df_with_hours.drop(columns=['before_weaning_hr'], inplace=True)\n",
    "        cols = list(vital_sign_df_with_hours.columns)\n",
    "        # Move hours_in to position 1 (after stay_id)\n",
    "        if 'hours_in' in cols:\n",
    "            cols.remove('hours_in')\n",
    "        cols.insert(1, 'hours_in')\n",
    "        baseline_df = vital_sign_df_with_hours[cols]\n",
    "    baseline_df['gender_M'] = baseline_df['gender_M'].astype(int)\n",
    "    return baseline_df\n",
    "\n",
    "# Apply to all datasets\n",
    "eICU_df = add_hours_in(eICU_df)\n",
    "mimic_iv_df = add_hours_in(mimic_iv_df)\n",
    "mimic_iii_df = add_hours_in(mimic_iii_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MIMIC IV Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_charttime_ground_truth_first_24_hr_df = mimic_iv_df[mimic_iv_df['hours_in'] < 24]\n",
    "# mimic_iv_first_24_hr_df = baseline_charttime_ground_truth_first_24_hr_df.copy()\n",
    "# mimic_iv_first_24_hr_df\n",
    "mimic_iv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iii_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eICU_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MIMIC IV Naive Agent with Ventilation Patient Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_agent_generated_df = pd.read_csv(\n",
    "    os.path.join(prefix_path, \"naive_agent_generated_traj_cont_larger_24_hr_10000.csv\"))\n",
    "# naive_agent_generated_df[naive_agent_generated_df.isna().any(axis=1)].groupby('stay_id').size().reset_index(name='counts')\n",
    "# naive_agent_generated_df['respiratory_rate_set'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data/mimic_iv/naive_agent_generated_traj_cont_larger_24_hr_10000.csv\n",
    "naive_agent_generated_df = pd.read_csv(\n",
    "    os.path.join(prefix_path, \"naive_agent_generated_traj_cont_larger_24_hr_10000.csv\"))\n",
    "# Convert feature columns to numeric type\n",
    "for col in feature_cols:\n",
    "    if col in naive_agent_generated_df.columns:\n",
    "        naive_agent_generated_df[col] = pd.to_numeric(naive_agent_generated_df[col], errors='coerce')\n",
    "naive_agent_generated_df\n",
    "# naive_agent_generated_first_24_df = naive_agent_generated_df[naive_agent_generated_df['hours_in'] < 24]\n",
    "# naive_agent_generated_first_24_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_agent_generated_df.dropna(inplace=True)\n",
    "naive_agent_generated_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Health Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data/health_gym/Health_gym_FakeSepsis.csv\n",
    "health_gym_raw_df = pd.read_csv(\n",
    "    os.path.join(\"../data/health_gym/Health_gym_FakeSepsis.csv\"))\n",
    "health_gym_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_gym_raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def map_health_gym_to_mimic(health_gym_df):\n",
    "    \"\"\"\n",
    "    Map health_gym_df columns to mimic_iv_df's column names.\n",
    "\n",
    "    Args:\n",
    "        health_gym_df (pd.DataFrame): The input DataFrame with health_gym_df columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with columns mapped to mimic_iv_df's column names.\n",
    "    \"\"\"\n",
    "    health_gym_df['respiratory_rate_set'] = health_gym_df['RR']  # Use RR for respiratory_rate_set\n",
    "    # Define the mapping between health_gym_df and mimic_iv_df columns\n",
    "    column_mapping = {\n",
    "        'PatientID': 'stay_id',              # Map PatientID to stay_id\n",
    "        'Timepoints': 'hours_in',\n",
    "        'HR': 'heart_rate',               # Map HR to heart_rate\n",
    "        'RR': 'resp_rate',               # Map RR to resp_rate\n",
    "        'SpO2': 'spo2',                  # Map SpO2 to spo2\n",
    "        'FiO2': 'fio2',                  # Map FiO2 to fio2\n",
    "        'respiratory_rate_set': 'respiratory_rate_set',    # Use RR for respiratory_rate_set\n",
    "        'Gender': 'gender_M',            # Map Gender to gender_M\n",
    "        'Age': 'age'                     # Map Age to age\n",
    "    }\n",
    "\n",
    "    # Select and rename the columns\n",
    "    mimic_iv_df = health_gym_df[list(column_mapping.keys())].rename(columns=column_mapping)\n",
    "    mimic_iv_df['fio2'] = mimic_iv_df['fio2'] * 100  # Scale FiO2 to percentage\n",
    "    mimic_iv_df['spo2'] = (mimic_iv_df['spo2'] + 1) * 2 + 80  # TODO: since it is category, guess the mapping method: Scale FiO2 to percentage\n",
    "\n",
    "    return mimic_iv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map health_gym_df to mimic_iv_df\n",
    "health_gym_df = map_health_gym_to_mimic(health_gym_raw_df)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "health_gym_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_gym_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SDV PAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data/SDV_PAR/synthetic_data_1_epochs.csv\n",
    "sdv_par_df = pd.read_csv(\n",
    "    os.path.join(\"../data/SDV_PAR/synthetic_data_4_epochs.csv\"))\n",
    "sdv_par_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_par_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN generate data and TABLE generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_gen_data = np.load(f'{output_dir}/fake/gen_data_499.npz')['c_gen_data']\n",
    "TABLE_gen_data = np.load(f'{output_dir}/fake/table_gen_data.npz')['c_gen_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_gen_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renormalizing Generated Data Back to Original Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renormalize_data(gan_data, table_data, original_df):\n",
    "    \"\"\"\n",
    "    Renormalize GAN and TABLE generated data back to the original value ranges.\n",
    "    \n",
    "    Args:\n",
    "        gan_data: GAN generated data (normalized 0-1)\n",
    "        table_data: TABLE generated data (discretized 0-19, then normalized to 0-1)\n",
    "        original_df: Original DataFrame containing the real data with original ranges\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (renormalized_gan_data, renormalized_table_data)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Get original feature columns (excluding stay_id and hours_in)\n",
    "    feature_cols = ['heart_rate', 'resp_rate', 'spo2', 'fio2', 'respiratory_rate_set']\n",
    "    \n",
    "    # Calculate min and max for each feature in the original data\n",
    "    feature_stats = {}\n",
    "    for i, col in enumerate(feature_cols):\n",
    "        feature_stats[i] = {\n",
    "            'min': original_df[col].min(),\n",
    "            'max': original_df[col].max(),\n",
    "            'range': original_df[col].max() - original_df[col].min()\n",
    "        }\n",
    "    \n",
    "    # Create copies to avoid modifying the original arrays\n",
    "    gan_renormalized = np.zeros_like(gan_data)\n",
    "    table_renormalized = np.zeros_like(table_data)\n",
    "    \n",
    "    # Process each feature\n",
    "    for i in range(len(feature_cols)):\n",
    "        # For GAN data: scale from [0,1] to [min, max]\n",
    "        min_val = feature_stats[i]['min']\n",
    "        max_val = feature_stats[i]['max']\n",
    "        gan_renormalized[:, :, i] = gan_data[:, :, i] * (max_val - min_val) + min_val\n",
    "        \n",
    "        # For TABLE data: \n",
    "        # 1) Multiply by 19 to get back to bin indices (0-19)\n",
    "        # 2) Convert to original range\n",
    "        bins = 20\n",
    "        bin_indices = np.round(table_data[:, :, i] * 19).astype(int)\n",
    "        \n",
    "        # Calculate bin edges for this feature\n",
    "        bin_edges = np.linspace(min_val, max_val, bins + 1)\n",
    "        \n",
    "        # Map bin indices to bin centers\n",
    "        for idx in range(bins):\n",
    "            # Find all values with this bin index\n",
    "            mask = (bin_indices == idx)\n",
    "            # Replace with bin center value\n",
    "            bin_center = (bin_edges[idx] + bin_edges[idx + 1]) / 2\n",
    "            table_renormalized[:, :, i][mask] = bin_center\n",
    "    \n",
    "    print(f\"Data renormalized to original ranges:\")\n",
    "    for i, col in enumerate(feature_cols):\n",
    "        print(f\"  {col}: Original range [{feature_stats[i]['min']:.1f}, {feature_stats[i]['max']:.1f}]\")\n",
    "        print(f\"    GAN range: [{gan_renormalized[:,:,i].min():.1f}, {gan_renormalized[:,:,i].max():.1f}]\")\n",
    "        print(f\"    TABLE range: [{table_renormalized[:,:,i].min():.1f}, {table_renormalized[:,:,i].max():.1f}]\")\n",
    "    \n",
    "    return gan_renormalized, table_renormalized\n",
    "\n",
    "# Get feature statistics from original data\n",
    "feature_cols = ['heart_rate', 'resp_rate', 'spo2', 'fio2', 'respiratory_rate_set']\n",
    "feature_stats = baseline_charttime_ground_truth_first_24_hr_df[feature_cols].describe()\n",
    "print(\"Original data statistics:\")\n",
    "print(feature_stats)\n",
    "\n",
    "# Renormalize the data\n",
    "gan_renormalized, table_renormalized = renormalize_data(\n",
    "    gan_data=GAN_gen_data,\n",
    "    table_data=TABLE_gen_data,\n",
    "    original_df=baseline_charttime_ground_truth_first_24_hr_df\n",
    ")\n",
    "\n",
    "# Sample visualization comparing original vs renormalized distributions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot distributions for all features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(feature_cols):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    \n",
    "    # Flatten arrays for distribution plotting\n",
    "    original_values = baseline_charttime_ground_truth_first_24_hr_df[col].values\n",
    "    gan_values = gan_renormalized[:,:,i].flatten()\n",
    "    table_values = table_renormalized[:,:,i].flatten()\n",
    "    \n",
    "    # Plot distributions\n",
    "    sns.kdeplot(original_values, label='Original', linewidth=2)\n",
    "    sns.kdeplot(gan_values, label='GAN', linewidth=2)\n",
    "    sns.kdeplot(table_values, label='TABLE', linewidth=2)\n",
    "    \n",
    "    plt.title(f'{col} Distribution')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the renormalized data if needed\n",
    "np.savez(f'{output_dir}/fake/renormalized_gan_data.npz', data=gan_renormalized)\n",
    "np.savez(f'{output_dir}/fake/renormalized_table_data.npz', data=table_renormalized)\n",
    "\n",
    "print(f\"Renormalized data saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_renormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def gan_to_dataframe(gan_renormalized, start_stay_id=30000000):\n",
    "    \"\"\"\n",
    "    Convert gan_renormalized array into a DataFrame with the desired format.\n",
    "\n",
    "    Args:\n",
    "        gan_renormalized (np.ndarray): The GAN-generated data array of shape (n_patients, n_timesteps, n_features).\n",
    "        start_stay_id (int): The starting stay_id for generating fake IDs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns ['stay_id', 'hours_in', 'heart_rate', 'resp_rate', 'spo2', 'fio2', 'respiratory_rate_set'].\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate over each patient\n",
    "    for patient_idx, patient_data in enumerate(gan_renormalized):\n",
    "        stay_id = start_stay_id + patient_idx  # Generate a fake stay_id\n",
    "        for hour, timestep_data in enumerate(patient_data):\n",
    "            # Extract the relevant features\n",
    "            heart_rate, resp_rate, spo2, fio2, respiratory_rate_set = timestep_data[:5]\n",
    "            # Append the row to the list\n",
    "            rows.append({\n",
    "                'stay_id': stay_id,\n",
    "                'hours_in': hour,\n",
    "                'heart_rate': round(heart_rate, 1),\n",
    "                'resp_rate': round(resp_rate, 1),\n",
    "                'spo2': round(spo2, 1),\n",
    "                'fio2': round(fio2, 1),\n",
    "                'respiratory_rate_set': round(respiratory_rate_set, 1)\n",
    "            })\n",
    "\n",
    "    # Convert the list of rows into a DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert gan_renormalized to DataFrame\n",
    "gan_df = gan_to_dataframe(gan_renormalized)\n",
    "\n",
    "# Display the first few rows\n",
    "gan_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Assuming dataframes are already loaded: mimic_iv_df, mimic_iii_df, etc.\n",
    "dataframes = {\n",
    "    'mimic_iv': mimic_iv_df,\n",
    "    'mimic_iii': mimic_iii_df,\n",
    "    'eICU': eICU_df,\n",
    "    'physician_policy': naive_agent_generated_df,\n",
    "    'health_gym': health_gym_df,\n",
    "    'sdv_par': sdv_par_df,\n",
    "    'gan': gan_df\n",
    "}\n",
    "\n",
    "# Define feature lists for flexibility\n",
    "common_features = ['heart_rate', 'resp_rate', 'spo2', 'fio2', 'respiratory_rate_set']\n",
    "additional_features = ['gender_M', 'age']\n",
    "all_features = common_features + additional_features\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_features = ['gender_M']\n",
    "numerical_features = [f for f in all_features if f not in categorical_features]\n",
    "\n",
    "# Set time horizon (20 hours for consistency with health_gym_df, adjustable)\n",
    "max_hours = 20\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '../data/evaluation_results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to filter dataframes by max_hours\n",
    "def filter_by_hours(df, max_hours):\n",
    "    return df[df['hours_in'] < max_hours]\n",
    "\n",
    "# Function to plot feature distributions\n",
    "def plot_feature_distributions(dataframes, numerical_features, categorical_features, output_dir, max_hours):\n",
    "    # Plot numerical features with KDE\n",
    "    for feature in numerical_features:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for name, df in dataframes.items():\n",
    "            if feature in df.columns:\n",
    "                filtered_df = filter_by_hours(df, max_hours)\n",
    "                sns.kdeplot(filtered_df[feature].dropna(), label=name)\n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(output_dir, f'{feature}_distribution.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # Plot categorical features with bar plots\n",
    "    for feature in categorical_features:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for name, df in dataframes.items():\n",
    "            if feature in df.columns:\n",
    "                filtered_df = filter_by_hours(df, max_hours)\n",
    "                value_counts = filtered_df[feature].value_counts(normalize=True)\n",
    "                plt.bar([f\"{name}_{val}\" for val in value_counts.index], value_counts.values, alpha=0.5, label=name)\n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.savefig(os.path.join(output_dir, f'{feature}_distribution.png'))\n",
    "        plt.close()\n",
    "\n",
    "# Function to compute summary statistics\n",
    "def compute_summary_stats(dataframes, features, output_dir, max_hours):\n",
    "    for name, df in dataframes.items():\n",
    "        filtered_df = filter_by_hours(df, max_hours)\n",
    "        available_features = [f for f in features if f in df.columns]\n",
    "        if available_features:\n",
    "            summary = filtered_df[available_features].describe()\n",
    "            summary.to_csv(os.path.join(output_dir, f'{name}_summary.csv'))\n",
    "\n",
    "# Function to compute KS test against reference (mimic_iv)\n",
    "def compute_ks_tests(dataframes, features, output_dir, max_hours):\n",
    "    reference_df = filter_by_hours(dataframes['mimic_iv'], max_hours)\n",
    "    ks_results = {}\n",
    "    for feature in features:\n",
    "        if feature in reference_df.columns:\n",
    "            ks_results[feature] = {}\n",
    "            ref_data = reference_df[feature].dropna()\n",
    "            for name, df in dataframes.items():\n",
    "                if name == 'mimic_iv' or feature not in df.columns:\n",
    "                    continue\n",
    "                filtered_df = filter_by_hours(df, max_hours)\n",
    "                synth_data = filtered_df[feature].dropna()\n",
    "                ks_stat, p_value = ks_2samp(ref_data, synth_data)\n",
    "                ks_results[feature][name] = {'ks_stat': ks_stat, 'p_value': p_value}\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'ks_results.txt'), 'w') as f:\n",
    "        for feature, results in ks_results.items():\n",
    "            f.write(f'Feature: {feature}\\n')\n",
    "            for name, res in results.items():\n",
    "                f.write(f'  {name}: KS stat = {res[\"ks_stat\"]:.4f}, p-value = {res[\"p_value\"]:.4f}\\n')\n",
    "\n",
    "# Function from evaluation_metrics.ipynb: Feature-temporal correlation\n",
    "def feature_temporal_correlation(real_df, synthetic_dfs, synthetic_names, feature_names, output_dir, max_hours):\n",
    "    filtered_real = filter_by_hours(real_df, max_hours)\n",
    "    synthetic_dfs_filtered = [filter_by_hours(df, max_hours) for df in synthetic_dfs]\n",
    "    \n",
    "    # Set the number of plots and figure size\n",
    "    num_plots = len(synthetic_dfs) + 1\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(5 * num_plots, 5), gridspec_kw={'wspace': 0.4})\n",
    "    \n",
    "    for ax, df, title in zip(axes, [filtered_real] + synthetic_dfs_filtered, ['Real'] + synthetic_names):\n",
    "        corr = df[feature_names].corr()\n",
    "        sns.heatmap(corr, annot=True, cmap='coolwarm', ax=ax, square=True, cbar=False)\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    # Adjust layout and save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'temporal_correlation.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Function from evaluation_metrics.ipynb: Feature cross-correlation difference\n",
    "def feature_cross_correlation_difference(real_df, *synthetic_dfs, feature_names, output_dir, max_hours):\n",
    "    filtered_real = filter_by_hours(real_df, max_hours)\n",
    "    synthetic_dfs_filtered = [filter_by_hours(df, max_hours) for df in synthetic_dfs]\n",
    "    \n",
    "    real_corr = filtered_real[feature_names].corr()\n",
    "    corr_diff_summary = []\n",
    "    \n",
    "    for i, (synth_df, name) in enumerate(zip(synthetic_dfs_filtered, [key for key in dataframes.keys() if key not in ['mimic_iv']])):\n",
    "        synth_corr = synth_df[feature_names].corr()\n",
    "        corr_diff = np.abs(real_corr - synth_corr)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(corr_diff, annot=True, cmap='coolwarm')\n",
    "        plt.title(f'Correlation Difference: Real vs {name}')\n",
    "        plt.savefig(os.path.join(output_dir, f'corr_diff_{name}.png'))\n",
    "        plt.close()\n",
    "\n",
    "        mean_diff = corr_diff.values.mean()\n",
    "        max_diff = corr_diff.values.max()\n",
    "        corr_diff_summary.append({'Method': f'{name} vs Real', 'Mean Abs Diff': mean_diff, 'Max Abs Diff': max_diff})\n",
    "        print(f\"{name} - Real mean absolute correlation difference: {mean_diff:.4f}\")\n",
    "    \n",
    "    summary_df = pd.DataFrame(corr_diff_summary)\n",
    "    summary_df.to_csv(os.path.join(output_dir, 'corr_diff_summary.csv'))\n",
    "    print(\"\\nCorrelation Difference Summary:\\n\", summary_df)\n",
    "    return summary_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all dataframes\n",
    "filtered_dataframes = {name: filter_by_hours(df, max_hours) for name, df in dataframes.items()}\n",
    "print(f\"Filtered dataframes for max_hours={max_hours}:\\n{filtered_dataframes}\")\n",
    "\n",
    "# 1. Visualize feature distributions\n",
    "plot_feature_distributions(dataframes, numerical_features, categorical_features, output_dir, max_hours)\n",
    "print(\"Feature distributions plotted and saved.\")\n",
    "\n",
    "# 2. Compute summary statistics\n",
    "compute_summary_stats(dataframes, all_features, output_dir, max_hours)\n",
    "print(\"Summary statistics computed and saved.\")\n",
    "\n",
    "# 3. Compute KS tests\n",
    "compute_ks_tests(dataframes, common_features, output_dir, max_hours)\n",
    "print(\"KS tests computed and saved.\")\n",
    "\n",
    "# 4. Run evaluation metric functions from evaluation_metrics.ipynb\n",
    "real_df = dataframes['mimic_iv']\n",
    "synthetic_dfs = [dataframes['physician_policy'], dataframes['health_gym'], dataframes['gan'], dataframes['sdv_par']]\n",
    "synthetic_names = ['Naive Agent', 'Health Gym', 'GAN', 'SDV PAR']\n",
    "# synthetic_dfs = [dataframes['gan'], dataframes['sdv_par'], dataframes['physician_policy'], dataframes['health_gym']]\n",
    "feature_names = common_features  # Use only common features for consistency\n",
    "\n",
    "print(\"\\nGenerating feature-temporal correlation heatmaps...\")\n",
    "feature_temporal_correlation(real_df, synthetic_dfs, synthetic_names, feature_names=feature_names, output_dir=output_dir, max_hours=max_hours)\n",
    "\n",
    "print(\"\\nGenerating correlation difference heatmaps...\")\n",
    "feature_cross_correlation_difference(real_df, *synthetic_dfs, feature_names=feature_names, output_dir=output_dir, max_hours=max_hours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='This figure includes Axes that are not compatible with tight_layout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = dataframes['mimic_iv']\n",
    "# synthetic_dfs = [dataframes['naive_agent'], dataframes['health_gym'], dataframes['gan'], dataframes['sdv_par']]\n",
    "# synthetic_names = ['Naive Agent', 'Health Gym', 'ehrMGAN', 'SDV PAR']\n",
    "synthetic_dfs = [dataframes['mimic_iii'], dataframes['eICU']]\n",
    "synthetic_names = ['MIMIC III', 'eICU']\n",
    "\n",
    "feature_names = common_features  # Use only common features for consistency\n",
    "\n",
    "print(\"\\nGenerating feature-temporal correlation heatmaps...\")\n",
    "feature_temporal_correlation(real_df, synthetic_dfs, synthetic_names, feature_names=feature_names, output_dir=output_dir, max_hours=max_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the unique values of respiratory_rate_set in all the dataframes\n",
    "for name, df in dataframes.items():\n",
    "    if 'respiratory_rate_set' in df.columns:\n",
    "        print(f\"{name}: {len(df['respiratory_rate_set'].unique())}\")\n",
    "        # print(f\"{name}: {df['respiratory_rate_set'].min()}\")\n",
    "        # print(f\"{name}: {df['respiratory_rate_set'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up matplotlib\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Configuration\n",
    "output_dir = \"../data\"  # Adjust as needed\n",
    "evaluation_dir = f\"{output_dir}/evaluations_v2/\"\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "max_hours = 20  # Set to 20 for consistency with health_gym_raw_df; adjustable\n",
    "feature_cols = ['heart_rate', 'resp_rate', 'spo2', 'fio2', 'respiratory_rate_set']  # Common time-varying features\n",
    "feature_names = ['Heart Rate', 'Resp. Rate', 'SpO2', 'FiO2', 'RR Set']  # For plotting\n",
    "\n",
    "# Dataframes dictionary\n",
    "dataframes = {\n",
    "    'mimic_iv': mimic_iv_df,\n",
    "    'mimic_iii': mimic_iii_df,\n",
    "    'eICU': eICU_df,\n",
    "    'physician_policy': naive_agent_generated_df,\n",
    "    'health_gym': health_gym_df,\n",
    "    'sdv_par': sdv_par_df,\n",
    "    'gan': gan_df\n",
    "}\n",
    "\n",
    "# Helper function to convert dataframe to 3D array\n",
    "def df_to_array(df, feature_cols=feature_cols, max_hours=max_hours):\n",
    "    patient_ids = df['stay_id'].unique()\n",
    "    n_patients = len(patient_ids)\n",
    "    n_features = len(feature_cols)\n",
    "    \n",
    "    patient_id_to_idx = {id: i for i, id in enumerate(patient_ids)}\n",
    "    array_3d = np.zeros((n_patients, max_hours, n_features))\n",
    "    array_3d.fill(np.nan)\n",
    "    \n",
    "    df_filtered = df[df['hours_in'] < max_hours]\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        patient_idx = patient_id_to_idx[row['stay_id']]\n",
    "        hour = int(row['hours_in'])\n",
    "        features = row[feature_cols].values\n",
    "        array_3d[patient_idx, hour, :] = features\n",
    "    \n",
    "    # Forward-fill missing values\n",
    "    for p in range(n_patients):\n",
    "        for f in range(n_features):\n",
    "            last_valid = None\n",
    "            for h in range(max_hours):\n",
    "                if np.isnan(array_3d[p, h, f]) and last_valid is not None:\n",
    "                    array_3d[p, h, f] = last_valid\n",
    "                elif not np.isnan(array_3d[p, h, f]):\n",
    "                    last_valid = array_3d[p, h, f]\n",
    "    \n",
    "    return array_3d\n",
    "\n",
    "# Convert all dataframes to 3D arrays\n",
    "arrays = {name: df_to_array(df) for name, df in dataframes.items()}\n",
    "real_array = arrays['mimic_iv']  # Reference real dataset\n",
    "\n",
    "# 1. Visualize feature distributions\n",
    "def plot_feature_distributions(dataframes, feature_cols, max_hours=max_hours):\n",
    "    filtered_dfs = {name: df[df['hours_in'] < max_hours] for name, df in dataframes.items()}\n",
    "    all_features = feature_cols + ['gender_M', 'age']\n",
    "    \n",
    "    for feature in all_features:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        if feature in ['gender_M']:  # Categorical\n",
    "            counts = {}\n",
    "            for name, df in filtered_dfs.items():\n",
    "                if feature in df.columns:\n",
    "                    counts[name] = df[feature].value_counts(normalize=True)\n",
    "            if counts:\n",
    "                count_df = pd.DataFrame(counts).T\n",
    "                count_df.plot(kind='bar', ax=plt.gca())\n",
    "                plt.title(f'Distribution of {feature}')\n",
    "                plt.xlabel('Dataset')\n",
    "                plt.ylabel('Proportion')\n",
    "                plt.legend(['Female', 'Male'])\n",
    "        else:  # Numerical\n",
    "            for name, df in filtered_dfs.items():\n",
    "                if feature in df.columns:\n",
    "                    sns.kdeplot(df[feature], label=name, warn_singular=False)\n",
    "            plt.title(f'Distribution of {feature}')\n",
    "            plt.xlabel(feature)\n",
    "            plt.ylabel('Density')\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{evaluation_dir}/{feature}_distribution.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "plot_feature_distributions(dataframes, feature_cols)\n",
    "\n",
    "# 2.1 Max Mean Discrepancy (MMD)\n",
    "def calculate_mmd(real_data, synthetic_data, bandwidth_multipliers=[0.2, 0.5, 0.9, 1.3]):\n",
    "    from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "    # Flatten data to 2D\n",
    "    real_flat = real_data.reshape(real_data.shape[0], -1)\n",
    "    synth_flat = synthetic_data.reshape(synthetic_data.shape[0], -1)\n",
    "\n",
    "    # Remove rows with NaN values\n",
    "    real_flat = real_flat[~np.isnan(real_flat).any(axis=1)]\n",
    "    synth_flat = synth_flat[~np.isnan(synth_flat).any(axis=1)]\n",
    "\n",
    "    # Sample if too large (for computational efficiency)\n",
    "    max_samples = 1000\n",
    "    if len(real_flat) > max_samples:\n",
    "        indices = np.random.choice(len(real_flat), max_samples, replace=False)\n",
    "        real_flat = real_flat[indices]\n",
    "    if len(synth_flat) > max_samples:\n",
    "        indices = np.random.choice(len(synth_flat), max_samples, replace=False)\n",
    "        synth_flat = synth_flat[indices]\n",
    "\n",
    "    # Calculate median bandwidth using median heuristic\n",
    "    X = np.vstack([real_flat, synth_flat])\n",
    "    median_dist = np.median(np.sqrt(np.sum((X[:, None, :] - X[None, :, :]) ** 2, axis=-1)))\n",
    "\n",
    "    # Handle case where median_dist is 0 or NaN\n",
    "    if np.isnan(median_dist) or median_dist == 0:\n",
    "        raise ValueError(\"Median distance is NaN or zero. Check your data for invalid values.\")\n",
    "\n",
    "    mmd_values = []\n",
    "    for bandwidth_multiplier in bandwidth_multipliers:\n",
    "        bandwidth = bandwidth_multiplier * median_dist\n",
    "\n",
    "        # Calculate kernel matrices\n",
    "        K_XX = rbf_kernel(real_flat, real_flat, gamma=1.0 / (2 * bandwidth**2))\n",
    "        K_YY = rbf_kernel(synth_flat, synth_flat, gamma=1.0 / (2 * bandwidth**2))\n",
    "        K_XY = rbf_kernel(real_flat, synth_flat, gamma=1.0 / (2 * bandwidth**2))\n",
    "\n",
    "        # Calculate MMD\n",
    "        mmd = np.mean(K_XX) - 2 * np.mean(K_XY) + np.mean(K_YY)\n",
    "        # mmd_values.append(max(0, mmd))  # MMD should be non-negative\n",
    "        mmd_values.append(max(0, np.sqrt(mmd)))  # MMD should be non-negative\n",
    "\n",
    "    # Return average MMD across bandwidths\n",
    "    return np.mean(mmd_values)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "# 2.1 Max Mean Discrepancy (MMD) [update] aligned with transition level evaluation\n",
    "\"\"\" def calculate_mmd(real_data, synthetic_data, bandwidth_multipliers=[0.2, 0.5, 0.9, 1.3]):\n",
    "    \n",
    "    # Calculate Maximum Mean Discrepancy (MMD) between real_data and synthetic_data\n",
    "    # using an unbiased estimator and Gaussian kernel, similar to MMD_evaluation.\n",
    "   \n",
    "    # Flatten data to 2D\n",
    "    real_flat = real_data.reshape(real_data.shape[0], -1)\n",
    "    synth_flat = synthetic_data.reshape(synthetic_data.shape[0], -1)\n",
    "\n",
    "    # Remove rows with NaN values\n",
    "    real_flat = real_flat[~np.isnan(real_flat).any(axis=1)]\n",
    "    synth_flat = synth_flat[~np.isnan(synth_flat).any(axis=1)]\n",
    "\n",
    "    # Sample if too large (for computational efficiency)\n",
    "    max_samples = 1000\n",
    "    if len(real_flat) > max_samples:\n",
    "        indices = np.random.choice(len(real_flat), max_samples, replace=False)\n",
    "        real_flat = real_flat[indices]\n",
    "    if len(synth_flat) > max_samples:\n",
    "        indices = np.random.choice(len(synth_flat), max_samples, replace=False)\n",
    "        synth_flat = synth_flat[indices]\n",
    "\n",
    "    # Compute squared distances\n",
    "    xx = cdist(real_flat, real_flat, 'euclidean') ** 2\n",
    "    yy = cdist(synth_flat, synth_flat, 'euclidean') ** 2\n",
    "    xy = cdist(real_flat, synth_flat, 'euclidean') ** 2\n",
    "\n",
    "    # Median heuristic for bandwidth\n",
    "    all_states = np.vstack([real_flat, synth_flat])\n",
    "    all_distances = cdist(all_states, all_states, 'euclidean')\n",
    "    sigma = np.median(all_distances[all_distances > 0])\n",
    "    if np.isnan(sigma) or sigma == 0:\n",
    "        raise ValueError(\"Median distance is NaN or zero. Check your data for invalid values.\")\n",
    "\n",
    "    mmd_values = []\n",
    "    for bandwidth_multiplier in bandwidth_multipliers:\n",
    "        bandwidth = bandwidth_multiplier * sigma\n",
    "        # Gaussian kernel\n",
    "        k_xx = np.exp(-xx / (2 * bandwidth ** 2))\n",
    "        k_yy = np.exp(-yy / (2 * bandwidth ** 2))\n",
    "        k_xy = np.exp(-xy / (2 * bandwidth ** 2))\n",
    "\n",
    "        m, n = len(real_flat), len(synth_flat)\n",
    "        # Unbiased MMD^2\n",
    "        if m > 1:\n",
    "            term1 = (np.sum(k_xx) - np.trace(k_xx)) / (m * (m - 1))\n",
    "        else:\n",
    "            term1 = 0\n",
    "        if n > 1:\n",
    "            term2 = (np.sum(k_yy) - np.trace(k_yy)) / (n * (n - 1))\n",
    "        else:\n",
    "            term2 = 0\n",
    "        term3 = np.sum(k_xy) / (m * n)\n",
    "        mmd = np.sqrt(max(0, term1 + term2 - 2 * term3)) # with sqrt for consistency with MMD_evaluation\n",
    "        # mmd = max(0, term1 + term2 - 2 * term3) # without sqrt\n",
    "        mmd_values.append(mmd)\n",
    "\n",
    "    return float(np.mean(mmd_values)) \"\"\"\n",
    "\n",
    "mmd_results = {}\n",
    "for name, array in arrays.items():\n",
    "    if name != 'mimic_iv':\n",
    "        mmd_results[name] = calculate_mmd(real_array, array)\n",
    "print(\"MMD results:\")\n",
    "for name, mmd in mmd_results.items():\n",
    "    print(f\"{name}: {mmd:.6f}\")\n",
    "\n",
    "# 2.2 Pearson Correlation\n",
    "def compute_correlation_matrix(data_array):\n",
    "    flat_data = data_array.reshape(-1, data_array.shape[-1])\n",
    "    return np.corrcoef(flat_data.T)\n",
    "\n",
    "real_corr = compute_correlation_matrix(real_array)\n",
    "corr_diffs = {}\n",
    "for name, array in arrays.items():\n",
    "    if name != 'mimic_iv':\n",
    "        synth_corr = compute_correlation_matrix(array)\n",
    "        diff = synth_corr - real_corr\n",
    "        corr_diffs[name] = np.linalg.norm(diff)\n",
    "\n",
    "print(\"\\nCorrelation difference (Frobenius norm):\")\n",
    "for name, diff in corr_diffs.items():\n",
    "    print(f\"{name}: {diff:.4f}\")\n",
    "\n",
    "# Visualize correlation for selected datasets\n",
    "# selected_datasets = {'mimic_iv': real_array, 'naive_agent_df': arrays['naive_agent'], 'health_gym_df': arrays['health_gym'], 'gan': arrays['gan'], 'sdv_par': arrays['sdv_par']}\n",
    "selected_datasets = {'mimic_iv': real_array, 'naive_agent_df': arrays['physician_policy'], 'health_gym_df': arrays['health_gym'], 'gan': arrays['gan'], 'sdv_par': arrays['sdv_par']}\n",
    "fig, axes = plt.subplots(1, len(selected_datasets), figsize=(6*len(selected_datasets), 5))\n",
    "for ax, (name, array) in zip(axes, selected_datasets.items()):\n",
    "    corr = compute_correlation_matrix(array)\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, ax=ax, xticklabels=feature_names, yticklabels=feature_names)\n",
    "    ax.set_title(f'Correlation - {name}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{evaluation_dir}/correlation_selected.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 2.3 Dimension-wise Probability\n",
    "dim_stats = {}\n",
    "real_means = np.nanmean(real_array, axis=(0,1))\n",
    "for name, array in arrays.items():\n",
    "    if name != 'mimic_iv':\n",
    "        synth_means = np.nanmean(array, axis=(0,1))\n",
    "        cc = np.corrcoef(real_means, synth_means)[0,1]\n",
    "        rmse = sqrt(mean_squared_error(real_means, synth_means))\n",
    "        dim_stats[name] = {'cc': cc, 'rmse': rmse}\n",
    "\n",
    "print(\"\\nDimension-wise statistics:\")\n",
    "for name, stats in dim_stats.items():\n",
    "    print(f\"{name}: CC={stats['cc']:.4f}, RMSE={stats['rmse']:.4f}\")\n",
    "\n",
    "# Plot means\n",
    "means = {name: np.nanmean(array, axis=(0,1)) for name, array in arrays.items()}\n",
    "x = np.arange(len(feature_cols))\n",
    "width = 0.1\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for i, (name, mean) in enumerate(means.items()):\n",
    "    offset = width * (i - len(means)/2)\n",
    "    ax.bar(x + offset, mean, width, label=name)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(feature_names, rotation=45)\n",
    "ax.set_ylabel('Mean Value')\n",
    "ax.set_title('Mean Feature Values Comparison')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{evaluation_dir}/feature_means.png\", dpi=300)\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Discriminative Score\n",
    "def train_discriminator(real_data, synthetic_data):\n",
    "    \"\"\"\n",
    "    Train a discriminator to distinguish between real and synthetic data.\n",
    "    Returns the AUC and Average Precision scores.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "    # Combine real and synthetic data\n",
    "    real_labels = np.ones(len(real_data))\n",
    "    synthetic_labels = np.zeros(len(synthetic_data))\n",
    "    X = np.vstack([real_data, synthetic_data])\n",
    "    y = np.hstack([real_labels, synthetic_labels])\n",
    "\n",
    "    # Flatten the input data (combine time steps and features)\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build a simple neural network discriminator\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_pred = model.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "    # Remove NaN values from y_test and y_pred\n",
    "    valid_indices = ~np.isnan(y_pred)\n",
    "    y_test = y_test[valid_indices]\n",
    "    y_pred = y_pred[valid_indices]\n",
    "\n",
    "    # Check if arrays are empty\n",
    "    if len(y_test) == 0 or len(y_pred) == 0:\n",
    "        print(\"Warning: No valid samples after filtering NaN values.\")\n",
    "        return float('nan'), float('nan')\n",
    "\n",
    "    # Calculate AUC and Average Precision\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    apr = average_precision_score(y_test, y_pred)\n",
    "    return auc, apr\n",
    "\n",
    "disc_scores = {}\n",
    "for name, array in arrays.items():\n",
    "    if name not in ['mimic_iv']:\n",
    "    # if name not in ['mimic_iv', 'mimic_iii', 'eICU']:\n",
    "        print(f\"\\nTraining discriminator for {name}...\")\n",
    "        auc, apr = train_discriminator(real_array, array)\n",
    "        disc_scores[name] = {'auc': auc, 'apr': apr}\n",
    "print(\"\\nDiscriminative scores:\")\n",
    "for name, scores in disc_scores.items():\n",
    "    print(f\"{name}: AUC={scores['auc']:.4f}, APR={scores['apr']:.4f}\")\n",
    "\n",
    "# 2.5 Visualization Functions\n",
    "def visualize_trajectories(data_dict, feature_names=feature_names, n_patients=5):\n",
    "    n_features = len(feature_names)\n",
    "    n_datasets = len(data_dict)\n",
    "    fig, axes = plt.subplots(n_features, n_datasets, figsize=(6*n_datasets, 4*n_features))\n",
    "    axes = np.atleast_2d(axes)\n",
    "    \n",
    "    for col, (name, data) in enumerate(data_dict.items()):\n",
    "        patients = np.random.choice(data.shape[0], n_patients, replace=False)\n",
    "        for row in range(n_features):\n",
    "            ax = axes[row, col]\n",
    "            for p in patients:\n",
    "                ax.plot(data[p, :, row], alpha=0.7)\n",
    "            ax.set_title(f'{feature_names[row]} - {name}')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{evaluation_dir}/trajectories.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "visualize_trajectories(selected_datasets)\n",
    "\n",
    "def visualize_dim_reduction(data_dict):\n",
    "    combined = []\n",
    "    labels = []\n",
    "    for i, (name, data) in enumerate(data_dict.items()):\n",
    "        flat = data.reshape(data.shape[0], -1)\n",
    "        max_samples = 1000\n",
    "        if len(flat) > max_samples:\n",
    "            indices = np.random.choice(len(flat), max_samples, replace=False)\n",
    "            flat = flat[indices]\n",
    "        combined.append(flat)\n",
    "        labels.append(np.full(len(flat), i))\n",
    "    \n",
    "    combined = np.vstack(combined)\n",
    "    labels = np.concatenate(labels)\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embedded = tsne.fit_transform(combined)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(embedded[:, 0], embedded[:, 1], c=labels, cmap='viridis', alpha=0.7, s=5)\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=list(data_dict.keys()))\n",
    "    plt.title('t-SNE Visualization')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{evaluation_dir}/tsne.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "visualize_dim_reduction(selected_datasets)\n",
    "\n",
    "# 2.6 Feature Temporal Correlation\n",
    "def feature_temporal_correlation(data_dict, feature_names=feature_names):\n",
    "    fig, axes = plt.subplots(1, len(data_dict), figsize=(7*len(data_dict), 7))\n",
    "    axes = np.atleast_1d(axes)\n",
    "    \n",
    "    timepoints = np.linspace(0, max_hours-1, 6, dtype=int)\n",
    "    for ax, (name, data) in zip(axes, data_dict.items()):\n",
    "        selected_data = []\n",
    "        combined_names = []\n",
    "        for i, feature in enumerate(feature_names):\n",
    "            for tp in timepoints:\n",
    "                selected_data.append(data[:, tp, i])\n",
    "                combined_names.append(f\"{feature}_{tp:02d}\")\n",
    "        \n",
    "        selected_data = np.array(selected_data).T\n",
    "        df = pd.DataFrame(data=selected_data, columns=combined_names)\n",
    "        corr_matrix = df.corr()\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, ax=ax, mask=mask, cmap=\"coolwarm\", vmin=-1, vmax=1, center=0)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=75)\n",
    "        ax.set_title(f\"{name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{evaluation_dir}/temporal_correlation.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "feature_temporal_correlation(selected_datasets)\n",
    "\n",
    "# Summary\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'Dataset': list(mmd_results.keys()),\n",
    "    'MMD': list(mmd_results.values()),\n",
    "    'Corr_Diff': [corr_diffs[name] for name in mmd_results.keys()],\n",
    "    'Dim_CC': [dim_stats[name]['cc'] for name in mmd_results.keys()],\n",
    "    'Dim_RMSE': [dim_stats[name]['rmse'] for name in mmd_results.keys()],\n",
    "    'Disc_AUC': [disc_scores.get(name, {'auc': np.nan})['auc'] for name in mmd_results.keys()],\n",
    "    'Disc_APR': [disc_scores.get(name, {'apr': np.nan})['apr'] for name in mmd_results.keys()]\n",
    "})\n",
    "print(\"\\nSummary of Evaluation Metrics:\")\n",
    "print(metrics_summary)\n",
    "metrics_summary.to_csv(f\"{evaluation_dir}/metrics_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mean_variance(data_dict, feature_names=feature_names, max_hours=20):\n",
    "    \"\"\"\n",
    "    Visualize the mean and variance of features over the first max_hours timestamps for each dataset.\n",
    "\n",
    "    Args:\n",
    "        data_dict (dict): Dictionary of datasets (3D arrays) with dataset names as keys.\n",
    "        feature_names (list): List of feature names corresponding to the last dimension of the arrays.\n",
    "        max_hours (int): Number of timestamps to consider (e.g., 20).\n",
    "    \"\"\"\n",
    "    n_features = len(feature_names)\n",
    "    n_datasets = len(data_dict)\n",
    "    fig, axes = plt.subplots(n_features, 2, figsize=(12, 4 * n_features))  # 2 columns: mean and variance\n",
    "\n",
    "    for row, feature_idx in enumerate(range(n_features)):\n",
    "        for col, stat in enumerate(['Mean', 'Variance']):\n",
    "            ax = axes[row, col] if n_features > 1 else axes[col]\n",
    "            for name, data in data_dict.items():\n",
    "                # Compute mean or variance over the first max_hours timestamps\n",
    "                if stat == 'Mean':\n",
    "                    values = np.nanmean(data[:, :max_hours, feature_idx], axis=0)\n",
    "                elif stat == 'Variance':\n",
    "                    values = np.nanvar(data[:, :max_hours, feature_idx], axis=0)\n",
    "\n",
    "                # Plot the computed values\n",
    "                ax.plot(range(max_hours), values, label=name, alpha=0.8)\n",
    "\n",
    "            ax.set_title(f'{feature_names[feature_idx]} - {stat}')\n",
    "            ax.set_xlabel('Time (hours)')\n",
    "            ax.set_ylabel(stat)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{evaluation_dir}/mean_variance.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Visualize mean and variance for the first 20 hours\n",
    "visualize_mean_variance(selected_datasets, feature_names=feature_names, max_hours=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up matplotlib\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create output directory for evaluation results\n",
    "evaluation_dir = f\"{output_dir}/evaluations/\"\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "# Helper function to convert dataframe to 3D array for comparison\n",
    "def df_to_array(df, feature_cols=None):\n",
    "    \"\"\"Convert DataFrame to 3D numpy array (n_patients, n_timesteps, n_features)\"\"\"\n",
    "    if feature_cols is None:\n",
    "        feature_cols = ['heart_rate', 'resp_rate', 'spo2', 'fio2', 'respiratory_rate_set']\n",
    "    \n",
    "    # Get unique patient IDs\n",
    "    patient_ids = df['stay_id'].unique()\n",
    "    n_patients = len(patient_ids)\n",
    "    n_features = len(feature_cols)\n",
    "    \n",
    "    # Create mapping from stay_id to array index\n",
    "    patient_id_to_idx = {id: i for i, id in enumerate(patient_ids)}\n",
    "    \n",
    "    # Initialize output array\n",
    "    array_3d = np.zeros((n_patients, 24, n_features))\n",
    "    array_3d.fill(np.nan)\n",
    "    \n",
    "    # Fill the array with values\n",
    "    for _, row in df.iterrows():\n",
    "        patient_idx = patient_id_to_idx[row['stay_id']]\n",
    "        hour = int(row['hours_in'])\n",
    "        \n",
    "        # Skip if hour is outside range\n",
    "        if hour < 0 or hour >= 24:\n",
    "            continue\n",
    "            \n",
    "        # Extract features\n",
    "        features = row[feature_cols].values\n",
    "        \n",
    "        # Fill the array at the correct position\n",
    "        array_3d[patient_idx, hour, :] = features\n",
    "    \n",
    "    # Forward-fill missing values\n",
    "    for p in range(n_patients):\n",
    "        for f in range(n_features):\n",
    "            last_valid = None\n",
    "            for h in range(24):\n",
    "                if np.isnan(array_3d[p, h, f]) and last_valid is not None:\n",
    "                    array_3d[p, h, f] = last_valid\n",
    "                elif not np.isnan(array_3d[p, h, f]):\n",
    "                    last_valid = array_3d[p, h, f]\n",
    "    \n",
    "    return array_3d\n",
    "\n",
    "# Convert the dataframe to 3D array for comparison\n",
    "real_array = df_to_array(baseline_charttime_ground_truth_first_24_hr_df) # TODO: not sure use all or just first 24 hours\n",
    "print(f\"Real data array shape: {real_array.shape}\")\n",
    "\n",
    "# 1. MAX MEAN DISCREPANCY (MMD)\n",
    "def calculate_mmd(real_data, synthetic_data, bandwidth_multipliers=[0.2, 0.5, 0.9, 1.3]):\n",
    "    \"\"\"\n",
    "    Calculate Maximum Mean Discrepancy using sklearn implementation\n",
    "    \n",
    "    Args:\n",
    "        real_data: Real data array (n_samples, n_features)\n",
    "        synthetic_data: Synthetic data array (n_samples, n_features)\n",
    "        bandwidth_multipliers: List of multipliers for the median heuristic\n",
    "        \n",
    "    Returns:\n",
    "        MMD value\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import rbf_kernel\n",
    "    \n",
    "    # Flatten data to 2D\n",
    "    real_flat = real_data.reshape(real_data.shape[0], -1)\n",
    "    synth_flat = synthetic_data.reshape(synthetic_data.shape[0], -1)\n",
    "    \n",
    "    # Sample if too large (for computational efficiency)\n",
    "    max_samples = 1000\n",
    "    if len(real_flat) > max_samples:\n",
    "        indices = np.random.choice(len(real_flat), max_samples, replace=False)\n",
    "        real_flat = real_flat[indices]\n",
    "    \n",
    "    if len(synth_flat) > max_samples:\n",
    "        indices = np.random.choice(len(synth_flat), max_samples, replace=False)\n",
    "        synth_flat = synth_flat[indices]\n",
    "    \n",
    "    # Calculate median bandwidth using median heuristic\n",
    "    X = np.vstack([real_flat, synth_flat])\n",
    "    median_dist = np.median(np.sqrt(np.sum((X[:, None, :] - X[None, :, :]) ** 2, axis=-1)))\n",
    "    \n",
    "    mmd_values = []\n",
    "    for bandwidth_multiplier in bandwidth_multipliers:\n",
    "        bandwidth = bandwidth_multiplier * median_dist\n",
    "        \n",
    "        # Calculate kernel matrices\n",
    "        K_XX = rbf_kernel(real_flat, real_flat, gamma=1.0/(2*bandwidth**2))\n",
    "        K_YY = rbf_kernel(synth_flat, synth_flat, gamma=1.0/(2*bandwidth**2))\n",
    "        K_XY = rbf_kernel(real_flat, synth_flat, gamma=1.0/(2*bandwidth**2))\n",
    "        \n",
    "        # Calculate MMD\n",
    "        mmd = np.mean(K_XX) - 2 * np.mean(K_XY) + np.mean(K_YY)\n",
    "        mmd_values.append(max(0, mmd))  # MMD should be non-negative\n",
    "    \n",
    "    # Return average MMD across bandwidths\n",
    "    return np.mean(mmd_values)\n",
    "\n",
    "# Calculate MMD for GAN and TABLE\n",
    "mmd_gan = calculate_mmd(real_array, gan_renormalized)\n",
    "mmd_table = calculate_mmd(real_array, table_renormalized)\n",
    "\n",
    "print(f\"MMD (GAN): {mmd_gan:.6f}\")\n",
    "print(f\"MMD (TABLE): {mmd_table:.6f}\")\n",
    "\n",
    "# 2. PEARSON CORRELATION \n",
    "def pearson_correlation_comparison(real_data, gan_data, table_data, feature_names=None):\n",
    "    \"\"\"\n",
    "    Compare correlation matrices between real, GAN, and TABLE data\n",
    "    \n",
    "    Args:\n",
    "        real_data: 3D array of real data\n",
    "        gan_data: 3D array of GAN-generated data\n",
    "        table_data: 3D array of TABLE-generated data\n",
    "        feature_names: List of feature names\n",
    "    \"\"\"\n",
    "    if feature_names is None:\n",
    "        feature_names = ['HR', 'RR', 'SpO2', 'FiO2', 'RR_set']\n",
    "    \n",
    "    # Flatten data for correlation calculation\n",
    "    # We'll use the first 1000 samples if available\n",
    "    real_flat = real_data.reshape(-1, real_data.shape[-1])\n",
    "    gan_flat = gan_data.reshape(-1, gan_data.shape[-1])\n",
    "    table_flat = table_data.reshape(-1, table_data.shape[-1])\n",
    "    \n",
    "    # Calculate correlation matrices\n",
    "    real_corr = np.corrcoef(real_flat.T)\n",
    "    gan_corr = np.corrcoef(gan_flat.T)\n",
    "    table_corr = np.corrcoef(table_flat.T)\n",
    "    \n",
    "    # Calculate differences\n",
    "    gan_diff = gan_corr - real_corr\n",
    "    table_diff = table_corr - real_corr\n",
    "    \n",
    "    # Calculate Frobenius norm of difference matrices\n",
    "    gan_frob = np.linalg.norm(gan_diff)\n",
    "    table_frob = np.linalg.norm(table_diff)\n",
    "    \n",
    "    print(f\"Correlation difference (Frobenius norm) - GAN: {gan_frob:.4f}, TABLE: {table_frob:.4f}\")\n",
    "    \n",
    "    # Create correlation plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Real data correlation\n",
    "    sns.heatmap(real_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, \n",
    "                xticklabels=feature_names, yticklabels=feature_names, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Real Data Correlation')\n",
    "    \n",
    "    # GAN data correlation\n",
    "    sns.heatmap(gan_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, \n",
    "                xticklabels=feature_names, yticklabels=feature_names, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('GAN Data Correlation')\n",
    "    \n",
    "    # TABLE data correlation\n",
    "    sns.heatmap(table_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, \n",
    "                xticklabels=feature_names, yticklabels=feature_names, ax=axes[0, 2])\n",
    "    axes[0, 2].set_title('TABLE Data Correlation')\n",
    "    \n",
    "    # GAN difference\n",
    "    sns.heatmap(gan_diff, annot=True, cmap='coolwarm', vmin=-0.5, vmax=0.5, center=0, \n",
    "                xticklabels=feature_names, yticklabels=feature_names, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('GAN - Real Difference')\n",
    "    \n",
    "    # TABLE difference\n",
    "    sns.heatmap(table_diff, annot=True, cmap='coolwarm', vmin=-0.5, vmax=0.5, center=0, \n",
    "                xticklabels=feature_names, yticklabels=feature_names, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('TABLE - Real Difference')\n",
    "    \n",
    "    # GAN vs TABLE difference\n",
    "    gan_table_diff = gan_corr - table_corr\n",
    "    sns.heatmap(gan_table_diff, annot=True, cmap='coolwarm', vmin=-0.5, vmax=0.5, center=0, \n",
    "                xticklabels=feature_names, yticklabels=feature_names, ax=axes[1, 2])\n",
    "    axes[1, 2].set_title('GAN - TABLE Difference')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{evaluation_dir}/correlation_comparison.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return real_corr, gan_corr, table_corr\n",
    "\n",
    "# Run correlation analysis\n",
    "feature_names = ['Heart Rate', 'Resp. Rate', 'SpO2', 'FiO2', 'RR Set']\n",
    "real_corr, gan_corr, table_corr = pearson_correlation_comparison(\n",
    "    real_array, gan_renormalized, table_renormalized, feature_names)\n",
    "\n",
    "# 3. DIMENSION-WISE PROBABILITY\n",
    "def dimension_wise_probability(real_data, gan_data, table_data, feature_names=None):\n",
    "    \"\"\"\n",
    "    Calculate and plot dimension-wise statistics\n",
    "    \"\"\"\n",
    "    if feature_names is None:\n",
    "        feature_names = ['Heart Rate', 'Resp. Rate', 'SpO2', 'FiO2', 'RR Set']\n",
    "    \n",
    "    n_features = real_data.shape[2]\n",
    "    \n",
    "    # Calculate means and stds for each feature over time\n",
    "    real_means = np.nanmean(real_data, axis=(0, 1))  # Average over patients and time steps\n",
    "    gan_means = np.nanmean(gan_data, axis=(0, 1))\n",
    "    table_means = np.nanmean(table_data, axis=(0, 1))\n",
    "    \n",
    "    real_stds = np.nanstd(real_data, axis=(0, 1))\n",
    "    gan_stds = np.nanstd(gan_data, axis=(0, 1))\n",
    "    table_stds = np.nanstd(table_data, axis=(0, 1))\n",
    "    \n",
    "    # Calculate temporal means\n",
    "    real_temporal_means = np.nanmean(real_data, axis=0)  # Average over patients only\n",
    "    gan_temporal_means = np.nanmean(gan_data, axis=0)\n",
    "    table_temporal_means = np.nanmean(table_data, axis=0)\n",
    "    \n",
    "    # Calculate correlation coefficients and RMSEs\n",
    "    gan_cc = np.corrcoef(real_means, gan_means)[0, 1]\n",
    "    table_cc = np.corrcoef(real_means, table_means)[0, 1]\n",
    "    \n",
    "    gan_rmse = sqrt(mean_squared_error(real_means, gan_means))\n",
    "    table_rmse = sqrt(mean_squared_error(real_means, table_means))\n",
    "    \n",
    "    # Plot feature means comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(feature_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, real_means, width, label='Real')\n",
    "    plt.bar(x, gan_means, width, label='GAN')\n",
    "    plt.bar(x + width, table_means, width, label='TABLE')\n",
    "    \n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Mean Value')\n",
    "    plt.title('Mean Feature Values Comparison')\n",
    "    plt.xticks(x, feature_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{evaluation_dir}/feature_means_comparison.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot temporal patterns for each feature\n",
    "    fig, axes = plt.subplots(n_features, 1, figsize=(12, 4*n_features))\n",
    "    for i in range(n_features):\n",
    "        axes[i].plot(real_temporal_means[:, i], 'b-', label='Real')\n",
    "        axes[i].plot(gan_temporal_means[:, i], 'r-', label='GAN')\n",
    "        axes[i].plot(table_temporal_means[:, i], 'g-', label='TABLE')\n",
    "        axes[i].set_title(f'{feature_names[i]} - Temporal Pattern')\n",
    "        axes[i].set_xlabel('Time Step')\n",
    "        axes[i].set_ylabel('Mean Value')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{evaluation_dir}/temporal_patterns.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Create scatter plot comparing real vs. synthetic means\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(real_means, gan_means, c=range(len(feature_names)), cmap='viridis')\n",
    "    for i in range(len(feature_names)):\n",
    "        plt.annotate(feature_names[i], (real_means[i], gan_means[i]))\n",
    "    plt.plot([min(real_means), max(real_means)], [min(real_means), max(real_means)], 'k--')\n",
    "    plt.title(f'GAN vs. Real (CC={gan_cc:.4f}, RMSE={gan_rmse:.4f})')\n",
    "    plt.xlabel('Real Data Mean')\n",
    "    plt.ylabel('GAN Data Mean')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(real_means, table_means, c=range(len(feature_names)), cmap='viridis')\n",
    "    for i in range(len(feature_names)):\n",
    "        plt.annotate(feature_names[i], (real_means[i], table_means[i]))\n",
    "    plt.plot([min(real_means), max(real_means)], [min(real_means), max(real_means)], 'k--')\n",
    "    plt.title(f'TABLE vs. Real (CC={table_cc:.4f}, RMSE={table_rmse:.4f})')\n",
    "    plt.xlabel('Real Data Mean')\n",
    "    plt.ylabel('TABLE Data Mean')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{evaluation_dir}/feature_scatter_comparison.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'gan_cc': gan_cc,\n",
    "        'table_cc': table_cc,\n",
    "        'gan_rmse': gan_rmse,\n",
    "        'table_rmse': table_rmse\n",
    "    }\n",
    "\n",
    "# Run dimension-wise probability analysis\n",
    "dim_stats = dimension_wise_probability(real_array, gan_renormalized, table_renormalized)\n",
    "\n",
    "# 4. DISCRIMINATIVE SCORE\n",
    "def train_discriminator(real_data, synthetic_data):\n",
    "    \"\"\"\n",
    "    Train a model to distinguish real from synthetic data\n",
    "    \n",
    "    Returns:\n",
    "        AUC, AP score and training history\n",
    "    \"\"\"\n",
    "    # Flatten data while preserving patient structure\n",
    "    real_flat = real_data.reshape(real_data.shape[0], -1)\n",
    "    syn_flat = synthetic_data.reshape(synthetic_data.shape[0], -1)\n",
    "    \n",
    "    # Combine and create labels\n",
    "    X = np.vstack([real_flat, syn_flat])\n",
    "    y = np.concatenate([np.zeros(len(real_flat)), np.ones(len(syn_flat))])\n",
    "    \n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    apr = average_precision_score(y_test, y_pred)\n",
    "    \n",
    "    return auc, apr, history\n",
    "\n",
    "# Calculate discriminative scores\n",
    "print(\"\\nTraining discriminator for GAN data...\")\n",
    "gan_auc, gan_apr, gan_history = train_discriminator(real_array, gan_renormalized)\n",
    "print(f\"GAN - AUC: {gan_auc:.4f}, APR: {gan_apr:.4f}\")\n",
    "\n",
    "print(\"\\nTraining discriminator for TABLE data...\")\n",
    "table_auc, table_apr, table_history = train_discriminator(real_array, table_renormalized)\n",
    "print(f\"TABLE - AUC: {table_auc:.4f}, APR: {table_apr:.4f}\")\n",
    "\n",
    "# Plot discriminator results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(gan_history.history['accuracy'], label='train')\n",
    "plt.plot(gan_history.history['val_accuracy'], label='test')\n",
    "plt.title(f'GAN Discriminator (AUC={gan_auc:.4f})')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(table_history.history['accuracy'], label='train')\n",
    "plt.plot(table_history.history['val_accuracy'], label='test')\n",
    "plt.title(f'TABLE Discriminator (AUC={table_auc:.4f})')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{evaluation_dir}/discriminator_comparison.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 5. VISUALIZATION FUNCTIONS\n",
    "def visualize_trajectories(real_data, gan_data, table_data, feature_names=None, n_patients=5):\n",
    "    \"\"\"\n",
    "    Visualize example trajectories from real, GAN, and TABLE data\n",
    "    \"\"\"\n",
    "    if feature_names is None:\n",
    "        feature_names = ['Heart Rate', 'Resp. Rate', 'SpO2', 'FiO2', 'RR Set']\n",
    "    \n",
    "    n_features = len(feature_names)\n",
    "    fig, axes = plt.subplots(n_features, 3, figsize=(18, 4*n_features))\n",
    "    \n",
    "    # Randomly select patients\n",
    "    real_patients = np.random.choice(real_data.shape[0], n_patients, replace=False)\n",
    "    gan_patients = np.random.choice(gan_data.shape[0], n_patients, replace=False)\n",
    "    table_patients = np.random.choice(table_data.shape[0], n_patients, replace=False)\n",
    "    \n",
    "    # Plot each feature\n",
    "    for i in range(n_features):\n",
    "        # Real data\n",
    "        for p in real_patients:\n",
    "            axes[i, 0].plot(real_data[p, :, i], 'b-', alpha=0.7)\n",
    "        axes[i, 0].set_title(f'{feature_names[i]} - Real')\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # GAN data\n",
    "        for p in gan_patients:\n",
    "            axes[i, 1].plot(gan_data[p, :, i], 'r-', alpha=0.7)\n",
    "        axes[i, 1].set_title(f'{feature_names[i]} - GAN')\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # TABLE data\n",
    "        for p in table_patients:\n",
    "            axes[i, 2].plot(table_data[p, :, i], 'g-', alpha=0.7)\n",
    "        axes[i, 2].set_title(f'{feature_names[i]} - TABLE')\n",
    "        axes[i, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{evaluation_dir}/trajectory_visualization.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize example trajectories\n",
    "visualize_trajectories(real_array, gan_renormalized, table_renormalized)\n",
    "\n",
    "# 6. DIMENSIONALITY REDUCTION VISUALIZATION\n",
    "def visualize_dim_reduction(real_data, gan_data, table_data):\n",
    "    \"\"\"\n",
    "    Visualize data in lower dimensions using t-SNE\n",
    "    \"\"\"\n",
    "    # Flatten data\n",
    "    real_flat = real_data.reshape(real_data.shape[0], -1)\n",
    "    gan_flat = gan_data.reshape(gan_data.shape[0], -1)\n",
    "    table_flat = table_data.reshape(table_data.shape[0], -1)\n",
    "    \n",
    "    # Sample for computational efficiency\n",
    "    max_samples = 1000\n",
    "    if len(real_flat) > max_samples:\n",
    "        real_indices = np.random.choice(len(real_flat), max_samples, replace=False)\n",
    "        real_flat = real_flat[real_indices]\n",
    "    \n",
    "    if len(gan_flat) > max_samples:\n",
    "        gan_indices = np.random.choice(len(gan_flat), max_samples, replace=False)\n",
    "        gan_flat = gan_flat[gan_indices]\n",
    "        \n",
    "    if len(table_flat) > max_samples:\n",
    "        table_indices = np.random.choice(len(table_flat), max_samples, replace=False)\n",
    "        table_flat = table_flat[table_indices]\n",
    "    \n",
    "    # Combine data\n",
    "    combined = np.vstack([real_flat, gan_flat, table_flat])\n",
    "    \n",
    "    # Create labels\n",
    "    labels = np.concatenate([\n",
    "        np.zeros(len(real_flat)),  # Real: 0\n",
    "        np.ones(len(gan_flat)),    # GAN: 1\n",
    "        2*np.ones(len(table_flat))  # TABLE: 2\n",
    "    ])\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    print(\"Computing t-SNE embedding...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embedded = tsne.fit_transform(combined)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(\n",
    "        embedded[:, 0], embedded[:, 1], \n",
    "        c=labels, cmap='viridis', \n",
    "        alpha=0.7, s=5\n",
    "    )\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#440154', label='Real', markersize=8),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#21918c', label='GAN', markersize=8),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#fde725', label='TABLE', markersize=8)\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements)\n",
    "    \n",
    "    plt.title('t-SNE Visualization of Real and Generated Data')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{evaluation_dir}/tsne_visualization.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Run t-SNE visualization\n",
    "visualize_dim_reduction(real_array, gan_renormalized, table_renormalized)\n",
    "\n",
    "# 7. SUMMARY METRICS TABLE\n",
    "# Create a summary table of all metrics\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Max Mean Discrepancy (MMD)', \n",
    "        'Correlation Matrix Difference (Frobenius Norm)',\n",
    "        'Feature Mean Correlation Coefficient (CC)',\n",
    "        'Feature Mean RMSE',\n",
    "        'Discriminator AUC',\n",
    "        'Discriminator APR'\n",
    "    ],\n",
    "    'GAN': [\n",
    "        mmd_gan,\n",
    "        np.linalg.norm(gan_corr - real_corr),\n",
    "        dim_stats['gan_cc'],\n",
    "        dim_stats['gan_rmse'],\n",
    "        gan_auc,\n",
    "        gan_apr\n",
    "    ],\n",
    "    'TABLE': [\n",
    "        mmd_table,\n",
    "        np.linalg.norm(table_corr - real_corr),\n",
    "        dim_stats['table_cc'],\n",
    "        dim_stats['table_rmse'],\n",
    "        table_auc,\n",
    "        table_apr\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nSummary of Evaluation Metrics:\")\n",
    "print(metrics_summary)\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_summary.to_csv(f\"{evaluation_dir}/metrics_summary.csv\", index=False)\n",
    "print(f\"\\nEvaluation metrics saved to {evaluation_dir}/metrics_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_temporal_correlation(real_data, gan_data, table_data, feature_names=None, \n",
    "                                 timepoints=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Create correlation heatmaps showing temporal relationships between features\n",
    "    for real, GAN, and TABLE generated data.\n",
    "    \n",
    "    Args:\n",
    "        real_data: Real data array (n_patients, n_timesteps, n_features)\n",
    "        gan_data: GAN generated data array (n_patients, n_timesteps, n_features)\n",
    "        table_data: TABLE generated data array (n_patients, n_timesteps, n_features)\n",
    "        feature_names: Names of features (default: uses feature_names list)\n",
    "        timepoints: Specific timepoints to include (default: select 6 evenly spread points)\n",
    "        output_dir: Directory to save figures (optional)\n",
    "    \"\"\"\n",
    "    if feature_names is None:\n",
    "        feature_names = ['Heart Rate', 'Resp. Rate', 'SpO2', 'FiO2', 'RR Set']\n",
    "    \n",
    "    # Select timepoints to analyze (to avoid overcrowding the plot)\n",
    "    n_timesteps = real_data.shape[1]\n",
    "    if timepoints is None:\n",
    "        # Select 6 evenly spaced timepoints\n",
    "        timepoints = np.linspace(0, n_timesteps-1, 6, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "    \n",
    "    # Function to process and plot one dataset\n",
    "    def create_correlation_plot(data, ax, title):\n",
    "        # Extract data and create feature names with timepoints\n",
    "        selected_data = []\n",
    "        combined_feature_names = []\n",
    "        \n",
    "        for i, feature in enumerate(feature_names):\n",
    "            for tp in timepoints:\n",
    "                # Extract data for this feature and timepoint\n",
    "                feature_data = data[:, tp, i]\n",
    "                selected_data.append(feature_data)\n",
    "                # Create label: \"HR_00\", \"HR_06\", etc.\n",
    "                combined_feature_names.append(f\"{feature}_{tp:02d}\")\n",
    "        \n",
    "        # Transpose to get shape (n_patients, n_selected_features)\n",
    "        selected_data = np.array(selected_data).T\n",
    "        \n",
    "        # Create DataFrame and calculate correlation\n",
    "        df = pd.DataFrame(data=selected_data, columns=combined_feature_names)\n",
    "        corr_matrix = df.corr()\n",
    "        \n",
    "        # Create mask for upper triangle\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        \n",
    "        # Plot heatmap - added linewidths=0 to remove grid lines\n",
    "        sns.heatmap(corr_matrix, ax=ax, mask=mask, cmap=\"coolwarm\", \n",
    "                   vmin=-1., vmax=1., center=0, square=True, \n",
    "                   linewidths=0, linecolor=None,\n",
    "                   cbar_kws={\"shrink\": .5})\n",
    "        \n",
    "        # Adjust labels\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=75)\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    # Create plots for real and generated data\n",
    "    create_correlation_plot(real_data, axes[0], \"Real Data - Temporal Correlations\")\n",
    "    create_correlation_plot(gan_data, axes[1], \"GAN Data - Temporal Correlations\")\n",
    "    create_correlation_plot(table_data, axes[2], \"TABLE Data - Temporal Correlations\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if output_dir is provided\n",
    "    if output_dir:\n",
    "        plt.savefig(f'{output_dir}/feature_time_correlations.png', dpi=300)\n",
    "        plt.savefig(f'{output_dir}/feature_time_correlations.pdf', format='pdf')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def feature_cross_correlation_difference(real_data, gan_data, table_data, feature_names=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Create correlation difference heatmaps showing how feature correlations \n",
    "    differ between real and generated data (both GAN and TABLE).\n",
    "    \n",
    "    Args:\n",
    "        real_data: Real data array (n_patients, n_timesteps, n_features)\n",
    "        gan_data: GAN generated data array (n_patients, n_timesteps, n_features)\n",
    "        table_data: TABLE generated data array (n_patients, n_timesteps, n_features)\n",
    "        feature_names: Names of features\n",
    "        output_dir: Directory to save figures (optional)\n",
    "    \"\"\"\n",
    "    if feature_names is None:\n",
    "        feature_names = ['HR', 'RR', 'SpO2', 'FiO2', 'RR_Set']\n",
    "    \n",
    "    # Reshape to 2D: (patients*timesteps, features)\n",
    "    real_flat = real_data.reshape(-1, real_data.shape[-1])\n",
    "    gan_flat = gan_data.reshape(-1, gan_data.shape[-1])\n",
    "    table_flat = table_data.reshape(-1, table_data.shape[-1])\n",
    "    \n",
    "    # Calculate correlation matrices\n",
    "    real_corr = np.corrcoef(real_flat.T)\n",
    "    gan_corr = np.corrcoef(gan_flat.T)\n",
    "    table_corr = np.corrcoef(table_flat.T)\n",
    "    \n",
    "    # Calculate differences\n",
    "    gan_diff = gan_corr - real_corr\n",
    "    table_diff = table_corr - real_corr\n",
    "    gan_table_diff = gan_corr - table_corr\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "    \n",
    "    # Plot GAN difference\n",
    "    sns.heatmap(gan_diff, annot=True, cmap='coolwarm', vmin=-0.5, vmax=0.5, center=0, \n",
    "                square=True, ax=axes[0], linewidths=0, linecolor=None,\n",
    "                xticklabels=feature_names, yticklabels=feature_names)\n",
    "    axes[0].set_title('GAN - Real Correlation Difference')\n",
    "    \n",
    "    # Plot TABLE difference\n",
    "    sns.heatmap(table_diff, annot=True, cmap='coolwarm', vmin=-0.5, vmax=0.5, center=0, \n",
    "                square=True, ax=axes[1], linewidths=0, linecolor=None,\n",
    "                xticklabels=feature_names, yticklabels=feature_names)\n",
    "    axes[1].set_title('TABLE - Real Correlation Difference')\n",
    "    \n",
    "    # Plot GAN vs TABLE difference\n",
    "    sns.heatmap(gan_table_diff, annot=True, cmap='coolwarm', vmin=-0.5, vmax=0.5, center=0, \n",
    "                square=True, ax=axes[2], linewidths=0, linecolor=None,\n",
    "                xticklabels=feature_names, yticklabels=feature_names)\n",
    "    axes[2].set_title('GAN - TABLE Correlation Difference')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if output_dir is provided\n",
    "    if output_dir:\n",
    "        plt.savefig(f'{output_dir}/correlation_differences.png', dpi=300)\n",
    "        plt.savefig(f'{output_dir}/correlation_differences.pdf', format='pdf')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and return metrics\n",
    "    gan_mean_abs_diff = np.mean(np.abs(gan_diff))\n",
    "    table_mean_abs_diff = np.mean(np.abs(table_diff))\n",
    "    \n",
    "    print(f\"GAN - Real mean absolute correlation difference: {gan_mean_abs_diff:.4f}\")\n",
    "    print(f\"TABLE - Real mean absolute correlation difference: {table_mean_abs_diff:.4f}\")\n",
    "    \n",
    "    # Create a summary dataframe\n",
    "    diff_summary = pd.DataFrame({\n",
    "        'Method': ['GAN vs Real', 'TABLE vs Real', 'GAN vs TABLE'],\n",
    "        'Mean Abs Diff': [\n",
    "            gan_mean_abs_diff,\n",
    "            table_mean_abs_diff,\n",
    "            np.mean(np.abs(gan_table_diff))\n",
    "        ],\n",
    "        'Max Abs Diff': [\n",
    "            np.max(np.abs(gan_diff)),\n",
    "            np.max(np.abs(table_diff)),\n",
    "            np.max(np.abs(gan_table_diff))\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nCorrelation Difference Summary:\")\n",
    "    print(diff_summary)\n",
    "    \n",
    "    return diff_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run temporal correlation analysis\n",
    "print(\"\\nGenerating feature-temporal correlation heatmaps...\")\n",
    "feature_temporal_correlation(\n",
    "    real_array, \n",
    "    gan_renormalized,\n",
    "    table_renormalized,\n",
    "    feature_names=['Heart Rate', 'Resp. Rate', 'SpO2', 'FiO2', 'RR Set'],\n",
    "    output_dir=evaluation_dir\n",
    ")\n",
    "\n",
    "# Run correlation difference analysis\n",
    "print(\"\\nGenerating correlation difference heatmaps...\")\n",
    "corr_diff_summary = feature_cross_correlation_difference(\n",
    "    real_array, \n",
    "    gan_renormalized,\n",
    "    table_renormalized,\n",
    "    feature_names=['HR', 'RR', 'SpO2', 'FiO2', 'RR_Set'],\n",
    "    output_dir=evaluation_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic_extract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
