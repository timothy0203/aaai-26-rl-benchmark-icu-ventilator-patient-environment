{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ventilation Patient Environment\n",
    "- Builds a state transition dictionary (experiment: bin strategies, observation hours, and variables)\n",
    "- Implements approximation strategies for unseen state-action pairs\n",
    "- Designs rewards based on ventilation weaning (extubation) guidelines to build a Python Gym environment\n",
    "- Generates trajectories by interacting with different offline policy doctor agents\n",
    "- Visualizes trajectories with transition action options\n",
    "- Visualizes trajectories with the same initial state (digital twin concept) to demonstrate evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library, Settings and configuration, Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Settings and configuration\n",
    "# ================================\n",
    "bins = 10                          # number of bins for discretization\n",
    "bins_strategy = \"s2\"               # \"s1\": linearly cut, \"s2\": NEWS score\n",
    "obs_hrs = 3                        # number of observation hours (e.g., current + previous)\n",
    "subset_size = 10000                # number of distinct stay_id to use (sample a subset)\n",
    "train_percentage = 0.7             # percentage of data to use for training : testing -> 7 : 3\n",
    "test_cases_num = 1000              # number of test cases to generate\n",
    "id_cols = ['stay_id', 'before_weaning_hr']\n",
    "feature_cols = ['heart_rate', 'resp_rate', 'spo2', 'fio2', 'respiratory_rate_set', 'gender_M', 'age'] # TODO: add gender, age, peep, (tidal_volume_set, tidal_volume_observed, sbp, dbp, mbp)\n",
    "selected_cols = id_cols + feature_cols\n",
    "state_cols = ['heart_rate', 'resp_rate', 'spo2']  # State variables\n",
    "action_cols = ['fio2', 'respiratory_rate_set']  # Action variables\n",
    "state_units = ['bpm', 'bpm', '%']  # Units for state variables\n",
    "action_units = ['%', 'bpm']  # Units for action variables\n",
    "baseline_cols = ['gender_M', 'age']  # Baseline variables\n",
    "\n",
    "GROUND_TRUTH_WITH_NEIGHBORS = True  # Use ground truth neighbors for evaluation, False for run different K for finding best neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Data reading\n",
    "# ================================\n",
    "# Define data paths (update if needed)\n",
    "eICU_prefix_path = \"../data/eICU\"\n",
    "mimic_iii_prefix_path = \"../data/mimic_iii\"\n",
    "mimic_iv_prefix_path = \"../data/mimic_iv\"\n",
    "\n",
    "eICU_file = os.path.join(eICU_prefix_path, \"baseline_charttime_ground_truth.csv\")\n",
    "mimic_iv_file = os.path.join(mimic_iv_prefix_path, \"baseline_charttime_ground_truth.csv\")\n",
    "mimic_iii_file = os.path.join(mimic_iii_prefix_path, \"baseline_charttime_ground_truth.csv\")\n",
    "\n",
    "# Read dataframes\n",
    "eICU_df = pd.read_csv(eICU_file)\n",
    "mimic_iv_df = pd.read_csv(mimic_iv_file)\n",
    "mimic_iii_df = pd.read_csv(mimic_iii_file)\n",
    "\n",
    "# ================================\n",
    "# Add \"hours_in\" column (if missing)\n",
    "# ================================\n",
    "def add_hours_in(baseline_df):\n",
    "    # if hours_in not present, create it based on before_weaning_hr\n",
    "    if 'hours_in' not in list(baseline_df.columns):\n",
    "        vital_sign_df = baseline_df[selected_cols]\n",
    "        vital_sign_df_with_hours = vital_sign_df.copy()\n",
    "        # For each stay_id, use the maximum before_weaning_hr as baseline\n",
    "        max_before_weaning = vital_sign_df.groupby('stay_id')['before_weaning_hr'].transform('max')\n",
    "        vital_sign_df_with_hours['hours_in'] = max_before_weaning - vital_sign_df['before_weaning_hr']\n",
    "        vital_sign_df_with_hours = vital_sign_df_with_hours.sort_values(['stay_id', 'hours_in'])\n",
    "        vital_sign_df_with_hours.drop(columns=['before_weaning_hr'], inplace=True)\n",
    "        cols = list(vital_sign_df_with_hours.columns)\n",
    "        # Move hours_in to position 1 (after stay_id)\n",
    "        if 'hours_in' in cols:\n",
    "            cols.remove('hours_in')\n",
    "        cols.insert(1, 'hours_in')\n",
    "        baseline_df = vital_sign_df_with_hours[cols]\n",
    "    baseline_df['gender_M'] = baseline_df['gender_M'].astype(int)\n",
    "    return baseline_df\n",
    "\n",
    "# Apply to all datasets\n",
    "eICU_df = add_hours_in(eICU_df)\n",
    "mimic_iv_df = add_hours_in(mimic_iv_df)\n",
    "mimic_iii_df = add_hours_in(mimic_iii_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iv_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the hours_in distribution for each stay_id (only pick the max hours_in)\n",
    "def check_hours_in_distribution(baseline_df):\n",
    "    # Check the distribution of hours_in for each stay_id\n",
    "    stay_id_distribution = baseline_df.groupby('stay_id')['hours_in'].max().reset_index()\n",
    "    stay_id_distribution['hours_in'] = stay_id_distribution['hours_in'].astype(int)\n",
    "    return stay_id_distribution\n",
    "eICU_hours_in_distribution = check_hours_in_distribution(eICU_df)\n",
    "mimic_iv_hours_in_distribution = check_hours_in_distribution(mimic_iv_df)\n",
    "mimic_iii_hours_in_distribution = check_hours_in_distribution(mimic_iii_df)\n",
    "# Plot the distribution of hours_in for each stay_id\n",
    "def plot_hours_in_distribution(hours_in_distribution, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(hours_in_distribution['hours_in'], bins=30, color='blue', alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Hours In')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "plot_hours_in_distribution(eICU_hours_in_distribution, \"eICU Hours In Distribution\")\n",
    "plot_hours_in_distribution(mimic_iv_hours_in_distribution, \"MIMIC-IV Hours In Distribution\")\n",
    "plot_hours_in_distribution(mimic_iii_hours_in_distribution, \"MIMIC-III Hours In Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State transition dictionary\n",
    "- (experiment: bin strategies, observation hours, and variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin strategies\n",
    "- Baseline:\n",
    "    - age: 5 bin (age_bins = np.linspace(20, 100, 6))\n",
    "    - gender: binary\n",
    "- State:\n",
    "    - strategy 1: linear cut\n",
    "    - strategy 2: NEWS for reference boundary\n",
    "    - (advanced: Chi-Merge / similar transition behavior for finding the varibles interval)\n",
    "- Action:\n",
    "    - (strategy 1: qcut with 10 bin, can be less than 10 bins)\n",
    "    - strategy 2:\n",
    "        - fio2, (peep or others) use 10 bins\n",
    "        - resp rate set: same as resp_rate (due to visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Data visualization\n",
    "# ================================\n",
    "def visualize_data(df, feature_cols):\n",
    "    # Plot histograms for each feature\n",
    "    n_features = len(feature_cols)\n",
    "    fig, axes = plt.subplots(nrows=n_features, ncols=1, figsize=(12, 5 * n_features), sharex=False)\n",
    "\n",
    "    if n_features == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable\n",
    "\n",
    "    for idx, col in enumerate(feature_cols):\n",
    "        ax = axes[idx]\n",
    "        df[col].hist(bins=30, ax=ax, color='blue', alpha=0.7)\n",
    "        ax.set_title(f'Distribution of {col}', fontsize=14)\n",
    "        ax.set_xlabel(col, fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Visualize data for mimic_iv_df\n",
    "visualize_data(mimic_iv_df, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define min_max_values as provided (from MIMIC-IV)\n",
    "min_max_values = {\n",
    "    \"fio2\": {\"min\": 21.0, \"max\": 100.0},\n",
    "    \"respiratory_rate_set\": {\"min\": 5.0, \"max\": 40.0},\n",
    "    \"heart_rate\": {\"min\": 37.0, \"max\": 180.0},\n",
    "    \"resp_rate\": {\"min\": 4.5, \"max\": 36.5},\n",
    "    \"spo2\": {\"min\": 80.0, \"max\": 100.0},\n",
    "    \"age\": {\"min\": 20.0, \"max\": 100.0}\n",
    "}\n",
    "\n",
    "# 1. Baseline: Age discretization\n",
    "def discretize_age(df):\n",
    "    age_bins = np.linspace(20, 100, 6)  # 5 bins\n",
    "    df['age'] = pd.cut(df['age'], bins=age_bins, labels=range(len(age_bins) - 1), include_lowest=True).astype('Int64')\n",
    "    return df, age_bins\n",
    "\n",
    "# 2. Action Discretization Function\n",
    "def discretize_actions(df, action_cols, bins):\n",
    "    bin_edges_dict = {\n",
    "        'fio2': [21, 30, 35, 40, 50, 60, 70, 80, 90, 100],\n",
    "        # 'fio2': [21, 30, 35, 40, 50, 60, 70, 80, 90, 100, float('inf')],\n",
    "        'respiratory_rate_set': [0, 4, 8, 11, 14, 17, 20, 24, 27, 30, 33, 40]\n",
    "        # 'respiratory_rate_set': [0, 4, 8, 11, 14, 17, 20, 24, 27, 30, 33, 37, float('inf')]\n",
    "    }\n",
    "    df_disc = df.copy()\n",
    "    for col in action_cols:\n",
    "        if col in bin_edges_dict:\n",
    "            bin_edges = bin_edges_dict[col]\n",
    "            try:\n",
    "                # Use pd.cut with fixed bin edges\n",
    "                df_disc[col] = pd.cut(\n",
    "                    df_disc[col].clip(lower=bin_edges[0], upper=bin_edges[-1]),  # Clip values to the range\n",
    "                    bins=bin_edges,\n",
    "                    labels=range(len(bin_edges) - 1),\n",
    "                    include_lowest=True\n",
    "                ).astype('Int64')\n",
    "            except ValueError:\n",
    "                # Handle cases where pd.cut fails (e.g., not enough unique values)\n",
    "                df_disc[col] = pd.NA\n",
    "        else:\n",
    "            # If no bin edges are provided for the column, set it to NaN\n",
    "            df_disc[col] = pd.NA\n",
    "    return df_disc, bin_edges_dict\n",
    "\n",
    "# 3.a. State and Action: Strategy 1 - Equal-width bins\n",
    "def get_bin_edges_strategy1(df, feature_cols, min_max_values, bins): # TODO: make sure bin for eICU won't cause Nan\n",
    "    \"\"\"\n",
    "    Generate bin edges for state and action variables using equal-width bins.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        feature_cols (list): List of feature columns (state + action variables).\n",
    "        min_max_values (dict): Dictionary containing min and max values for each feature.\n",
    "        bins (int): Number of bins.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing bin edges for each feature.\n",
    "    \"\"\"\n",
    "    bin_edges_dict = {}\n",
    "    for col in feature_cols:\n",
    "        if col in min_max_values:\n",
    "            col_min = min(df[col].min(), min_max_values[col][\"min\"])\n",
    "            col_max = max(df[col].max(), min_max_values[col][\"max\"])\n",
    "        else:\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "        bin_edges = np.linspace(col_min, col_max, bins + 1)\n",
    "        bin_edges_dict[col] = bin_edges\n",
    "    return bin_edges_dict\n",
    "\n",
    "# 3.b. State and Action: Strategy 2 - Fixed NEWS boundaries for state, equal-width for action\n",
    "def get_bin_edges_strategy2(action_cols, min_max_values, bins):\n",
    "    \"\"\"\n",
    "    Generate bin edges for state variables using fixed NEWS boundaries and action variables using equal-width bins.\n",
    "\n",
    "    Args:\n",
    "        action_cols (list): List of action variable columns.\n",
    "        min_max_values (dict): Dictionary containing min and max values for each feature.\n",
    "        bins (int): Number of bins for action variables.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing bin edges for each feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fixed NEWS boundaries for state variables v2\n",
    "    bin_edges_dict = {\n",
    "        'heart_rate': [0, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 180],\n",
    "        'resp_rate': [0, 4, 8, 11, 14, 17, 20, 24, 27, 30, 33, 40],\n",
    "        'spo2': [0, 80, 90, 91, 93, 95, 98, 100],\n",
    "        'fio2': [21, 30, 35, 40, 50, 60, 70, 80, 90, 100],\n",
    "        'respiratory_rate_set': [0, 4, 8, 11, 14, 17, 20, 24, 27, 30, 33, 40],\n",
    "        'gender_M': [0, 1, 2], # TODO: not sure\n",
    "        'age':[20, 36, 52, 68, 84, 100],\n",
    "    }\n",
    "\n",
    "    return bin_edges_dict\n",
    "\n",
    "# Function to discretize using fixed bin edges\n",
    "def discretize_with_fixed_bins(df, feature_cols, bin_edges_dict):\n",
    "    df_disc = df.copy()\n",
    "    for col in feature_cols:\n",
    "        bin_edges = bin_edges_dict[col]\n",
    "        labels = list(range(len(bin_edges) - 1))  # 0 to bins-1\n",
    "        df_disc[col] = pd.cut(\n",
    "            df_disc[col].clip(lower=bin_edges[0], upper=bin_edges[-1]),  # Clip values to the range\n",
    "            bins=bin_edges,\n",
    "            labels=list(range(len(bin_edges) - 1)),  # 0 to bins-1\n",
    "            include_lowest=True\n",
    "        ).astype('Int64')\n",
    "    return df_disc\n",
    "\n",
    "# 4. Calculate bin edges for Strategy 1 using mimic_iv_df\n",
    "bin_edges_dict_s1 = get_bin_edges_strategy1(mimic_iv_df, feature_cols, min_max_values, bins)\n",
    "\n",
    "# 4. Calculate bin edges for Strategy 2\n",
    "bin_edges_dict_s2 = get_bin_edges_strategy2(action_cols, min_max_values, bins)\n",
    "\n",
    "# 4. Discretize all datasets using Strategy 1\n",
    "eICU_disc_s1 = discretize_with_fixed_bins(eICU_df, state_cols, bin_edges_dict_s1)\n",
    "mimic_iv_disc_s1 = discretize_with_fixed_bins(mimic_iv_df, state_cols, bin_edges_dict_s1)\n",
    "mimic_iii_disc_s1 = discretize_with_fixed_bins(mimic_iii_df, state_cols, bin_edges_dict_s1)\n",
    "\n",
    "# 4. Discretize all datasets using Strategy 2\n",
    "eICU_disc_s2 = discretize_with_fixed_bins(eICU_df, state_cols, bin_edges_dict_s2)\n",
    "mimic_iv_disc_s2 = discretize_with_fixed_bins(mimic_iv_df, state_cols, bin_edges_dict_s2)\n",
    "mimic_iii_disc_s2 = discretize_with_fixed_bins(mimic_iii_df, state_cols, bin_edges_dict_s2)\n",
    "\n",
    "# 5. Visualization: Compare bin edges for different strategies\n",
    "def compare_bin_edges(bin_edges_dict_s1, bin_edges_dict_s2, state_cols):\n",
    "    for col in state_cols:\n",
    "        print(f\"\\nFeature: {col}\")\n",
    "        print(f\"Strategy 1 (Equal-Width): {bin_edges_dict_s1[col]}\")\n",
    "        print(f\"Strategy 2 (Fixed NEWS): {bin_edges_dict_s2[col]}\")\n",
    "\n",
    "compare_bin_edges(bin_edges_dict_s1, bin_edges_dict_s2, state_cols)\n",
    "\n",
    "# 6. Visualization: Compare distributions for different strategies\n",
    "def plot_discretization_comparison(df_s1, df_s2, state_cols):\n",
    "    n_features = len(state_cols)\n",
    "    fig, axes = plt.subplots(nrows=n_features, ncols=1, figsize=(12, 5 * n_features), sharex=False)\n",
    "\n",
    "    if n_features == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable\n",
    "\n",
    "    for idx, col in enumerate(state_cols):\n",
    "        ax = axes[idx]\n",
    "        s1_data = df_s1[col].dropna()\n",
    "        s2_data = df_s2[col].dropna()\n",
    "\n",
    "        bins_range = np.arange(max(s1_data.max(), s2_data.max()) + 2)  # +2 to include the last bin edge\n",
    "        width = 0.4\n",
    "\n",
    "        s1_counts, _ = np.histogram(s1_data, bins=bins_range)\n",
    "        s2_counts, _ = np.histogram(s2_data, bins=bins_range)\n",
    "\n",
    "        x = np.arange(len(bins_range) - 1)\n",
    "        ax.bar(x - width / 2, s1_counts, width=width, label='Strategy 1 (Equal-Width)', color='blue', alpha=0.7)\n",
    "        ax.bar(x + width / 2, s2_counts, width=width, label='Strategy 2 (Fixed NEWS)', color='green', alpha=0.7)\n",
    "\n",
    "        ax.set_title(f'Discretization Comparison for {col}', fontsize=14)\n",
    "        ax.set_xlabel('Bin Number', fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot comparison for mimic_iv_df\n",
    "plot_discretization_comparison(mimic_iv_disc_s1, mimic_iv_disc_s2, state_cols)\n",
    "\n",
    "# 7. Discretize actions\n",
    "mimic_iv_disc_s1, action_bins_dict_s1 = discretize_actions(mimic_iv_disc_s1, action_cols, bins)\n",
    "mimic_iv_disc_s2, action_bins_dict_s2 = discretize_actions(mimic_iv_disc_s2, action_cols, bins)\n",
    "mimic_iii_disc_s1, action_bins_dict_s1 = discretize_actions(mimic_iii_disc_s1, action_cols, bins)\n",
    "mimic_iii_disc_s2, action_bins_dict_s2 = discretize_actions(mimic_iii_disc_s2, action_cols, bins)\n",
    "eICU_disc_s1, action_bins_dict_s1 = discretize_actions(eICU_disc_s1, action_cols, bins)\n",
    "eICU_disc_s2, action_bins_dict_s2 = discretize_actions(eICU_disc_s2, action_cols, bins)\n",
    "\n",
    "# 8. Discretize age\n",
    "mimic_iv_disc_s1, age_bins = discretize_age(mimic_iv_disc_s1)\n",
    "mimic_iv_disc_s2, age_bins = discretize_age(mimic_iv_disc_s2)\n",
    "mimic_iii_disc_s1, age_bins = discretize_age(mimic_iii_disc_s1)\n",
    "mimic_iii_disc_s2, age_bins = discretize_age(mimic_iii_disc_s2)\n",
    "eICU_disc_s1, age_bins = discretize_age(eICU_disc_s1)\n",
    "eICU_disc_s2, age_bins = discretize_age(eICU_disc_s2)\n",
    "\n",
    "# 9. Plot action and age discretization\n",
    "def plot_action_age_discretization(df, action_cols):\n",
    "    n_actions = len(action_cols)\n",
    "    fig, axes = plt.subplots(nrows=n_actions + 1, ncols=1, figsize=(12, 5 * (n_actions + 1)), sharex=False)\n",
    "    if n_actions == 1:\n",
    "        axes = [axes]\n",
    "    for idx, col in enumerate(action_cols): \n",
    "        ax = axes[idx]\n",
    "        df[col].hist(bins=30, ax=ax, color='blue', alpha=0.7)\n",
    "        ax.set_title(f'Distribution of {col}', fontsize=14)\n",
    "        ax.set_xlabel(col, fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax = axes[-1]\n",
    "    df['age'].value_counts().sort_index().plot(kind='bar', ax=ax, color='blue', alpha=0.7)\n",
    "    ax.set_title('Age Discretization', fontsize=14)\n",
    "    ax.set_xlabel('Age Bin', fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "# Plot action and age discretization for mimic_iv_df\n",
    "plot_action_age_discretization(mimic_iv_disc_s1, action_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select the strategy for using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set s1 or s2 or others\n",
    "if bins_strategy == \"s1\":\n",
    "    eICU_disc = eICU_disc_s1.copy()\n",
    "    mimic_iv_disc = mimic_iv_disc_s1.copy()\n",
    "    mimic_iii_disc = mimic_iii_disc_s1.copy()\n",
    "    num_bins_dict = {key: len(edges) - 1 for key, edges in bin_edges_dict_s1.items()}\n",
    "elif bins_strategy == \"s2\":\n",
    "    eICU_disc = eICU_disc_s2.copy()\n",
    "    mimic_iv_disc = mimic_iv_disc_s2.copy()\n",
    "    mimic_iii_disc = mimic_iii_disc_s2.copy()\n",
    "    num_bins_dict = {key: len(edges) - 1 for key, edges in bin_edges_dict_s2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eICU_disc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges_dict_s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iv_disc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iv_disc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subset / train / test for expericement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Sample a subset of distinct stay_id (applied to one dataset; you can repeat for others)\n",
    "# ================================\n",
    "def sample_subset(df, subset_size):\n",
    "    unique_ids = df['stay_id'].unique()\n",
    "    sampled_ids = random.sample(list(unique_ids), min(subset_size, len(unique_ids)))\n",
    "    return df[df['stay_id'].isin(sampled_ids)]\n",
    "\n",
    "# Use subset for further analysis (example uses mimic_iv_disc) -> use for debugging\n",
    "# mimic_iv_disc = sample_subset(mimic_iv_disc, subset_size)\n",
    "# print(f\"Subset shape: {mimic_iv_disc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(mimic_iv_disc['stay_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split MIMIC-IV for internal evaluation\n",
    "train_ids = random.sample(list(mimic_iv_disc['stay_id'].unique()), int(train_percentage * len(list(mimic_iv_disc['stay_id'].unique()))))\n",
    "test_ids = [sid for sid in mimic_iv_disc['stay_id'].unique() if sid not in train_ids]\n",
    "train_df = mimic_iv_disc[mimic_iv_disc['stay_id'].isin(train_ids)]\n",
    "test_df = mimic_iv_disc[mimic_iv_disc['stay_id'].isin(test_ids)]\n",
    "\n",
    "# [cont_update]\n",
    "train_cont_df = mimic_iv_df[mimic_iv_df['stay_id'].isin(train_ids)]\n",
    "test_cont_df = mimic_iv_df[mimic_iv_df['stay_id'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train : Test (on MIMIC IV) -> 7 : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['stay_id'].nunique(), test_df['stay_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train_ids and test_ids\n",
    "train_ids_df = pd.DataFrame(train_ids, columns=['train_ids'])\n",
    "test_ids_df = pd.DataFrame(test_ids, columns=['test_ids'])\n",
    "train_ids_df.to_csv(os.path.join(mimic_iv_prefix_path, 'train_ids.csv'), index=False)\n",
    "test_ids_df.to_csv(os.path.join(mimic_iv_prefix_path, 'test_ids.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In / Out of Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Assume train_df, test_df, eICU_disc are already loaded as pandas DataFrames\n",
    "\n",
    "# 1. Summary statistics\n",
    "def summary_stats(df, name):\n",
    "    print(f\"Summary statistics for {name}:\")\n",
    "    print(df.describe())\n",
    "    print(\"\\n\")\n",
    "\n",
    "summary_stats(train_df, \"train_df\")\n",
    "summary_stats(test_df, \"test_df\")\n",
    "summary_stats(eICU_disc, \"eICU_disc\")\n",
    "\n",
    "# 2. Kolmogorov-Smirnov test for each numerical column\n",
    "def ks_test(df1, df2, columns, name1, name2):\n",
    "    print(f\"KS test between {name1} and {name2}:\")\n",
    "    for col in columns:\n",
    "        stat, p = ks_2samp(df1[col], df2[col])\n",
    "        print(f\"{col}: KS statistic={stat:.3f}, p-value={p:.3e}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "num_cols = ['heart_rate', 'resp_rate', 'spo2', 'fio2', 'respiratory_rate_set', 'age']\n",
    "\n",
    "# Train vs Test (should be similar, high p-values)\n",
    "ks_test(train_df, test_df, num_cols, \"train_df\", \"test_df\")\n",
    "\n",
    "# Train vs eICU (should be different, low p-values)\n",
    "ks_test(train_df, eICU_disc, num_cols, \"train_df\", \"eICU_disc\")\n",
    "\n",
    "# Test vs eICU (should be different, low p-values)\n",
    "ks_test(test_df, eICU_disc, num_cols, \"test_df\", \"eICU_disc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assume mimic_iv_df and eICU_df are already loaded\n",
    "\n",
    "# 1. Visualize distributions\n",
    "cols = ['heart_rate', 'resp_rate', 'spo2', 'fio2', 'respiratory_rate_set', 'age']\n",
    "\n",
    "for col in cols:\n",
    "    plt.figure(figsize=(6,3))\n",
    "    sns.kdeplot(mimic_iv_df[col], label='MIMIC-IV', fill=True)\n",
    "    sns.kdeplot(eICU_df[col], label='eICU', fill=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Kolmogorov-Smirnov test for each variable\n",
    "print(\"Kolmogorov-Smirnov test results:\")\n",
    "for col in cols:\n",
    "    stat, p = ks_2samp(mimic_iv_df[col].dropna(), eICU_df[col].dropna())\n",
    "    print(f\"{col}: KS statistic={stat:.3f}, p-value={p:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct state & transition structures with KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Construct state & transition structures\n",
    "# ================================\n",
    "# Here we define a simple approach to create state with pre-state-action for obs_hrs.\n",
    "# For each stay_id, sort by hours_in and then generate tuples.\n",
    "from collections import defaultdict\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "def create_state_transitions(df, obs_hrs, state_cols, action_cols, baseline_cols=None):\n",
    "    \"\"\"\n",
    "    Create state transitions with flexibility to add state, action, and baseline variables.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe containing the data.\n",
    "        obs_hrs (int): Number of observation hours to include in the state history.\n",
    "        state_cols (list): List of columns to include in the state.\n",
    "        action_cols (list): List of columns to include in the action.\n",
    "        baseline_cols (list, optional): List of baseline columns (e.g., age, gender) to include. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are tuples of (state, action) history and values are lists of next states.\n",
    "    \"\"\"\n",
    "    transitions = defaultdict(list)\n",
    "    \n",
    "    # Group by stay_id\n",
    "    for stay_id, group in df.groupby('stay_id'):\n",
    "        group = group.sort_values('hours_in')\n",
    "        rows = group.to_dict('records')\n",
    "        \n",
    "        # Sliding window with length obs_hrs+1 to get (state/action history, next_state)\n",
    "        for i in range(len(rows) - obs_hrs):\n",
    "            window = rows[i:i+obs_hrs+1]\n",
    "            \n",
    "            # Build state_with_pre_state_action: for each of the first obs_hrs rows\n",
    "            key = []\n",
    "            for j in range(obs_hrs):\n",
    "                state = tuple(window[j][col] for col in state_cols)  # Extract state variables\n",
    "                action = tuple(window[j][col] for col in action_cols)  # Extract action variables\n",
    "                key.append((state, action))\n",
    "            \n",
    "            # Add baseline variables (if provided) to the key\n",
    "            if baseline_cols:\n",
    "                baseline = tuple(window[0][col] for col in baseline_cols)  # Use the first row for baseline variables\n",
    "                key.append(baseline)\n",
    "            \n",
    "            # The current state for the last row\n",
    "            curr_state = tuple(window[obs_hrs][col] for col in state_cols)\n",
    "            \n",
    "            # Store transition: key -> next_state\n",
    "            transitions[tuple(key)].append(curr_state)\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "def flatten_key(key):\n",
    "    # Flatten the key for KDTree\n",
    "    flat = []\n",
    "    for element in key[:-1]:  # Iterate over all elements except the last (state-action history)\n",
    "        state, action = element\n",
    "        flat.extend([int(x) for x in state])  # Convert state values to int\n",
    "        flat.extend([int(x) for x in action])  # Convert action values to int\n",
    "    flat.extend([int(x) for x in key[-1]])  # Convert baseline values to int\n",
    "    return np.array(flat, dtype=np.int32)\n",
    "\n",
    "def create_state_transitions_with_stay_id_mapping(df, obs_hrs, state_cols, action_cols, baseline_cols=None):\n",
    "    \"\"\"\n",
    "    Create state transitions and a mapping of transition keys to stay_id.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe containing the data.\n",
    "        obs_hrs (int): Number of observation hours to include in the state history.\n",
    "        state_cols (list): List of columns to include in the state.\n",
    "        action_cols (list): List of columns to include in the action.\n",
    "        baseline_cols (list, optional): List of baseline columns (e.g., age, gender) to include. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are tuples of (state, action) history and values are lists of next states.\n",
    "        dict: A dictionary mapping each transition key to a list of associated stay_id.\n",
    "    \"\"\"\n",
    "    transitions = defaultdict(list)\n",
    "    stay_id_mapping = defaultdict(list)\n",
    "\n",
    "    # Group by stay_id\n",
    "    for stay_id, group in df.groupby('stay_id'):\n",
    "        group = group.sort_values('hours_in')\n",
    "        rows = group.to_dict('records')\n",
    "\n",
    "        # Sliding window with length obs_hrs+1 to get (state/action history, next_state)\n",
    "        for i in range(len(rows) - obs_hrs):\n",
    "            window = rows[i:i+obs_hrs+1]\n",
    "\n",
    "            # Build state_with_pre_state_action: for each of the first obs_hrs rows\n",
    "            key = []\n",
    "            for j in range(obs_hrs):\n",
    "                state = tuple(window[j][col] for col in state_cols)  # Extract state variables\n",
    "                action = tuple(window[j][col] for col in action_cols)  # Extract action variables\n",
    "                key.append((state, action))\n",
    "\n",
    "            # Add baseline variables (if provided) to the key\n",
    "            if baseline_cols:\n",
    "                baseline = tuple(window[0][col] for col in baseline_cols)  # Use the first row for baseline variables\n",
    "                key.append(baseline)\n",
    "\n",
    "            # The current state for the last row\n",
    "            curr_state = tuple(window[obs_hrs][col] for col in state_cols)\n",
    "\n",
    "            # Store transition: key -> next_state\n",
    "            transitions[tuple(key)].append(curr_state)\n",
    "\n",
    "            # Map transition key to stay_id\n",
    "            stay_id_mapping[tuple(key)].append(stay_id)\n",
    "\n",
    "    return transitions, stay_id_mapping\n",
    "\n",
    "def filter_transitions_by_stay_id(transitions, stay_id_mapping, min_stay_ids=2):\n",
    "    \"\"\"\n",
    "    Filter transitions based on the number of distinct stay_id associated with each key.\n",
    "\n",
    "    Args:\n",
    "        transitions (dict): Dictionary of transitions where keys are transition keys and values are lists of next states.\n",
    "        stay_id_mapping (dict): Dictionary mapping each transition key to a list of associated stay_id.\n",
    "        min_stay_ids (int): Minimum number of distinct stay_id required for a key to be included.\n",
    "\n",
    "    Returns:\n",
    "        dict: Filtered transitions containing only keys with at least min_stay_ids distinct stay_id.\n",
    "    \"\"\"\n",
    "    filtered_transitions = {}\n",
    "    for key, next_states in transitions.items():\n",
    "        if len(set(stay_id_mapping.get(key, []))) >= min_stay_ids:\n",
    "            filtered_transitions[key] = next_states\n",
    "    return filtered_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: transitions for whole mimic_iv_disc\n",
    "transitions = create_state_transitions(mimic_iv_disc, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "print(f\"Number of unique state_with_pre_state_action keys: {len(transitions)}\")\n",
    "\n",
    "# [cont_update]\n",
    "transitions_cont = create_state_transitions(mimic_iv_df, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "print(f\"Number of unique state_with_pre_state_action keys: {len(transitions_cont)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to access the transitions\n",
    "# [ ((state_1, action_1), (state_2, action_2), ..., (state_obs_hrs, action_obs_hrs), (age, gender)) ]\n",
    "for key in list(transitions.keys())[:5]:  # Display first 5 keys\n",
    "    print(f\"Key: {key}, Next States: {transitions[key]}\")\n",
    "\n",
    "# [cont_update]\n",
    "for key in list(transitions_cont.keys())[:5]:  # Display first 5 keys\n",
    "    print(f\"Key: {key}, Next States: {transitions_cont[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### generate transitions for different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transitions, train_stay_id_mapping = create_state_transitions_with_stay_id_mapping(train_df, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "test_transitions, test_stay_id_mapping = create_state_transitions_with_stay_id_mapping(test_df, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "eicu_trans, eicu_stay_id_mapping = create_state_transitions_with_stay_id_mapping(eICU_disc, obs_hrs, state_cols, action_cols, baseline_cols) \n",
    "mimic_iii_trans, mimic_iii_stay_id_mapping = create_state_transitions_with_stay_id_mapping(mimic_iii_disc, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "eicu_mimic_iii_trans, eicu_mimic_iii_stay_id_mapping = create_state_transitions_with_stay_id_mapping(pd.concat([eICU_disc, mimic_iii_disc]), obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "eicu_mimic_iii_mimic_iv_train_trans, eicu_mimic_iii_mimic_iv_stay_id_mapping = create_state_transitions_with_stay_id_mapping(pd.concat([eICU_disc, mimic_iii_disc, train_df]), obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "eicu_mimic_iv_train_trans, eicu_mimic_iv_train_stay_id_mapping = create_state_transitions_with_stay_id_mapping(pd.concat([eICU_disc, train_df]), obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "\n",
    "# [cont_update]\n",
    "train_cont_transitions, train_cont_stay_id_mapping = create_state_transitions_with_stay_id_mapping(train_cont_df, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "test_cont_transitions, test_cont_stay_id_mapping = create_state_transitions_with_stay_id_mapping(test_cont_df, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "eicu_cont_trans, eicu_cont_stay_id_mapping = create_state_transitions_with_stay_id_mapping(eICU_df, obs_hrs, state_cols, action_cols, baseline_cols) \n",
    "mimic_iii_cont_trans, mimic_iii_cont_stay_id_mapping = create_state_transitions_with_stay_id_mapping(mimic_iii_df, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "eicu_mimic_iii_cont_trans, eicu_mimic_iii_cont_stay_id_mapping = create_state_transitions_with_stay_id_mapping(pd.concat([eICU_df, mimic_iii_df]), obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "eicu_mimic_iii_mimic_iv_train_cont_trans, eicu_mimic_iii_mimic_iv_cont_stay_id_mapping = create_state_transitions_with_stay_id_mapping(pd.concat([eICU_df, mimic_iii_df, train_cont_df]), obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "eicu_mimic_iv_train_cont_trans, eicu_mimic_iv_train_cont_stay_id_mapping = create_state_transitions_with_stay_id_mapping(pd.concat([eICU_df, train_cont_df]), obs_hrs, state_cols, action_cols, baseline_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keys_list = list(train_transitions.keys())\n",
    "train_X = np.array([flatten_key(key) for key in train_transitions.keys()])\n",
    "train_tree = KDTree(train_X, metric='manhattan')\n",
    "\n",
    "# [gt_with_neighbors_update] build KDTree for calculate the ground truth\n",
    "test_keys_list = list(test_transitions.keys())\n",
    "test_X = np.array([flatten_key(key) for key in test_transitions.keys()])\n",
    "test_tree = KDTree(test_X, metric='manhattan')\n",
    "\n",
    "eicu_keys_list = list(eicu_trans.keys())\n",
    "eicu_X = np.array([flatten_key(key) for key in eicu_trans.keys()])\n",
    "eicu_tree = KDTree(eicu_X, metric='manhattan')\n",
    "\n",
    "eicu_mimic_iii_keys_list = list(eicu_mimic_iii_trans.keys())\n",
    "eicu_mimic_iii_X = np.array([flatten_key(key) for key in eicu_mimic_iii_trans.keys()])\n",
    "eicu_mimic_iii_tree = KDTree(eicu_mimic_iii_X, metric='manhattan')\n",
    "\n",
    "eicu_mimic_iii_mimic_iv_train_keys_list = list(eicu_mimic_iii_mimic_iv_train_trans.keys())\n",
    "eicu_mimic_iii_mimic_iv_train_X = np.array([flatten_key(key) for key in eicu_mimic_iii_mimic_iv_train_trans.keys()])\n",
    "eicu_mimic_iii_mimic_iv_train_tree = KDTree(eicu_mimic_iii_mimic_iv_train_X, metric='manhattan')\n",
    "\n",
    "eicu_mimic_iv_train_keys_list = list(eicu_mimic_iv_train_trans.keys())\n",
    "eicu_mimic_iv_train_X = np.array([flatten_key(key) for key in eicu_mimic_iv_train_trans.keys()])\n",
    "eicu_mimic_iv_train_tree = KDTree(eicu_mimic_iv_train_X, metric='manhattan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [cont_update]\n",
    "train_cont_keys_list = list(train_cont_transitions.keys())\n",
    "train_cont_X = np.array([flatten_key(key) for key in train_cont_transitions.keys()])\n",
    "train_cont_tree = KDTree(train_cont_X, metric='manhattan')\n",
    "\n",
    "# [gt_with_neighbors_update] build KDTree for calculate the ground truth\n",
    "test_cont_keys_list = list(test_cont_transitions.keys())\n",
    "test_cont_X = np.array([flatten_key(key) for key in test_cont_transitions.keys()])\n",
    "test_cont_tree = KDTree(test_cont_X, metric='manhattan')\n",
    "\n",
    "eicu_cont_keys_list = list(eicu_cont_trans.keys())\n",
    "eicu_cont_X = np.array([flatten_key(key) for key in eicu_cont_trans.keys()])\n",
    "eicu_cont_tree = KDTree(eicu_cont_X, metric='manhattan')\n",
    "\n",
    "eicu_mimic_iii_cont_keys_list = list(eicu_mimic_iii_cont_trans.keys())\n",
    "eicu_mimic_iii_cont_X = np.array([flatten_key(key) for key in eicu_mimic_iii_cont_trans.keys()])\n",
    "eicu_mimic_iii_cont_tree = KDTree(eicu_mimic_iii_cont_X, metric='manhattan')\n",
    "\n",
    "eicu_mimic_iii_mimic_iv_train_cont_keys_list = list(eicu_mimic_iii_mimic_iv_train_cont_trans.keys())\n",
    "eicu_mimic_iii_mimic_iv_train_cont_X = np.array([flatten_key(key) for key in eicu_mimic_iii_mimic_iv_train_cont_trans.keys()])\n",
    "eicu_mimic_iii_mimic_iv_train_cont_tree = KDTree(eicu_mimic_iii_mimic_iv_train_cont_X, metric='manhattan')\n",
    "\n",
    "eicu_mimic_iv_train_cont_keys_list = list(eicu_mimic_iv_train_cont_trans.keys())\n",
    "eicu_mimic_iv_train_cont_X = np.array([flatten_key(key) for key in eicu_mimic_iv_train_cont_trans.keys()])\n",
    "eicu_mimic_iv_train_cont_tree = KDTree(eicu_mimic_iv_train_cont_X, metric='manhattan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filter transitions based on distinct stay_id number appear in this key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GROUND_TRUTH_WITH_NEIGHBORS:\n",
    "    min_stay_ids = 0\n",
    "else:\n",
    "    min_stay_ids = 3\n",
    "\n",
    "train_filtered_transitions = filter_transitions_by_stay_id(train_transitions, train_stay_id_mapping, min_stay_ids=min_stay_ids)\n",
    "test_filtered_transitions = filter_transitions_by_stay_id(test_transitions, test_stay_id_mapping, min_stay_ids=min_stay_ids)\n",
    "eicu_filtered_transitions = filter_transitions_by_stay_id(eicu_trans, eicu_stay_id_mapping, min_stay_ids=min_stay_ids)\n",
    "mimic_iii_filtered_transitions = filter_transitions_by_stay_id(mimic_iii_trans, mimic_iii_stay_id_mapping, min_stay_ids=min_stay_ids)\n",
    "eicu_mimic_iii_filtered_transitions = filter_transitions_by_stay_id(eicu_mimic_iii_trans, eicu_mimic_iii_stay_id_mapping, min_stay_ids=min_stay_ids)\n",
    "eicu_mimic_iii_mimic_iv_train_filtered_transitions = filter_transitions_by_stay_id(eicu_mimic_iii_mimic_iv_train_trans, eicu_mimic_iii_mimic_iv_stay_id_mapping, min_stay_ids=min_stay_ids)\n",
    "\n",
    "# common_keys = set(train_transitions.keys()) & set(eicu_filtered_transitions.keys()) & set(mimic_iii_filtered_transitions.keys()) \n",
    "common_keys = set(train_transitions.keys()) & set(eicu_filtered_transitions.keys()) # [cont_update] remove mimic-iii\n",
    "not_in_mimic_iv = set(eicu_filtered_transitions.keys()) - set(train_transitions.keys()) # [cont_update] remove mimic-iii\n",
    "# not_in_mimic_iv = (set(eicu_filtered_transitions.keys()) | set(mimic_iii_filtered_transitions.keys())) - set(train_transitions.keys()) # TODO: since use & only have 4 keys left, so use |\n",
    "# not_in_mimic_iv = set(eicu_filtered_transitions.keys()) & set(mimic_iii_filtered_transitions.keys()) - set(train_transitions.keys()) \n",
    "external_common = list(common_keys)[:test_cases_num] \n",
    "external_not_mimic_iv = list(not_in_mimic_iv)[:test_cases_num]\n",
    "\n",
    "\n",
    "# [cont_update]\n",
    "train_cont_filtered_transitions = filter_transitions_by_stay_id(train_cont_transitions, train_cont_stay_id_mapping, min_stay_ids=min_stay_ids)\n",
    "test_cont_filtered_transitions = filter_transitions_by_stay_id(test_cont_transitions, test_cont_stay_id_mapping, min_stay_ids=min_stay_ids)\n",
    "eicu_cont_filtered_transitions = filter_transitions_by_stay_id(eicu_cont_trans, eicu_cont_stay_id_mapping, min_stay_ids=min_stay_ids)\n",
    "mimic_iii_cont_filtered_transitions = filter_transitions_by_stay_id(mimic_iii_cont_trans, mimic_iii_cont_stay_id_mapping, min_stay_ids=min_stay_ids)\n",
    "eicu_mimic_iii_cont_filtered_transitions = filter_transitions_by_stay_id(eicu_mimic_iii_cont_trans, eicu_mimic_iii_cont_stay_id_mapping, min_stay_ids=min_stay_ids)\n",
    "eicu_mimic_iii_mimic_iv_train_cont_filtered_transitions = filter_transitions_by_stay_id(eicu_mimic_iii_mimic_iv_train_cont_trans, eicu_mimic_iii_mimic_iv_cont_stay_id_mapping, min_stay_ids=min_stay_ids)\n",
    "\n",
    "common_cont_keys = set(train_cont_transitions.keys()) & set(eicu_cont_filtered_transitions.keys()) # [cont_update] remove mimic-iii\n",
    "not_in_mimic_iv_cont = set(eicu_cont_filtered_transitions.keys()) - set(train_cont_transitions.keys()) # [cont_update] remove mimic-iii\n",
    "external_common_cont = list(common_cont_keys)[:test_cases_num] \n",
    "external_not_mimic_iv_cont = list(not_in_mimic_iv_cont)[:test_cases_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition approximation strategies\n",
    "- unseen state-action pairs\n",
    "- seen state-action pairs (for avoid overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keep current state: next state always keep current state\n",
    "- Random work: random (+1, 0, -1) on each of the state variables\n",
    "- Action-Based KNN:\n",
    "    - sample next_state if state_action_with_pre_state_action_baseline appear in train_transitions\n",
    "    - approximate next_state by KNN with similar state_with_pre_state_action_baseline with the same action, get the delta distribution\n",
    "- State-Based KNN\n",
    "    - sample next_state if state_action_with_pre_state_action_baseline appear in train_transitions\n",
    "    - approximate next_state by KNN with the same state_with_pre_state_action_baseline with different action, get the delta distribution\n",
    "- Bayesian\n",
    "- (Random Forest Regressor: since the next state should not be deterministic)\n",
    "- (XGBoost Regressor)\n",
    "- XGBoost Classifier\n",
    "- (Random Forest Classifier)\n",
    "- (SVC Classifier)\n",
    "- Transformer\n",
    "- RNN (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition approximation strategies with n samples and save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# Helper function to extract the current state from the key\n",
    "def get_current_state(key):\n",
    "    return key[-2][0]  # The last element's state\n",
    "\n",
    "# Helper function to extract the baseline variables from the key\n",
    "def get_baseline(key):\n",
    "    return key[-1]  # The last element (age, gender)\n",
    "\n",
    "# Helper function to flatten the key for KDTree\n",
    "def flatten_key(key):\n",
    "    # Flatten the key for KDTree\n",
    "    flat = []\n",
    "    for element in key[:-1]:  # Iterate over all elements except the last (state-action history)\n",
    "        state, action = element\n",
    "        flat.extend([int(x) for x in state])  # Convert state values to int\n",
    "        flat.extend([int(x) for x in action])  # Convert action values to int\n",
    "    flat.extend([int(x) for x in key[-1]])  # Convert baseline values to int\n",
    "    return np.array(flat, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Keep Current State - Batch Version\n",
    "# ================================\n",
    "def keep_current_state_approximation(key, transitions, tree, k=10, bins=10, n_samples=1):\n",
    "    \"\"\"Approximate next_state using current state (batch version).\"\"\"\n",
    "    current_state = get_current_state(key)\n",
    "    # Return n_samples copies of the current state\n",
    "    return [current_state] * n_samples\n",
    "\n",
    "# ================================\n",
    "# Random Walk - Batch Version\n",
    "# ================================\n",
    "def random_walk_approximation(key, transitions, tree, k=10, bins=10, n_samples=1):\n",
    "    \"\"\"Approximate next_state using small random changes from current state (batch version).\"\"\"\n",
    "    s_t = get_current_state(key)  # Get current state\n",
    "    results = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Generate random noise for each sample\n",
    "        clipped_state = []\n",
    "        for i, s in enumerate(s_t):\n",
    "            variable_name = list(num_bins_dict.keys())[i]  # Get the variable name\n",
    "            max_value = num_bins_dict[variable_name] - 1   # Max value is bins - 1\n",
    "            clipped_value = np.clip(s + random.randint(-1, 1), 0, max_value)   # Clip to [0, max_value]\n",
    "            clipped_state.append(int(clipped_value))\n",
    "        results.append(tuple(clipped_state))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ================================\n",
    "# Action-Based KNN - Batch Version\n",
    "# ================================\n",
    "# TODO: analysis percentage of action-based neighbors usage\n",
    "def action_based_approximation(key, transitions, tree, k=100, bins=10, n_samples=1, keys_list=train_keys_list, used_for_ground_truth_with_neighbors=False):  # TODO: shold k be larger? -> k larger means higher diversity\n",
    "    \"\"\"Approximate next_state using neighbors with the same action (batch version).\"\"\"\n",
    "    # if key in transitions:\n",
    "    #     # Use reservoir sampling if n_samples <= available transitions\n",
    "    #     if n_samples <= len(transitions[key]):\n",
    "    #         return random.sample(transitions[key], n_samples)\n",
    "    #     else:\n",
    "    #         # If more samples needed than available, sample with replacement\n",
    "    #         return [random.choice(transitions[key]) for _ in range(n_samples)]\n",
    "        \n",
    "    # TODO: [2025/04/26] if the transition level evaluation can also be good even we don't use random.choice(transitions[key])\n",
    "    # which means the transition is quite based on the action\n",
    "    \n",
    "    desired_action = key[-2][1]  # Extract the last action\n",
    "    flat_key = flatten_key(key)\n",
    "    distances, indices = tree.query(flat_key.reshape(1, -1), k=k)\n",
    "    \n",
    "    candidates = [keys_list[i] for i in indices[0] if keys_list[i][-2][1] == desired_action]\n",
    "    if not candidates:\n",
    "        # If no candidates, return current state\n",
    "        current_state = get_current_state(key)\n",
    "        if not used_for_ground_truth_with_neighbors:\n",
    "            return [current_state] * n_samples\n",
    "        else:\n",
    "            return [] # [gt_with_neighbors_update] if used for ground truth with neighbors meet isolated (obs, action), return empty list\n",
    "    \n",
    "    # TODO: [2025/04/26] another version of candidates -> make sure it can get k candidates that match the desired action\n",
    "    # i.e., for all keys with the same action, find the k nearest neighbors -> need to do experiment\n",
    "    \n",
    "    all_deltas = []\n",
    "    for cand_key in candidates:\n",
    "        s_t = get_current_state(cand_key)\n",
    "        for s_next in transitions[cand_key]:\n",
    "            delta = tuple(np.array(s_next, dtype=int) - np.array(s_t, dtype=int))\n",
    "            all_deltas.append(delta)\n",
    "    \n",
    "    # If no deltas found, use (0,0,0)\n",
    "    if not all_deltas:\n",
    "        current_state = get_current_state(key)\n",
    "        return [current_state] * n_samples\n",
    "    \n",
    "    # Sample deltas with replacement\n",
    "    sampled_deltas = random.choices(all_deltas, k=n_samples)\n",
    "    \n",
    "    # Apply deltas to current state\n",
    "    s_t = get_current_state(key)\n",
    "    results = []\n",
    "    \n",
    "    for delta in sampled_deltas:\n",
    "        # Apply each delta and clip values\n",
    "        clipped_state = []\n",
    "        for i, (s, d) in enumerate(zip(s_t, delta)):\n",
    "            variable_name = list(num_bins_dict.keys())[i]\n",
    "            max_value = num_bins_dict[variable_name] - 1\n",
    "            clipped_value = np.clip(s + d, 0, max_value)\n",
    "            clipped_state.append(int(clipped_value))\n",
    "        results.append(tuple(clipped_state))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def action_based_approximation_cont(key, transitions, tree, k=100, bins=10, n_samples=1, keys_list=train_cont_keys_list, used_for_ground_truth_with_neighbors=False):  # TODO: shold k be larger? -> k larger means higher diversity\n",
    "    \"\"\"Approximate next_state using neighbors with the same action (batch version).\"\"\"\n",
    "    # if key in transitions:\n",
    "    #     # Use reservoir sampling if n_samples <= available transitions\n",
    "    #     if n_samples <= len(transitions[key]):\n",
    "    #         return random.sample(transitions[key], n_samples)\n",
    "    #     else:\n",
    "    #         # If more samples needed than available, sample with replacement\n",
    "    #         return [random.choice(transitions[key]) for _ in range(n_samples)]\n",
    "        \n",
    "    # TODO: [2025/04/26] if the transition level evaluation can also be good even we don't use random.choice(transitions[key])\n",
    "    # which means the transition is quite based on the action\n",
    "    \n",
    "    desired_action = key[-2][1]  # Extract the last action\n",
    "    flat_key = flatten_key(key)\n",
    "    distances, indices = tree.query(flat_key.reshape(1, -1), k=k)\n",
    "    \n",
    "    candidates = [keys_list[i] for i in indices[0] if keys_list[i][-2][1] == desired_action]\n",
    "    # print(len(candidates), \"candidates found for action:\", desired_action)  # Debugging\n",
    "    # print(candidates[0])  # Display first 5 candidates for debugging\n",
    "    if not candidates:\n",
    "        # If no candidates, return current state\n",
    "        # print(\"Anomalous action\")\n",
    "        current_state = get_current_state(key)\n",
    "        if not used_for_ground_truth_with_neighbors:\n",
    "            return [current_state] * n_samples\n",
    "        else:\n",
    "            return [] # [gt_with_neighbors_update] if used for ground truth with neighbors meet isolated (obs, action), return empty list\n",
    "    \n",
    "    # TODO: [2025/04/26] another version of candidates -> make sure it can get k candidates that match the desired action\n",
    "    # i.e., for all keys with the same action, find the k nearest neighbors -> need to do experiment\n",
    "    \n",
    "    all_deltas = []\n",
    "    for cand_key in candidates:\n",
    "        s_t = get_current_state(cand_key)\n",
    "        for s_next in transitions[cand_key]:\n",
    "            delta = tuple(np.array(s_next, dtype=int) - np.array(s_t, dtype=int))\n",
    "            all_deltas.append(delta)\n",
    "    \n",
    "    # If no deltas found, use (0,0,0)\n",
    "    if not all_deltas:\n",
    "        current_state = get_current_state(key)\n",
    "        return [current_state] * n_samples\n",
    "    \n",
    "    # Sample deltas with replacement\n",
    "    sampled_deltas = random.choices(all_deltas, k=n_samples)\n",
    "    \n",
    "    # Apply deltas to current state\n",
    "    s_t = get_current_state(key)\n",
    "    results = []\n",
    "    \n",
    "    # [cont_update]\n",
    "    for delta in sampled_deltas:\n",
    "        # Apply each delta and clip values\n",
    "        clipped_state = []\n",
    "        for i, (s, d) in enumerate(zip(s_t, delta)):\n",
    "            # variable_name = list(num_bins_dict.keys())[i]\n",
    "            # max_value = num_bins_dict[variable_name] - 1\n",
    "            # clipped_value = np.clip(s + d, 0, max_value)\n",
    "            clipped_value = s + d # [cont_update] remove clipping\n",
    "            clipped_state.append(int(clipped_value))\n",
    "        results.append(tuple(clipped_state))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ================================\n",
    "# State-Based KNN - Batch Version\n",
    "# ================================\n",
    "def state_based_approximation(key, transitions, tree, k=10, bins=10, n_samples=1):\n",
    "    \"\"\"Approximate next_state using neighbors with similar state history (batch version).\"\"\"\n",
    "    # if key in transitions:\n",
    "    #     # Sample from available transitions\n",
    "    #     if n_samples <= len(transitions[key]):\n",
    "    #         return random.sample(transitions[key], n_samples)\n",
    "    #     else:\n",
    "    #         return [random.choice(transitions[key]) for _ in range(n_samples)]\n",
    "    \n",
    "    flat_key = flatten_key(key)\n",
    "    distances, indices = tree.query(flat_key.reshape(1, -1), k=k)\n",
    "    \n",
    "    all_deltas = []\n",
    "    for idx in indices[0]:\n",
    "        cand_key = train_keys_list[idx]\n",
    "        s_t = get_current_state(cand_key)\n",
    "        for s_next in transitions[cand_key]:\n",
    "            delta = tuple(np.array(s_next, dtype=int) - np.array(s_t, dtype=int))\n",
    "            all_deltas.append(delta)\n",
    "    \n",
    "    if not all_deltas:\n",
    "        current_state = get_current_state(key)\n",
    "        return [current_state] * n_samples\n",
    "    \n",
    "    # Sample deltas with replacement\n",
    "    sampled_deltas = random.choices(all_deltas, k=n_samples)\n",
    "    \n",
    "    # Apply deltas to current state\n",
    "    s_t = get_current_state(key)\n",
    "    results = []\n",
    "    \n",
    "    for delta in sampled_deltas:\n",
    "        # Apply delta and clip values\n",
    "        clipped_state = []\n",
    "        for i, (s, d) in enumerate(zip(s_t, delta)):\n",
    "            variable_name = list(num_bins_dict.keys())[i]\n",
    "            max_value = num_bins_dict[variable_name] - 1\n",
    "            clipped_value = np.clip(s + d, 0, max_value)\n",
    "            clipped_state.append(int(clipped_value))\n",
    "        results.append(tuple(clipped_state))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ================================\n",
    "# Bayesian Approximation - Batch Version\n",
    "# ================================\n",
    "def bayesian_approximation(key, transitions, tree, k=100, bins=10, alpha=1.0, n_samples=1): # TODO: [2025/04/29] bins should be the same as num_bins_dict (?)\n",
    "    \"\"\"Approximate next_state distribution using a Bayesian approach with Dirichlet prior (batch version).\"\"\"\n",
    "    # if key in transitions: # TODO: [2025/04/26] should we use this? or don't use random.choice(transitions[key])\n",
    "    #     # Sample from available transitions\n",
    "    #     if n_samples <= len(transitions[key]):\n",
    "    #         return random.sample(transitions[key], n_samples)\n",
    "    #     else:\n",
    "    #         return [random.choice(transitions[key]) for _ in range(n_samples)]\n",
    "    \n",
    "    flat_key = flatten_key(key)\n",
    "    distances, indices = tree.query(flat_key.reshape(1, -1), k=k)\n",
    "    \n",
    "    all_next_states = []\n",
    "    for idx in indices[0]:\n",
    "        all_next_states.extend(transitions[train_keys_list[idx]])\n",
    "    \n",
    "    if not all_next_states:\n",
    "        return [get_current_state(key)] * n_samples\n",
    "    \n",
    "    counter = Counter(all_next_states)\n",
    "    \n",
    "    # Calculate dirichlet prior\n",
    "    total = sum(counter.values()) + alpha * sum(num_bins_dict[col] for col in state_cols)\n",
    "    probs = {state: (count + alpha) / total for state, count in counter.items()}\n",
    "    # TODO: [ERROR]: use num_bins_dict for mapping the bin value instead of bins\n",
    "    \n",
    "    # Convert to lists for random.choices\n",
    "    states, weights = zip(*probs.items())\n",
    "    \n",
    "    # Return n_samples samples based on the calculated weights\n",
    "    return random.choices(states, weights=weights, k=n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# XGBoost Classification Approximation - Batch Version\n",
    "# ================================\n",
    "def xgb_classification_approximation(key, transitions, tree, k=10, bins=10, retrain=False, n_samples=1):\n",
    "    \"\"\"XGBoost-based approximation with randomness for uncertainty (classification version, batch).\"\"\"\n",
    "    # Ensure the model directory exists\n",
    "    model_dir = os.path.join(mimic_iv_prefix_path, \"models\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if we need to train or load XGBoost classification models\n",
    "    xgb_cls_models = None\n",
    "    # model_paths = [os.path.join(model_dir, f\"xgb_cls_model_{col}.pkl\") for col in state_cols]\n",
    "    # mapping_path = os.path.join(model_dir, \"xgb_cls_mappings.pkl\")\n",
    "    \n",
    "    # [gt_with_neighbors_update] training smooth with neighbors\n",
    "    model_paths = [os.path.join(model_dir, f\"xgb_cls_model_{col}_ground_truth_with_neighbors.pkl\") for col in state_cols]\n",
    "    mapping_path = os.path.join(model_dir, \"xgb_cls_mappings_ground_truth_with_neighbors.pkl\")\n",
    "    \n",
    "    if not retrain and all(os.path.exists(path) for path in model_paths) and os.path.exists(mapping_path):\n",
    "        # Load models and mappings\n",
    "        xgb_cls_models = []\n",
    "        for path in model_paths:\n",
    "            with open(path, 'rb') as f:\n",
    "                xgb_cls_models.append(pickle.load(f))\n",
    "                \n",
    "        with open(mapping_path, 'rb') as f:\n",
    "            mappings = pickle.load(f)\n",
    "            globals()['next_state_to_class'] = mappings['next_state_to_class']\n",
    "            globals()['class_to_next_state'] = mappings['class_to_next_state']\n",
    "            \n",
    "        # print(\"Loaded XGBoost classification models from disk\")\n",
    "    else:\n",
    "        # Train the models\n",
    "        print(\"Training XGBoost classification models...\")\n",
    "        xgb_cls_models = train_xgb_models_classification(transitions, state_cols)\n",
    "        \n",
    "        # Save the models\n",
    "        for i, col in enumerate(state_cols):\n",
    "            with open(model_paths[i], 'wb') as f:\n",
    "                pickle.dump(xgb_cls_models[i], f)\n",
    "                \n",
    "        # Save the mappings\n",
    "        mappings = {\n",
    "            'next_state_to_class': globals()['next_state_to_class'],\n",
    "            'class_to_next_state': globals()['class_to_next_state']\n",
    "        }\n",
    "        with open(mapping_path, 'wb') as f:\n",
    "            pickle.dump(mappings, f)\n",
    "            \n",
    "        print(\"Saved XGBoost classification models to disk\")\n",
    "    \n",
    "    # Store in globals for reuse\n",
    "    globals()['xgb_cls_models'] = xgb_cls_models\n",
    "    \n",
    "    # Generate predictions\n",
    "    features = flatten_key(key).reshape(1, -1)\n",
    "    results = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Get probability distributions for each target variable\n",
    "        hr_probs = xgb_cls_models[0].predict_proba(features)[0]\n",
    "        rr_probs = xgb_cls_models[1].predict_proba(features)[0]\n",
    "        spo2_probs = xgb_cls_models[2].predict_proba(features)[0]\n",
    "        \n",
    "        # Sample from probability distributions\n",
    "        hr_class = np.random.choice(len(hr_probs), p=hr_probs)\n",
    "        rr_class = np.random.choice(len(rr_probs), p=rr_probs)\n",
    "        spo2_class = np.random.choice(len(spo2_probs), p=spo2_probs)\n",
    "        \n",
    "        # Convert class indices to actual values\n",
    "        hr = globals()['class_to_next_state'][hr_class][0]\n",
    "        rr = globals()['class_to_next_state'][rr_class][1]\n",
    "        spo2 = globals()['class_to_next_state'][spo2_class][2]\n",
    "        \n",
    "        # Clip values to valid ranges\n",
    "        clipped_state = []\n",
    "        for i, pred in enumerate([hr, rr, spo2]):\n",
    "            variable_name = list(num_bins_dict.keys())[i]\n",
    "            max_value = num_bins_dict[variable_name] - 1\n",
    "            clipped_value = np.clip(pred, 0, max_value)\n",
    "            clipped_state.append(int(clipped_value))\n",
    "            \n",
    "        results.append(tuple(clipped_state))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def train_xgb_models_classification(transitions, state_cols):\n",
    "    \"\"\"Train XGBoost classification models for next state prediction.\"\"\"\n",
    "    X, y_next_states = [], []\n",
    "    \n",
    "    # Extract features (X) and next states (y)\n",
    "    for key, next_states in transitions.items():\n",
    "        features = flatten_key(key)\n",
    "        for next_state in next_states:\n",
    "            X.append(features)\n",
    "            # y_next_states.append(next_state)\n",
    "            y_next_states.append(tuple(next_state)) # [update]\n",
    "        if GROUND_TRUTH_WITH_NEIGHBORS: # [gt_with_neighbors_update] training smooth with nighbors\n",
    "            true_next_states = np.array(action_based_approximation(key, transitions, train_tree, k=100, bins=bins, n_samples=1, keys_list=train_keys_list, used_for_ground_truth_with_neighbors=True)) # [gt_with_neighbors_update]\n",
    "            for true_next_state in true_next_states:\n",
    "                X.append(features)\n",
    "                y_next_states.append(tuple(true_next_state))\n",
    "    \n",
    "    # Create mappings between next states and class indices\n",
    "    next_state_to_class = {}\n",
    "    class_to_next_state = {}\n",
    "    for next_state in set(y_next_states):\n",
    "        next_state_tuple = tuple(next_state)\n",
    "        if next_state_tuple not in next_state_to_class:\n",
    "            class_index = len(next_state_to_class)\n",
    "            next_state_to_class[next_state_tuple] = class_index\n",
    "            class_to_next_state[class_index] = next_state_tuple\n",
    "    \n",
    "    # Map next states to class indices\n",
    "    y = [next_state_to_class[tuple(next_state)] for next_state in y_next_states]\n",
    "    y = np.array(y)\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Store mappings for decoding\n",
    "    globals()['next_state_to_class'] = next_state_to_class\n",
    "    globals()['class_to_next_state'] = class_to_next_state\n",
    "    \n",
    "    # Train models\n",
    "    models = []\n",
    "    for i, col in enumerate(state_cols):\n",
    "        # model = xgb.XGBClassifier(\n",
    "        #     objective='multi:softprob', \n",
    "        #     max_depth=6, \n",
    "        #     n_estimators=100, \n",
    "        #     learning_rate=0.1,\n",
    "        #     n_jobs=-1\n",
    "        # )\n",
    "        model = xgb.XGBClassifier(\n",
    "            objective='multi:softprob', \n",
    "            max_depth=3, \n",
    "            n_estimators=30, \n",
    "            learning_rate=0.1,\n",
    "            n_jobs=-1,\n",
    "            tree_method='hist',\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8\n",
    "        )\n",
    "        print(f'Fitting {col} model')\n",
    "        model.fit(X, y)\n",
    "        models.append(model)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ================================\n",
    "# Random Forest Approximation - Batch Version\n",
    "# ================================\n",
    "\n",
    "def rf_approximation_classification(key, transitions, tree, k=10, bins=10, retrain=False, n_samples=1):\n",
    "    \"\"\"Random Forest classification-based approximation for next state prediction.\"\"\"\n",
    "    # Ensure the model directory exists\n",
    "    model_dir = os.path.join(mimic_iv_prefix_path, \"models\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if we need to train or load RF classification models\n",
    "    rf_cls_models = None\n",
    "    model_paths = [os.path.join(model_dir, f\"rf_cls_model_{col}.pkl\") for col in state_cols]\n",
    "    mapping_path = os.path.join(model_dir, \"rf_cls_mappings.pkl\")\n",
    "    \n",
    "    if not retrain and all(os.path.exists(path) for path in model_paths) and os.path.exists(mapping_path):\n",
    "        # Load models if they exist\n",
    "        rf_cls_models = []\n",
    "        for path in model_paths:\n",
    "            with open(path, 'rb') as f:\n",
    "                rf_cls_models.append(pickle.load(f))\n",
    "                \n",
    "        with open(mapping_path, 'rb') as f:\n",
    "            mappings = pickle.load(f)\n",
    "            globals()['next_state_to_class_rf'] = mappings['next_state_to_class']\n",
    "            globals()['class_to_next_state_rf'] = mappings['class_to_next_state']\n",
    "    else:\n",
    "        # Train the models\n",
    "        print(\"Training Random Forest classification models...\")\n",
    "        rf_cls_models = train_rf_models_classification(transitions, state_cols)\n",
    "        \n",
    "        # Save the models\n",
    "        for i, col in enumerate(state_cols):\n",
    "            with open(model_paths[i], 'wb') as f:\n",
    "                pickle.dump(rf_cls_models[i], f)\n",
    "                \n",
    "        # Save the mappings\n",
    "        mappings = {\n",
    "            'next_state_to_class': globals()['next_state_to_class_rf'],\n",
    "            'class_to_next_state_rf': globals()['class_to_next_state_rf']\n",
    "        }\n",
    "        with open(mapping_path, 'wb') as f:\n",
    "            pickle.dump(mappings, f)\n",
    "        print(\"Saved Random Forest classification models to disk\")\n",
    "    \n",
    "    # Store in globals for reuse\n",
    "    globals()['rf_cls_models'] = rf_cls_models\n",
    "    \n",
    "    # Generate predictions\n",
    "    features = flatten_key(key).reshape(1, -1)\n",
    "    results = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Get probability distributions for each target variable\n",
    "        hr_probs = rf_cls_models[0].predict_proba(features)[0]\n",
    "        rr_probs = rf_cls_models[1].predict_proba(features)[0]\n",
    "        spo2_probs = rf_cls_models[2].predict_proba(features)[0]\n",
    "        \n",
    "        # Sample from probability distributions\n",
    "        hr_class = np.random.choice(len(hr_probs), p=hr_probs)\n",
    "        rr_class = np.random.choice(len(rr_probs), p=rr_probs)\n",
    "        spo2_class = np.random.choice(len(spo2_probs), p=spo2_probs)\n",
    "        \n",
    "        # Convert class indices to actual values\n",
    "        hr = globals()['class_to_next_state_rf'][hr_class][0]\n",
    "        rr = globals()['class_to_next_state_rf'][rr_class][1]\n",
    "        spo2 = globals()['class_to_next_state_rf'][spo2_class][2]\n",
    "        \n",
    "        # Clip values to valid ranges\n",
    "        clipped_state = []\n",
    "        for i, pred in enumerate([hr, rr, spo2]):\n",
    "            variable_name = list(num_bins_dict.keys())[i]\n",
    "            max_value = num_bins_dict[variable_name] - 1\n",
    "            clipped_value = np.clip(pred, 0, max_value)\n",
    "            clipped_state.append(int(clipped_value))\n",
    "            \n",
    "        results.append(tuple(clipped_state))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def train_rf_models_classification(transitions, state_cols):\n",
    "    \"\"\"Train Random Forest classification models for each state variable.\"\"\"\n",
    "    X, y_next_states = [], []\n",
    "    \n",
    "    # Extract features (X) and next states (y)\n",
    "    for key, next_states in transitions.items():\n",
    "        features = flatten_key(key)\n",
    "        for next_state in next_states:\n",
    "            X.append(features)\n",
    "            y_next_states.append(next_state)\n",
    "    \n",
    "    # Create mappings between next states and class indices\n",
    "    next_state_to_class_rf = {}\n",
    "    class_to_next_state_rf = {}\n",
    "    for next_state in set(y_next_states):\n",
    "        next_state_tuple = tuple(next_state)\n",
    "        if next_state_tuple not in next_state_to_class_rf:\n",
    "            class_index = len(next_state_to_class_rf)\n",
    "            next_state_to_class_rf[next_state_tuple] = class_index\n",
    "            class_to_next_state_rf[class_index] = next_state_tuple\n",
    "    \n",
    "    # Map next states to class indices\n",
    "    y = [next_state_to_class_rf[tuple(next_state)] for next_state in y_next_states]\n",
    "    y = np.array(y)\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Split data for training and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Store mappings for decoding\n",
    "    globals()['next_state_to_class_rf'] = next_state_to_class_rf\n",
    "    globals()['class_to_next_state_rf'] = class_to_next_state_rf\n",
    "    \n",
    "    # Train models\n",
    "    models = []\n",
    "    for i, col in enumerate(state_cols):\n",
    "        print(f'Fitting RF classifier for {col}')\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=15,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Validate\n",
    "        val_accuracy = model.score(X_val, y_val)\n",
    "        print(f\"Validation accuracy for {col}: {val_accuracy:.4f}\")\n",
    "        \n",
    "        models.append(model)\n",
    "    \n",
    "    return models\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ===============================\n",
    "# Support Vector Classifier Approximation - Batch Version\n",
    "# ===============================\n",
    "def svm_approximation_classification(key, transitions, tree, k=10, bins=10, retrain=False, n_samples=1):\n",
    "    \"\"\"SVM classification-based approximation for next state prediction.\"\"\"\n",
    "    # Ensure the model directory exists\n",
    "    model_dir = os.path.join(mimic_iv_prefix_path, \"models\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if we need to train or load SVM classification models\n",
    "    svm_cls_models = None\n",
    "    model_paths = [os.path.join(model_dir, f\"svm_cls_model_{col}.pkl\") for col in state_cols]\n",
    "    mapping_path = os.path.join(model_dir, \"svm_cls_mappings.pkl\")\n",
    "    scaler_path = os.path.join(model_dir, \"svm_scaler.pkl\")\n",
    "    \n",
    "    if not retrain and all(os.path.exists(path) for path in model_paths) and os.path.exists(mapping_path) and os.path.exists(scaler_path):\n",
    "        # Load models if they exist\n",
    "        svm_cls_models = []\n",
    "        for path in model_paths:\n",
    "            with open(path, 'rb') as f:\n",
    "                svm_cls_models.append(pickle.load(f))\n",
    "                \n",
    "        with open(mapping_path, 'rb') as f:\n",
    "            mappings = pickle.load(f)\n",
    "            globals()['next_state_to_class_svm'] = mappings['next_state_to_class']\n",
    "            globals()['class_to_next_state_svm'] = mappings['class_to_next_state']\n",
    "            \n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            globals()['svm_scaler'] = pickle.load(f)\n",
    "    else:\n",
    "        # Train the models\n",
    "        print(\"Training SVM classification models...\")\n",
    "        svm_cls_models = train_svm_models_classification(transitions, state_cols)\n",
    "        \n",
    "        # Save the models\n",
    "        for i, col in enumerate(state_cols):\n",
    "            with open(model_paths[i], 'wb') as f:\n",
    "                pickle.dump(svm_cls_models[i], f)\n",
    "                \n",
    "        # Save the mappings\n",
    "        mappings = {\n",
    "            'next_state_to_class': globals()['next_state_to_class_svm'],\n",
    "            'class_to_next_state': globals()['class_to_next_state_svm']\n",
    "        }\n",
    "        with open(mapping_path, 'wb') as f:\n",
    "            pickle.dump(mappings, f)\n",
    "            \n",
    "        # Save the scaler\n",
    "        with open(scaler_path, 'wb') as f:\n",
    "            pickle.dump(globals()['svm_scaler'], f)\n",
    "            \n",
    "        print(\"Saved SVM classification models to disk\")\n",
    "    \n",
    "    # Store in globals for reuse\n",
    "    globals()['svm_cls_models'] = svm_cls_models\n",
    "    \n",
    "    # Generate predictions\n",
    "    features = flatten_key(key).reshape(1, -1)\n",
    "    # Scale features\n",
    "    features = globals()['svm_scaler'].transform(features)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Predict classes for each state variable\n",
    "        hr_class = svm_cls_models[0].predict(features)[0]\n",
    "        rr_class = svm_cls_models[1].predict(features)[0]\n",
    "        spo2_class = svm_cls_models[2].predict(features)[0]\n",
    "        \n",
    "        # Add some randomness (since SVM doesn't naturally give probabilities)\n",
    "        # We'll occasionally pick a random class based on prediction certainty\n",
    "        if random.random() < 0.2:  # 20% chance to pick random class\n",
    "            classes = list(globals()['class_to_next_state_svm'].keys())\n",
    "            hr_class = random.choice(classes)\n",
    "            rr_class = random.choice(classes)\n",
    "            spo2_class = random.choice(classes)\n",
    "        \n",
    "        # Convert class indices to actual values\n",
    "        hr = globals()['class_to_next_state_svm'][hr_class][0]\n",
    "        rr = globals()['class_to_next_state_svm'][rr_class][1]\n",
    "        spo2 = globals()['class_to_next_state_svm'][spo2_class][2]\n",
    "        \n",
    "        # Clip values to valid ranges\n",
    "        clipped_state = []\n",
    "        for i, pred in enumerate([hr, rr, spo2]):\n",
    "            variable_name = list(num_bins_dict.keys())[i]\n",
    "            max_value = num_bins_dict[variable_name] - 1\n",
    "            clipped_value = np.clip(pred, 0, max_value)\n",
    "            clipped_state.append(int(clipped_value))\n",
    "            \n",
    "        results.append(tuple(clipped_state))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def train_svm_models_classification(transitions, state_cols):\n",
    "    \"\"\"Train SVM classification models for each state variable.\"\"\"\n",
    "    X, y_next_states = [], []\n",
    "    \n",
    "    # Extract features (X) and next states (y)\n",
    "    for key, next_states in transitions.items():\n",
    "        features = flatten_key(key)\n",
    "        for next_state in next_states:\n",
    "            X.append(features)\n",
    "            y_next_states.append(next_state)\n",
    "    \n",
    "    # Create mappings between next states and class indices\n",
    "    next_state_to_class_svm = {}\n",
    "    class_to_next_state_svm = {}\n",
    "    for next_state in set(y_next_states):\n",
    "        next_state_tuple = tuple(next_state)\n",
    "        if next_state_tuple not in next_state_to_class_svm:\n",
    "            class_index = len(next_state_to_class_svm)\n",
    "            next_state_to_class_svm[next_state_tuple] = class_index\n",
    "            class_to_next_state_svm[class_index] = next_state_tuple\n",
    "    \n",
    "    # Map next states to class indices\n",
    "    y = [next_state_to_class_svm[tuple(next_state)] for next_state in y_next_states]\n",
    "    y = np.array(y)\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Split data for training and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    # Store mappings for decoding and scaler\n",
    "    globals()['next_state_to_class_svm'] = next_state_to_class_svm\n",
    "    globals()['class_to_next_state_svm'] = class_to_next_state_svm\n",
    "    globals()['svm_scaler'] = scaler\n",
    "    \n",
    "    # Train models\n",
    "    models = []\n",
    "    for i, col in enumerate(state_cols):\n",
    "        print(f'Fitting SVM classifier for {col}')\n",
    "        # Use linear kernel for speed with large datasets\n",
    "        model = SVC(\n",
    "            kernel='linear',\n",
    "            C=1.0,\n",
    "            probability=True,  # Enable probability estimates\n",
    "            random_state=42,\n",
    "            class_weight='balanced'  # Handle class imbalance\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Validate\n",
    "        val_accuracy = model.score(X_val, y_val)\n",
    "        print(f\"Validation accuracy for {col}: {val_accuracy:.4f}\")\n",
    "        \n",
    "        models.append(model)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN / Transformer for approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class StateTransitionDataset(Dataset):\n",
    "    def __init__(self, transitions):\n",
    "        self.features = []\n",
    "        self.targets = []\n",
    "        \n",
    "        # Convert transitions to PyTorch tensors\n",
    "        for key, next_states in transitions.items():\n",
    "            features = flatten_key(key)\n",
    "            for next_state in next_states:\n",
    "                self.features.append(features)\n",
    "                self.targets.append(next_state)\n",
    "            if GROUND_TRUTH_WITH_NEIGHBORS: # [gt_with_neighbors_update] training smooth with nighbors\n",
    "                # [gt_with_neighbors_update] since n_samples will affect training data size, change from 30 to 5\n",
    "                true_next_states = np.array(action_based_approximation(key, transitions, train_tree, k=100, bins=bins, n_samples=5, keys_list=train_keys_list, used_for_ground_truth_with_neighbors=True)) # [gt_with_neighbors_update]\n",
    "                for true_next_state in true_next_states:\n",
    "                    self.features.append(features)\n",
    "                    self.targets.append(true_next_state)\n",
    "        \n",
    "        self.features = torch.tensor(self.features, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(self.targets, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Versions of RNN and Transformer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get any key from transitions\n",
    "first_key = next(iter(transitions))\n",
    "# Flatten it to get the feature vector\n",
    "sample_features = flatten_key(first_key)\n",
    "# Get the dimension\n",
    "input_dim = len(sample_features)\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticTransformerNextStatePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim_dict, nhead=4, num_layers=2, dropout=0.2):\n",
    "        \"\"\"\n",
    "        output_dim_dict: Dictionary mapping state variable names to number of bins\n",
    "        \"\"\"\n",
    "        super(ProbabilisticTransformerNextStatePredictor, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim_dict = output_dim_dict\n",
    "        self.variable_names = list(output_dim_dict.keys())\n",
    "        \n",
    "        # Input embedding\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=hidden_dim*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layers - one for each state variable\n",
    "        self.output_heads = nn.ModuleDict({\n",
    "            var_name: nn.Linear(hidden_dim, num_bins) \n",
    "            for var_name, num_bins in output_dim_dict.items()\n",
    "        })\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape input: [batch_size, seq_len=1, input_dim]\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Embed input\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Get the output\n",
    "        x = x.squeeze(1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Pass through output layers to get logits for each state variable\n",
    "        logits = {var_name: head(x) for var_name, head in self.output_heads.items()}\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticRNNNextStatePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim_dict, num_layers=2, dropout=0.2):\n",
    "        \"\"\"\n",
    "        output_dim_dict: Dictionary mapping state variable names to number of bins\n",
    "        \"\"\"\n",
    "        super(ProbabilisticRNNNextStatePredictor, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim_dict = output_dim_dict\n",
    "        self.variable_names = list(output_dim_dict.keys())\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # RNN layers\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output layers - one for each state variable\n",
    "        self.output_heads = nn.ModuleDict({\n",
    "            var_name: nn.Linear(hidden_dim, num_bins) \n",
    "            for var_name, num_bins in output_dim_dict.items()\n",
    "        })\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape input: [batch_size, seq_len=1, input_dim]\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Pass through RNN\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        \n",
    "        # Get the output of last time step\n",
    "        rnn_out = rnn_out[:, -1, :]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Pass through output layers to get logits for each state variable\n",
    "        logits = {var_name: head(rnn_out) for var_name, head in self.output_heads.items()}\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_probabilistic_model(model_class, transitions, state_cols, epochs=50, batch_size=64, lr=0.001, **model_kwargs):\n",
    "# def train_probabilistic_model(model_class, transitions, state_cols, epochs=50, batch_size=1024, lr=0.001, **model_kwargs): # [gt_with_neighbors_update] changed batch_size from 64 to 1024\n",
    "    \"\"\"Train a probabilistic next state prediction model.\"\"\"\n",
    "    # Prepare dataset\n",
    "    dataset = StateTransitionDataset(transitions)\n",
    "    \n",
    "    # Calculate dimensions\n",
    "    input_dim = dataset.features.shape[1]\n",
    "    \n",
    "    # Create output_dim_dict mapping state variables to number of bins\n",
    "    output_dim_dict = {var_name: num_bins_dict[var_name] for var_name in state_cols}\n",
    "    \n",
    "    # Split into train, validation, and test sets\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(0.7 * dataset_size)\n",
    "    val_size = int(0.15 * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size], \n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = model_class(\n",
    "        input_dim=input_dim,\n",
    "        output_dim_dict=output_dim_dict,\n",
    "        **model_kwargs\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss function (Cross Entropy for categorical distributions)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    model_name = model_class.__name__.lower()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for features, targets in progress_bar:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device).long()  # Need long type for CrossEntropyLoss\n",
    "            \n",
    "            # Forward pass - get logits dictionary\n",
    "            logits_dict = model(features)\n",
    "            \n",
    "            # Calculate loss for each state variable and sum them\n",
    "            loss = 0\n",
    "            for i, var_name in enumerate(model.variable_names):\n",
    "                var_logits = logits_dict[var_name]\n",
    "                var_targets = targets[:, i]\n",
    "                loss += criterion(var_logits, var_targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item() / len(model.variable_names)})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device).long()\n",
    "                \n",
    "                logits_dict = model(features)\n",
    "                \n",
    "                # Calculate validation loss\n",
    "                batch_loss = 0\n",
    "                for i, var_name in enumerate(model.variable_names):\n",
    "                    var_logits = logits_dict[var_name]\n",
    "                    var_targets = targets[:, i]\n",
    "                    batch_loss += criterion(var_logits, var_targets)\n",
    "                \n",
    "                val_loss += batch_loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            model_path = os.path.join(mimic_iv_prefix_path, \"models\", f\"probabilistic_{model_name}_best_ground_truth_with_neighbors.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'variable_names': model.variable_names,\n",
    "                'output_dim_dict': model.output_dim_dict\n",
    "            }, model_path)\n",
    "            print(f\"Saved best model with validation loss: {val_loss:.6f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def probabilistic_next_state_prediction(model, key, n_samples=5):\n",
    "    \"\"\"Sample next states from the probabilistic model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input features\n",
    "    features = torch.tensor(flatten_key(key), dtype=torch.float32).to(device)\n",
    "    \n",
    "    next_states = []\n",
    "    with torch.no_grad():\n",
    "        # Get logits dictionary\n",
    "        logits_dict = model(features.unsqueeze(0))\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probs_dict = {\n",
    "            var_name: F.softmax(logits, dim=1) \n",
    "            for var_name, logits in logits_dict.items()\n",
    "        }\n",
    "        \n",
    "        # Sample from the distributions\n",
    "        for _ in range(n_samples):\n",
    "            state = []\n",
    "            for var_name in model.variable_names:\n",
    "                # Get probability distribution for this variable\n",
    "                probs = probs_dict[var_name].squeeze(0)\n",
    "                \n",
    "                # Sample from categorical distribution\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                sample = dist.sample()\n",
    "                \n",
    "                state.append(sample.item())\n",
    "            \n",
    "            next_states.append(tuple(state))\n",
    "    \n",
    "    return next_states\n",
    "\n",
    "def predict_with_uncertainty(model, key):\n",
    "    \"\"\"Get next state prediction with uncertainty metrics.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input features\n",
    "    features = torch.tensor(flatten_key(key), dtype=torch.float32).to(device)\n",
    "    \n",
    "    result = {}\n",
    "    with torch.no_grad():\n",
    "        # Get logits dictionary\n",
    "        logits_dict = model(features.unsqueeze(0))\n",
    "        \n",
    "        # For each state variable\n",
    "        for var_name in model.variable_names:\n",
    "            logits = logits_dict[var_name].squeeze(0)\n",
    "            probs = F.softmax(logits, dim=0)\n",
    "            \n",
    "            # Most likely outcome\n",
    "            most_likely = torch.argmax(probs).item()\n",
    "            \n",
    "            # Entropy (uncertainty measure)\n",
    "            entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
    "            \n",
    "            # Top 3 most likely values and their probabilities\n",
    "            values, indices = torch.topk(probs, min(3, len(probs)))\n",
    "            top_predictions = [(idx.item(), val.item()) for idx, val in zip(indices, values)]\n",
    "            \n",
    "            result[var_name] = {\n",
    "                'prediction': most_likely,\n",
    "                'entropy': entropy,\n",
    "                'top_predictions': top_predictions\n",
    "            }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_approximation(key, transitions, tree, k=10, bins=10, n_samples=1):\n",
    "    \"\"\"RNN-based approximation of next state.\"\"\"    \n",
    "    \n",
    "    # Check if the model is already loaded in the global namespace\n",
    "    if 'probabilistic_rnn_model' not in globals():\n",
    "        model_dir = os.path.join(mimic_iv_prefix_path, \"models\")\n",
    "        # model_path = os.path.join(model_dir, 'probabilistic_probabilisticrnnnextstatepredictor_best.pth')\n",
    "        # [gt_with_neighbors]\n",
    "        model_path = os.path.join(model_dir, 'probabilistic_probabilisticrnnnextstatepredictor_best_ground_truth_with_neighbors.pth')\n",
    "\n",
    "        try:\n",
    "            # Load the checkpoint\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "            \n",
    "            # Use the saved output_dim_dict from checkpoint if available\n",
    "            if 'output_dim_dict' in checkpoint:\n",
    "                output_dim_dict = checkpoint['output_dim_dict']\n",
    "            else:\n",
    "                output_dim_dict = {var_name: num_bins_dict[var_name] for var_name in state_cols}\n",
    "            \n",
    "            # Initialize the model\n",
    "            probabilistic_rnn = ProbabilisticRNNNextStatePredictor(\n",
    "                # input_dim=StateTransitionDataset(transitions).features.shape[1],\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=128,\n",
    "                num_layers=2,\n",
    "                output_dim_dict=output_dim_dict,\n",
    "                dropout=0.2\n",
    "            )\n",
    "            \n",
    "            # Load the state dict using the correct key\n",
    "            probabilistic_rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(\"Successfully loaded RNN model from:\", model_path)\n",
    "            globals()['probabilistic_rnn_model'] = probabilistic_rnn\n",
    "            \n",
    "        except (FileNotFoundError, KeyError) as e:\n",
    "            print(f\"Error loading RNN model: {e}\")\n",
    "            print(\"Training a new probabilistic RNN model...\")\n",
    "            probabilistic_rnn = train_probabilistic_model(\n",
    "                ProbabilisticRNNNextStatePredictor,\n",
    "                train_transitions, \n",
    "                state_cols, \n",
    "                hidden_dim=128,\n",
    "                num_layers=2,\n",
    "                dropout=0.2\n",
    "            )\n",
    "            globals()['probabilistic_rnn_model'] = probabilistic_rnn\n",
    "    \n",
    "    # Use the model for prediction\n",
    "    rnn_samples = probabilistic_next_state_prediction(globals()['probabilistic_rnn_model'], key, n_samples=n_samples)\n",
    "    \n",
    "    return rnn_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_approximation(key, transitions, tree, k=10, bins=10, n_samples=1):\n",
    "    \"\"\"Transformer-based approximation of next state.\"\"\"\n",
    "    \n",
    "    # Load probabilistic Transformer model\n",
    "    if 'probabilistic_transformer_model' not in globals():\n",
    "        model_dir = os.path.join(mimic_iv_prefix_path, \"models\")\n",
    "        # model_path = os.path.join(model_dir, 'probabilistic_probabilistictransformernextstatepredictor_best.pth')\n",
    "        # [gt_with_neighbors]\n",
    "        model_path = os.path.join(model_dir, 'probabilistic_probabilistictransformernextstatepredictor_best_ground_truth_with_neighbors.pth')\n",
    "        \n",
    "        try:\n",
    "            # Load the checkpoint\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "            \n",
    "            # Use the saved output_dim_dict from checkpoint if available\n",
    "            if 'output_dim_dict' in checkpoint:\n",
    "                output_dim_dict = checkpoint['output_dim_dict']\n",
    "            else:\n",
    "                output_dim_dict = {var_name: num_bins_dict[var_name] for var_name in state_cols}\n",
    "            \n",
    "            # Initialize the model\n",
    "            probabilistic_transformer = ProbabilisticTransformerNextStatePredictor(\n",
    "                # input_dim=StateTransitionDataset(transitions).features.shape[1],\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=128,\n",
    "                num_layers=2,\n",
    "                output_dim_dict=output_dim_dict,\n",
    "                dropout=0.2,\n",
    "                nhead=4  # Add the nhead parameter that was used in training\n",
    "            )\n",
    "            \n",
    "            # Load the state dict using the correct key\n",
    "            probabilistic_transformer.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(\"Successfully loaded Transformer model from:\", model_path)\n",
    "            globals()['probabilistic_transformer_model'] = probabilistic_transformer\n",
    "            \n",
    "        except (FileNotFoundError, KeyError) as e:\n",
    "            print(f\"Error loading Transformer model: {e}\")\n",
    "            print(\"Training a new probabilistic Transformer model...\")\n",
    "            probabilistic_transformer = train_probabilistic_model(\n",
    "                ProbabilisticTransformerNextStatePredictor,\n",
    "                train_transitions, \n",
    "                state_cols, \n",
    "                hidden_dim=128, \n",
    "                nhead=4,\n",
    "                num_layers=2,\n",
    "                dropout=0.2\n",
    "            )\n",
    "            globals()['probabilistic_transformer_model'] = probabilistic_transformer\n",
    "    \n",
    "    # Use the cached model for prediction\n",
    "    transformer_samples = probabilistic_next_state_prediction(\n",
    "        globals()['probabilistic_transformer_model'], \n",
    "        key, \n",
    "        n_samples=n_samples\n",
    "    )\n",
    "    \n",
    "    return transformer_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get any key from transitions\n",
    "first_key = next(iter(transitions))\n",
    "# Flatten it to get the feature vector\n",
    "sample_features = flatten_key(first_key)\n",
    "# Get the dimension\n",
    "input_dim = len(sample_features)\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load probabilistic RNN model\n",
    "if 'probabilistic_rnn_model' not in globals():\n",
    "    model_dir = os.path.join(mimic_iv_prefix_path, \"models\")\n",
    "    # model_path = os.path.join(model_dir, 'probabilistic_probabilisticrnnnextstatepredictor_best.pth')\n",
    "    # [gt_with_neighbors]\n",
    "    model_path = os.path.join(model_dir, 'probabilistic_probabilisticrnnnextstatepredictor_best_ground_truth_with_neighbors.pth')\n",
    "    \n",
    "    try:\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        \n",
    "        # Use the saved output_dim_dict from checkpoint if available\n",
    "        if 'output_dim_dict' in checkpoint:\n",
    "            output_dim_dict = checkpoint['output_dim_dict']\n",
    "        else:\n",
    "            output_dim_dict = {var_name: num_bins_dict[var_name] for var_name in state_cols}\n",
    "        \n",
    "        # Initialize the model\n",
    "        probabilistic_rnn = ProbabilisticRNNNextStatePredictor(\n",
    "            # input_dim=StateTransitionDataset(transitions).features.shape[1],\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=128,\n",
    "            num_layers=2,\n",
    "            output_dim_dict=output_dim_dict,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Load the state dict using the correct key\n",
    "        probabilistic_rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Successfully loaded RNN model from:\", model_path)\n",
    "        globals()['probabilistic_rnn_model'] = probabilistic_rnn\n",
    "        \n",
    "    except (FileNotFoundError, KeyError) as e:\n",
    "        print(f\"Error loading RNN model: {e}\")\n",
    "        print(\"Training a new probabilistic RNN model...\")\n",
    "        probabilistic_rnn = train_probabilistic_model(\n",
    "            ProbabilisticRNNNextStatePredictor,\n",
    "            train_transitions, \n",
    "            state_cols, \n",
    "            hidden_dim=128,\n",
    "            num_layers=2,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        globals()['probabilistic_rnn_model'] = probabilistic_rnn\n",
    "\n",
    "# Load probabilistic Transformer model\n",
    "if 'probabilistic_transformer_model' not in globals():\n",
    "    model_dir = os.path.join(mimic_iv_prefix_path, \"models\")\n",
    "    # model_path = os.path.join(model_dir, 'probabilistic_probabilistictransformernextstatepredictor_best.pth')\n",
    "    # [gt_with_neighbors]\n",
    "    model_path = os.path.join(model_dir, 'probabilistic_probabilistictransformernextstatepredictor_best_ground_truth_with_neighbors.pth')\n",
    "    \n",
    "    try:\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        \n",
    "        # Use the saved output_dim_dict from checkpoint if available\n",
    "        if 'output_dim_dict' in checkpoint:\n",
    "            output_dim_dict = checkpoint['output_dim_dict']\n",
    "        else:\n",
    "            output_dim_dict = {var_name: num_bins_dict[var_name] for var_name in state_cols}\n",
    "        \n",
    "        # Initialize the model\n",
    "        probabilistic_transformer = ProbabilisticTransformerNextStatePredictor(\n",
    "            # input_dim=StateTransitionDataset(transitions).features.shape[1],\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=128,\n",
    "            num_layers=2,\n",
    "            output_dim_dict=output_dim_dict,\n",
    "            dropout=0.2,\n",
    "            nhead=4  # Add the nhead parameter that was used in training\n",
    "        )\n",
    "        \n",
    "        # Load the state dict using the correct key\n",
    "        probabilistic_transformer.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Successfully loaded Transformer model from:\", model_path)\n",
    "        globals()['probabilistic_transformer_model'] = probabilistic_transformer\n",
    "        \n",
    "    except (FileNotFoundError, KeyError) as e:\n",
    "        print(f\"Error loading Transformer model: {e}\")\n",
    "        print(\"Training a new probabilistic Transformer model...\")\n",
    "        probabilistic_transformer = train_probabilistic_model(\n",
    "            ProbabilisticTransformerNextStatePredictor,\n",
    "            train_transitions, \n",
    "            state_cols, \n",
    "            hidden_dim=128, \n",
    "            nhead=4,\n",
    "            num_layers=2,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        globals()['probabilistic_transformer_model'] = probabilistic_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example key\n",
    "example_key = next(iter(train_transitions.keys()))\n",
    "print(f\"Example key: {example_key}\")\n",
    "print(\"True next states:\", train_transitions[example_key])\n",
    "\n",
    "# Sample multiple next states\n",
    "transformer_samples = probabilistic_next_state_prediction(probabilistic_transformer, example_key, n_samples=5)\n",
    "rnn_samples = probabilistic_next_state_prediction(probabilistic_rnn, example_key, n_samples=5)\n",
    "\n",
    "print(\"Transformer samples:\", transformer_samples)\n",
    "print(\"RNN samples:\", rnn_samples)\n",
    "\n",
    "# Get predictions with uncertainty\n",
    "transformer_predictions = predict_with_uncertainty(probabilistic_transformer, example_key)\n",
    "rnn_predictions = predict_with_uncertainty(probabilistic_rnn, example_key)\n",
    "\n",
    "# Example of accessing the results\n",
    "for var_name in state_cols:\n",
    "    print(f\"\\nVariable: {var_name}\")\n",
    "    print(\"Transformer prediction:\", transformer_predictions[var_name]['prediction'], \n",
    "          \"Entropy:\", transformer_predictions[var_name]['entropy'])\n",
    "    print(\"Top predictions:\", transformer_predictions[var_name]['top_predictions'])\n",
    "    print(\"RNN prediction:\", rnn_predictions[var_name]['prediction'], \n",
    "          \"Entropy:\", rnn_predictions[var_name]['entropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for train (mimic_iv_disc)\n",
    "sample_key = train_keys_list[0] if train_keys_list else None\n",
    "if sample_key: # TODO: the baseline showed but no used warning\n",
    "    print(f\"Sample Key: {sample_key}, train_transitions: {train_transitions[sample_key]}\")\n",
    "    print(f\"Cont Sample Key: {train_cont_keys_list[0]}, train_transitions: {train_cont_transitions[train_cont_keys_list[0]]}\")\n",
    "    print(f\"Keep current state: {keep_current_state_approximation(sample_key, train_transitions, train_tree)}\")\n",
    "    print(f\"Random walk: {random_walk_approximation(sample_key, train_transitions, train_tree)}\")\n",
    "    print(f\"Action-Based: {action_based_approximation(sample_key, train_transitions, train_tree)}\")\n",
    "    print(f\"Action-Based continuous: {action_based_approximation_cont(train_cont_keys_list[0], train_cont_transitions, train_cont_tree)}\") # [cont_update]\n",
    "    print(f\"State-Based: {state_based_approximation(sample_key, train_transitions, train_tree)}\")\n",
    "    print(f\"Bayesian: {bayesian_approximation(sample_key, train_transitions, train_tree)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output of each strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(train_transitions.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use subset of train_transitions for testing if need to retrian the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use subset of train_transitions as train_transitions_subset\n",
    "train_transitions_subset = dict(random.sample(list(train_transitions.items()), 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_transitions_subset.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use subset for estimate the running time for train model\n",
    "sample_key = list(train_transitions.keys())[0]\n",
    "if sample_key: # TODO: the baseline showed but no used warning\n",
    "    print(f\"Sample Key: {sample_key}, train_transitions: {train_transitions[sample_key]}\")\n",
    "    print(f\"Keep current state: {keep_current_state_approximation(sample_key, train_transitions, train_tree, n_samples=30)}\")\n",
    "    print(f\"Random walk: {random_walk_approximation(sample_key, train_transitions, train_tree, n_samples=30)}\")\n",
    "    print(f\"Action-Based: {action_based_approximation(sample_key, train_transitions, train_tree, n_samples=30)}\")\n",
    "    print(f\"Action-Based continuous: {action_based_approximation_cont(train_cont_keys_list[0], train_cont_transitions, train_cont_tree, n_samples=30)}\") # [cont_update]\n",
    "    print(f\"State-Based: {state_based_approximation(sample_key, train_transitions, train_tree, n_samples=30)}\")\n",
    "    print(f\"Bayesian: {bayesian_approximation(sample_key, train_transitions, train_tree, n_samples=30)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Baseline features with lower priority)\n",
    "- transition structure: next state with baseline, when sample need to consider baseline first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition level Evaluation\n",
    "- Internal\n",
    "    - train (avoid overfitting)\n",
    "    - test\n",
    "        - seen key pair\n",
    "        - unseen key pair\n",
    "- External\n",
    "    - seen key pair (common in three datasets)\n",
    "    - unseen (in MIMIC-III and eICU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsition_approximation_strategies = {\n",
    "    \"Keep Current State\": keep_current_state_approximation,\n",
    "    \"Random Walk\": random_walk_approximation,\n",
    "    \"Bayesian\": bayesian_approximation,\n",
    "    \"Action-Based KNN\": action_based_approximation,\n",
    "    \"Action-Based KNN Continuous\": action_based_approximation_cont, # [cont_update]\n",
    "    \"GRU\": rnn_approximation, # \"Probabilistic RNN\"\n",
    "    \"Transformer\": transformer_approximation # \"Probabilistic Transformer\"\n",
    "}\n",
    "trainsition_approximation_strategies_names = list(trainsition_approximation_strategies.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MMD with multiple n samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "def MMD_evaluation(approximation_func, keys, true_transitions, train_transitions, tree, k=100, bins=bins):\n",
    "    mmd_scores = []\n",
    "    for key in keys:\n",
    "        true_next_states = np.array(true_transitions.get(key, []))\n",
    "        if len(true_next_states) == 0:\n",
    "            print(f\"Skipping key {key} due to no true transitions.\")\n",
    "            continue\n",
    "\n",
    "        # Get all approximated next states at once (more efficient)\n",
    "        # approx_next_states = np.array(approximation_func(key, train_transitions, tree, bins, n_samples=30))\n",
    "        approx_next_states = np.array(approximation_func(key, train_transitions, tree, k, bins, n_samples=30)) # [DEBUG]\n",
    "        \n",
    "        if len(approx_next_states) == 0:\n",
    "            print(f\"Skipping key {key} due to no approximated transitions.\")\n",
    "            continue\n",
    "\n",
    "        # Compute squared distances\n",
    "        xx = cdist(true_next_states, true_next_states, 'euclidean')**2\n",
    "        yy = cdist(approx_next_states, approx_next_states, 'euclidean')**2\n",
    "        xy = cdist(true_next_states, approx_next_states, 'euclidean')**2\n",
    "\n",
    "        # Median heuristic for bandwidth\n",
    "        all_states = np.vstack([true_next_states, approx_next_states])\n",
    "        all_distances = cdist(all_states, all_states, 'euclidean')\n",
    "        sigma = np.median(all_distances[all_distances > 0])\n",
    "\n",
    "        # Gaussian kernel\n",
    "        k_xx = np.exp(-xx / (2 * sigma**2))\n",
    "        k_yy = np.exp(-yy / (2 * sigma**2))\n",
    "        k_xy = np.exp(-xy / (2 * sigma**2))\n",
    "\n",
    "        # Unbiased MMD^2\n",
    "        m, n = len(true_next_states), len(approx_next_states)\n",
    "        if m > 1:\n",
    "            term1 = (np.sum(k_xx) - np.trace(k_xx)) / (m * (m - 1))\n",
    "        else:\n",
    "            term1 = 0  # or use biased estimator\n",
    "        term2 = (np.sum(k_yy) - np.trace(k_yy)) / (n * (n - 1))\n",
    "        term3 = np.sum(k_xy) / (m * n)\n",
    "        mmd = np.sqrt(max(0, term1 + term2 - 2 * term3))\n",
    "        mmd_scores.append(mmd)\n",
    "\n",
    "    if not mmd_scores:\n",
    "        print(f\"No valid MMD scores computed for keys: {keys}\")\n",
    "        return float('inf')\n",
    "    return np.mean(mmd_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [cont_update]\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "def MMD_evaluation_ground_truth_with_neighbors(approximation_func, keys, true_transitions, train_transitions, tree, ground_truth_tree, k=100, bins=bins, true_keys_list=train_keys_list, is_cont=False):\n",
    "    mmd_scores = []\n",
    "    for key in keys:\n",
    "        # true_next_states = np.array(true_transitions.get(key, []))\n",
    "        if not is_cont:\n",
    "            true_next_states = np.array(action_based_approximation(key, true_transitions, ground_truth_tree, k, bins, n_samples=30, keys_list=true_keys_list, used_for_ground_truth_with_neighbors=True)) # [gt_with_neighbors_update]\n",
    "        else:\n",
    "            true_next_states = np.array(action_based_approximation_cont(key, true_transitions, ground_truth_tree, k, bins, n_samples=30, keys_list=true_keys_list, used_for_ground_truth_with_neighbors=True)) # [gt_with_neighbors_update]\n",
    "        if len(true_next_states) == 0:\n",
    "            print(f\"Skipping key {key} due to no true transitions.\")\n",
    "            continue\n",
    "\n",
    "        # Get all approximated next states at once (more efficient)\n",
    "        approx_next_states = np.array(approximation_func(key, train_transitions, tree, k, bins, n_samples=30)) # [cont_update] DEBUG, since when approximation it will default use disc train_keys_list, need to use cont when input cont\n",
    "        # approx_next_states = np.array(approximation_func(key, train_transitions, tree, k, bins, n_samples=30, key_list=train_keys_list)) # [cont_update] DEBUG, since when approximation it will default use disc train_keys_list, need to use cont when input cont\n",
    "        \n",
    "        if len(approx_next_states) == 0:\n",
    "            print(f\"Skipping key {key} due to no approximated transitions.\")\n",
    "            continue\n",
    "\n",
    "        # Compute squared distances\n",
    "        xx = cdist(true_next_states, true_next_states, 'euclidean')**2\n",
    "        yy = cdist(approx_next_states, approx_next_states, 'euclidean')**2\n",
    "        xy = cdist(true_next_states, approx_next_states, 'euclidean')**2\n",
    "\n",
    "        # Median heuristic for bandwidth\n",
    "        all_states = np.vstack([true_next_states, approx_next_states])\n",
    "        all_distances = cdist(all_states, all_states, 'euclidean')\n",
    "        sigma = np.median(all_distances[all_distances > 0])\n",
    "\n",
    "        # Gaussian kernel\n",
    "        k_xx = np.exp(-xx / (2 * sigma**2))\n",
    "        k_yy = np.exp(-yy / (2 * sigma**2))\n",
    "        k_xy = np.exp(-xy / (2 * sigma**2))\n",
    "\n",
    "        # Unbiased MMD^2\n",
    "        m, n = len(true_next_states), len(approx_next_states)\n",
    "        if m > 1:\n",
    "            term1 = (np.sum(k_xx) - np.trace(k_xx)) / (m * (m - 1))\n",
    "        else:\n",
    "            term1 = 0  # or use biased estimator\n",
    "        term2 = (np.sum(k_yy) - np.trace(k_yy)) / (n * (n - 1))\n",
    "        term3 = np.sum(k_xy) / (m * n)\n",
    "        mmd = np.sqrt(max(0, term1 + term2 - 2 * term3))\n",
    "        mmd_scores.append(mmd)\n",
    "\n",
    "    if not mmd_scores:\n",
    "        print(f\"No valid MMD scores computed for keys: {keys}\")\n",
    "        return float('inf')\n",
    "    return np.mean(mmd_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Internal & External Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filter transitions based on the number of next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subgroup of train_transitions: select those next state occurrance > 1\n",
    "def filter_transitions(transitions, min_occurrences=2):\n",
    "    filtered_transitions = {}\n",
    "    for key, next_states in transitions.items():\n",
    "        if len(next_states) >= min_occurrences:\n",
    "            filtered_transitions[key] = next_states\n",
    "    return filtered_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [cont_update]\n",
    "# train_filtered_transitions = filter_transitions(train_transitions, 5)\n",
    "# test_filtered_transitions = filter_transitions(test_transitions, 5)\n",
    "# train_filtered_transitions = filter_transitions(train_transitions, 0) # [WARNING] def filter_transitions vs. def filter_transitions_by_stay_id \n",
    "# test_filtered_transitions = filter_transitions(test_transitions, 0)\n",
    "\n",
    "print(f\"train_transitions: {len(train_transitions.keys())}\")\n",
    "print(f\"test_transitions: {len(test_transitions.keys())}\")\n",
    "print(f\"train_filtered_transitions: {len(train_filtered_transitions.keys())}\")\n",
    "print(f\"test_filtered_transitions: {len(test_filtered_transitions.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filter transitions based on distinct stay_id number appear in this key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train_transitions: {len(train_transitions.keys())}\")\n",
    "print(f\"test_transitions: {len(test_transitions.keys())}\")\n",
    "print(f\"train_filtered_transitions: {len(train_filtered_transitions.keys())}\")\n",
    "print(f\"test_filtered_transitions: {len(test_filtered_transitions.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filter_transitions_by_stay_id with min_stay_ids 3\n",
    "- sample 1000 keys, but test_unseen_keys only have 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition level evaluation MMD combine discrete with continuous, all use ground truth with neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_disc_cont_MMD(methods, scenario_keys, scenario_transitions, scenario_tree, scenario_keys_list,\n",
    "                    train_transitions, train_tree, bins, k,\n",
    "                    cont_keys=None, cont_transitions=None, cont_tree=None, cont_keys_list=None,\n",
    "                    train_cont_transitions=None, train_cont_tree=None, train_cont_keys_list=None):\n",
    "    results = {}\n",
    "    for name, func in methods.items():\n",
    "        if name == \"Action-Based KNN Continuous\":\n",
    "            # Use continuous data\n",
    "            keys = cont_keys if cont_keys is not None else scenario_keys\n",
    "            transitions = cont_transitions if cont_transitions is not None else scenario_transitions\n",
    "            tree = cont_tree if cont_tree is not None else scenario_tree\n",
    "            keys_list = cont_keys_list if cont_keys_list is not None else scenario_keys_list\n",
    "            train_trans = train_cont_transitions if train_cont_transitions is not None else train_transitions\n",
    "            train_tr = train_cont_tree if train_cont_tree is not None else train_tree\n",
    "            train_keys_l = train_cont_keys_list if train_cont_keys_list is not None else train_keys_list\n",
    "            is_cont = True\n",
    "        else:\n",
    "            # Use discrete data\n",
    "            keys = scenario_keys\n",
    "            transitions = scenario_transitions\n",
    "            tree = scenario_tree\n",
    "            keys_list = scenario_keys_list\n",
    "            train_trans = train_transitions\n",
    "            train_tr = train_tree\n",
    "            train_keys_l = train_keys_list\n",
    "            is_cont = False\n",
    "        results[name] = MMD_evaluation_ground_truth_with_neighbors(\n",
    "            func, keys, transitions, train_trans, train_tr, tree, k, bins, keys_list, is_cont\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [cont_update]\n",
    "k = 100\n",
    "# Evaluate on training data\n",
    "# Sample training keys for training evaluation\n",
    "train_keys = random.sample(list(train_filtered_transitions.keys()), min(test_cases_num, len(train_filtered_transitions)))\n",
    "train_cont_keys = random.sample(list(train_cont_filtered_transitions.keys()), min(test_cases_num, len(train_cont_filtered_transitions)))\n",
    "\n",
    "# Separate test keys into seen and unseen with exactly test_cases_num keys each\n",
    "seen_keys = [key for key in test_filtered_transitions.keys() if key in train_transitions]\n",
    "unseen_keys = [key for key in test_filtered_transitions.keys() if key not in train_transitions]\n",
    "# Sample test_cases_num keys from seen and unseen keys\n",
    "test_seen_keys = random.sample(seen_keys, min(test_cases_num, len(seen_keys)))\n",
    "test_unseen_keys = random.sample(unseen_keys, min(test_cases_num, len(unseen_keys)))\n",
    "\n",
    "# Separate test keys into seen and unseen with exactly test_cases_num keys each\n",
    "cont_seen_keys = [key for key in test_cont_filtered_transitions.keys() if key in train_cont_transitions]\n",
    "cont_unseen_keys = [key for key in test_cont_filtered_transitions.keys() if key not in train_cont_transitions]\n",
    "test_cont_seen_keys = random.sample(cont_seen_keys, min(test_cases_num, len(cont_seen_keys)))\n",
    "test_cont_unseen_keys = random.sample(cont_unseen_keys, min(test_cases_num, len(cont_unseen_keys)))\n",
    "\n",
    "# --- Training ---\n",
    "train_results = combine_disc_cont_MMD(\n",
    "    trainsition_approximation_strategies,\n",
    "    train_keys, train_transitions, train_tree, train_keys_list,\n",
    "    train_transitions, train_tree, bins, k,\n",
    "    cont_keys=train_cont_keys, cont_transitions=train_cont_transitions, cont_tree=train_cont_tree, cont_keys_list=train_cont_keys_list,\n",
    "    train_cont_transitions=train_cont_transitions, train_cont_tree=train_cont_tree, train_cont_keys_list=train_cont_keys_list\n",
    ")\n",
    "print(\"train done\")\n",
    "\n",
    "# --- Test Seen ---\n",
    "test_seen_results = combine_disc_cont_MMD(\n",
    "    trainsition_approximation_strategies,\n",
    "    test_seen_keys, test_transitions, test_tree, test_keys_list,\n",
    "    train_transitions, train_tree, bins, k,\n",
    "    cont_keys=test_cont_seen_keys, cont_transitions=test_cont_transitions, cont_tree=test_cont_tree, cont_keys_list=test_cont_keys_list,\n",
    "    train_cont_transitions=train_cont_transitions, train_cont_tree=train_cont_tree, train_cont_keys_list=train_cont_keys_list\n",
    ")\n",
    "print(\"test seen done\")\n",
    "\n",
    "# --- Test Unseen ---\n",
    "test_unseen_results = combine_disc_cont_MMD(\n",
    "    trainsition_approximation_strategies,\n",
    "    test_unseen_keys, test_transitions, test_tree, test_keys_list,\n",
    "    train_transitions, train_tree, bins, k,\n",
    "    cont_keys=test_cont_unseen_keys, cont_transitions=test_cont_transitions, cont_tree=test_cont_tree, cont_keys_list=test_cont_keys_list,\n",
    "    train_cont_transitions=train_cont_transitions, train_cont_tree=train_cont_tree, train_cont_keys_list=train_cont_keys_list\n",
    ")\n",
    "print(\"test unseen done\")\n",
    "\n",
    "# --- External Common ---\n",
    "external_common_results = combine_disc_cont_MMD(\n",
    "    trainsition_approximation_strategies,\n",
    "    external_common, eicu_trans, eicu_tree, eicu_keys_list,\n",
    "    train_transitions, train_tree, bins, k,\n",
    "    cont_keys=external_common_cont, cont_transitions=eicu_cont_trans, cont_tree=eicu_cont_tree, cont_keys_list=eicu_cont_keys_list,\n",
    "    train_cont_transitions=train_cont_transitions, train_cont_tree=train_cont_tree, train_cont_keys_list=train_cont_keys_list\n",
    ")\n",
    "print(\"external common done\")\n",
    "\n",
    "# --- External Unique ---\n",
    "external_unique_results = combine_disc_cont_MMD(\n",
    "    trainsition_approximation_strategies,\n",
    "    external_not_mimic_iv, eicu_trans, eicu_tree, eicu_keys_list,\n",
    "    train_transitions, train_tree, bins, k,\n",
    "    cont_keys=external_not_mimic_iv_cont, cont_transitions=eicu_cont_trans, cont_tree=eicu_cont_tree, cont_keys_list=eicu_cont_keys_list,\n",
    "    train_cont_transitions=train_cont_transitions, train_cont_tree=train_cont_tree, train_cont_keys_list=train_cont_keys_list\n",
    ")\n",
    "print(\"external unique done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "x = np.arange(len(trainsition_approximation_strategies))\n",
    "width = 0.15\n",
    "\n",
    "plt.bar(x - 2*width, [train_results[name] for name in trainsition_approximation_strategies], width, label='Internal (Training)')\n",
    "plt.bar(x - width, [test_seen_results[name] for name in trainsition_approximation_strategies], width, label='Internal (Seen Keys)')\n",
    "plt.bar(x, [test_unseen_results[name] for name in trainsition_approximation_strategies], width, label='Internal (Unseen Keys)')\n",
    "plt.bar(x + width, [external_common_results[name] for name in trainsition_approximation_strategies], width, label='External (Seen Keys)')\n",
    "plt.bar(x + 2*width, [external_unique_results[name] for name in trainsition_approximation_strategies], width, label='External (Unseen Keys)')\n",
    "\n",
    "plt.title('Method Performance Across Different Evaluation Scenarios')\n",
    "plt.xticks(x, trainsition_approximation_strategies.keys(), rotation=45, ha='right')\n",
    "plt.ylabel('MMD Score (Lower is Better)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def print_results(title, results):\n",
    "    print(f\"\\n{title}:\")\n",
    "    for method, score in results.items():\n",
    "        print(f\" {method}: {score:.4f}\")\n",
    "\n",
    "print_results(\"Internal (Training)\", train_results)\n",
    "print_results(\"Internal (Seen Keys)\", test_seen_results)\n",
    "print_results(\"Internal (Unseen Keys)\", test_unseen_results)\n",
    "print_results(\"External (Seen Keys)\", external_common_results)\n",
    "print_results(\"External (Unseen Keys)\", external_unique_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# List of scenarios\n",
    "scenarios = [\n",
    "    \"Internal (Training)\",\n",
    "    \"Internal (Seen Keys)\",\n",
    "    \"Internal (Unseen Keys)\",\n",
    "    \"External (Seen Keys)\",\n",
    "    \"External (Unseen Keys)\"\n",
    "]\n",
    "\n",
    "# List of methods (transition approximation strategies)\n",
    "methods = list(trainsition_approximation_strategies.keys())\n",
    "\n",
    "# Gather the data for each scenario, in the order of methods\n",
    "data = [\n",
    "    [train_results[m] for m in methods],\n",
    "    [test_seen_results[m] for m in methods],\n",
    "    [test_unseen_results[m] for m in methods],\n",
    "    [external_common_results[m] for m in methods],\n",
    "    [external_unique_results[m] for m in methods],\n",
    "]\n",
    "\n",
    "data = np.array(data)  # shape: (num_scenarios, num_methods)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(16, 8))\n",
    "x = np.arange(len(scenarios))\n",
    "width = 0.1\n",
    "\n",
    "# Plot each method as a bar in each scenario group\n",
    "for i, method in enumerate(methods):\n",
    "    plt.bar(x + (i - len(methods)/2)*width + width/2, data[:, i], width, label=method)\n",
    "\n",
    "plt.title('Method Performance Across Different Evaluation Scenarios')\n",
    "plt.xticks(x, scenarios, rotation=30, ha='right')\n",
    "plt.ylabel('MMD Score (Lower is Better)')\n",
    "plt.legend(title=\"Transition Approximation Strategy\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average next_state number for train seen keys\n",
    "train_keys_avg_next_state = np.mean([len(train_transitions[key]) for key in train_keys])\n",
    "print(f\"Average number of next states for train keys: {train_keys_avg_next_state:.2f}, train keys: {len(train_keys)}\")\n",
    "# Show average next_state number for train seen keys\n",
    "test_seen_keys_avg_next_state = np.mean([len(test_transitions[key]) for key in test_seen_keys])\n",
    "print(f\"Average number of next states for test seen keys: {test_seen_keys_avg_next_state:.2f}, test seen keys: {len(test_seen_keys)}\")\n",
    "# show test_unseen_keys average next_state number\n",
    "test_unseen_keys_avg_next_state = np.mean([len(test_transitions[key]) for key in test_unseen_keys])\n",
    "print(f\"Average number of next states for test unseen keys: {test_unseen_keys_avg_next_state:.2f}, test unseen keys: {len(test_unseen_keys)}\")\n",
    "# show external_common average next_state number\n",
    "external_common_keys_avg_next_state = np.mean([len(eicu_mimic_iii_mimic_iv_train_trans[key]) for key in external_common])\n",
    "print(f\"Average number of next states for external common keys: {external_common_keys_avg_next_state:.2f}, external common keys: {len(external_common)}\")\n",
    "# show external_unique average next_state number\n",
    "external_unique_key_avg_next_state = np.mean([len(eicu_mimic_iii_trans[key]) for key in external_not_mimic_iv])\n",
    "print(f\"Average number of next states for external unique keys: {external_unique_key_avg_next_state:.2f}, external unique keys: {len(external_not_mimic_iv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example transitions for each evaluation category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print example transitions for each category\n",
    "# for i in range(1):\n",
    "#     print(f\"train:\\nkey: {train_keys[i]}\\nvalue: {train_transitions[train_keys[i]]}\")\n",
    "#     print(f\"test_seen:\\nkey: {test_seen_keys[i]}\\nvalue: {test_transitions[test_seen_keys[i]]}\")\n",
    "#     print(f\"test_unseen:\\nkey: {test_unseen_keys[i]}\\nvalue: {test_transitions[test_unseen_keys[i]]}\")\n",
    "#     print(f\"external_common:\\nkey: {external_common[i]}\\nvalue: {eicu_mimic_iii_mimic_iv_train_trans[external_common[i]]}\")\n",
    "#     print(f\"external_unique:\\nkey: {external_not_mimic_iv[i]}\\nvalue: {eicu_mimic_iii_trans[external_not_mimic_iv[i]]}\")\n",
    "\n",
    "# use action_based_approximation to generate next state for each of the above keys\n",
    "# def sample_multi_run(approximation_func, keys, transitions, tree, bins=bins):\n",
    "#     approx_next_states = np.array([approximation_func(keys, transitions, tree, bins) for _ in range(10)])\n",
    "#     # turn it into a list of tuples\n",
    "#     approx_next_states = [tuple(state) for state in approx_next_states]\n",
    "#     return approx_next_states\n",
    "\n",
    "# for i in range(1):\n",
    "#     print(f\"train:\\nkey: {train_keys[i]}\\nvalue: {train_transitions[train_keys[i]]}\")\n",
    "#     print(f\"train_approx:\\nvalue: {sample_multi_run(action_based_approximation, train_keys[i], train_transitions, train_tree)}\")\n",
    "#     print(f\"test_seen:\\nkey: {test_seen_keys[i]}\\nvalue: {test_transitions[test_seen_keys[i]]}\")\n",
    "#     print(f\"test_seen_approx:\\nvalue: {sample_multi_run(action_based_approximation, test_seen_keys[i], test_transitions, train_tree)}\")\n",
    "#     print(f\"test_unseen:\\nkey: {test_unseen_keys[i]}\\nvalue: {test_transitions[test_unseen_keys[i]]}\")\n",
    "#     print(f\"test_unseen_approx:\\nvalue: {sample_multi_run(action_based_approximation, test_unseen_keys[i], train_transitions, train_tree)}\")\n",
    "#     # print(f\"external_common:\\nkey: {external_common[i]}\\nvalue: {eicu_mimic_iii_mimic_iv_train_trans[external_common[i]]}\")\n",
    "#     # print(f\"external_common_approx:\\nvalue: {action_based_approximation(eicu_mimic_iii_mimic_iv_train_trans[external_common[i]], eicu_mimic_iii_mimic_iv_train_trans, train_tree)}\")\n",
    "#     # print(f\"external_unique:\\nkey: {external_not_mimic_iv[i]}\\nvalue: {eicu_mimic_iii_trans[external_not_mimic_iv[i]]}\")\n",
    "#     # print(f\"external_unique_approx:\\nvalue: {action_based_approximation(eicu_mimic_iii_trans[external_not_mimic_iv[i]], eicu_mimic_iii_trans, train_tree)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different K for action based approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training data\n",
    "# Sample training keys for training evaluation\n",
    "train_keys = random.sample(list(train_filtered_transitions.keys()), min(test_cases_num, len(train_filtered_transitions)))\n",
    "\n",
    "# Separate test keys into seen and unseen with exactly test_cases_num keys each\n",
    "seen_keys = [key for key in test_filtered_transitions.keys() if key in train_transitions]\n",
    "unseen_keys = [key for key in test_filtered_transitions.keys() if key not in train_transitions]\n",
    "# Sample test_cases_num keys from seen and unseen keys\n",
    "test_seen_keys = random.sample(seen_keys, min(test_cases_num, len(seen_keys)))\n",
    "test_unseen_keys = random.sample(unseen_keys, min(test_cases_num, len(unseen_keys)))\n",
    "# test_seen_keys = seen_keys[:min(test_cases_num, len(seen_keys))] # random.sample(seen_keys, min(test_cases_num, len(seen_keys)))\n",
    "# test_unseen_keys = unseen_keys[:min(test_cases_num, len(unseen_keys))] # random.sample(unseen_keys, min(test_cases_num, len(unseen_keys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [cont_update] [ground_truth_with_neighbors] \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_mmd_action_based_approximation_ground_truth_with_neighbors(k_values, data_sets, train_transitions, tree, bins):\n",
    "    \"\"\"\n",
    "    Visualize MMD scores for the 'action based approximation' method with different k values across multiple datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    - k_values (list): List of k values to evaluate (e.g., [3, 10, 30, 50, 100]).\n",
    "    - data_sets (list): List of tuples, each containing (label, keys, true_transitions).\n",
    "    - train_transitions (dict): Dictionary of training transitions for approximation.\n",
    "    - tree (object): KD-tree object for nearest neighbor search.\n",
    "    - bins (int): Number of bins used for discretization.\n",
    "    \n",
    "    Returns:\n",
    "    - None: Displays a single plot of MMD scores vs. k values for all datasets.\n",
    "    \"\"\"\n",
    "    # Function to create an approximation function with a fixed k\n",
    "    def make_action_based_approx(k):\n",
    "        def approx_func(key, transitions, tree, k, bins, n_samples):\n",
    "            return action_based_approximation(key, transitions, tree, k=k, bins=bins, n_samples=n_samples)\n",
    "        return approx_func\n",
    "    \n",
    "    # Create a single figure for all datasets\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Iterate over each dataset\n",
    "    for label, keys, true_transitions, true_tree, true_keys_list in data_sets:\n",
    "        mmd_scores = []\n",
    "        for k in k_values:\n",
    "            # Create approximation function with current k\n",
    "            approx_func = make_action_based_approx(k)\n",
    "            # Compute MMD score\n",
    "            # mmd = MMD_evaluation(approx_func, keys, true_transitions, train_transitions, tree, bins)\n",
    "            mmd = MMD_evaluation_ground_truth_with_neighbors(approx_func, keys, true_transitions, train_transitions, tree, true_tree, k, bins, true_keys_list)\n",
    "            mmd_scores.append(mmd)\n",
    "            print(f\"MMD for {label}, k={k}: {mmd:.4f}\")\n",
    "        \n",
    "        # Plot the MMD scores for this dataset\n",
    "        plt.plot(k_values, mmd_scores, marker='o', linestyle='-', label=label)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('k (Number of Neighbors)')\n",
    "    plt.ylabel('MMD Score')\n",
    "    plt.title('MMD vs k for Action Based Approximation Across Datasets')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(k_values)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Define the datasets to visualize together\n",
    "data_sets = [\n",
    "    (\"Internal (Training)\", train_keys, train_transitions, train_tree, train_keys_list),\n",
    "    (\"Internal (Seen Keys)\", test_seen_keys, test_transitions, test_tree, test_keys_list),\n",
    "    (\"Internal (Unseen Keys)\", test_unseen_keys, test_transitions, test_tree, test_keys_list),\n",
    "    (\"External (Seen Keys)\", external_common, eicu_trans, eicu_tree, eicu_keys_list),\n",
    "    (\"External (Unseen Keys)\", external_not_mimic_iv, eicu_trans, eicu_tree, eicu_keys_list)\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "k_values = [3, 10, 30, 50, 100, 150, 200]\n",
    "visualize_mmd_action_based_approximation_ground_truth_with_neighbors(\n",
    "    k_values=k_values,\n",
    "    data_sets=data_sets,\n",
    "    train_transitions=train_transitions,\n",
    "    tree=train_tree,\n",
    "    bins=bins\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_mmd_action_based_approximation(k_values, data_sets, train_transitions, tree, bins):\n",
    "    \"\"\"\n",
    "    Visualize MMD scores for the 'action based approximation' method with different k values across multiple datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    - k_values (list): List of k values to evaluate (e.g., [3, 10, 30, 50, 100]).\n",
    "    - data_sets (list): List of tuples, each containing (label, keys, true_transitions).\n",
    "    - train_transitions (dict): Dictionary of training transitions for approximation.\n",
    "    - tree (object): KD-tree object for nearest neighbor search.\n",
    "    - bins (int): Number of bins used for discretization.\n",
    "    \n",
    "    Returns:\n",
    "    - None: Displays a single plot of MMD scores vs. k values for all datasets.\n",
    "    \"\"\"\n",
    "    # Function to create an approximation function with a fixed k\n",
    "    def make_action_based_approx(k):\n",
    "        def approx_func(key, transitions, tree, k, bins, n_samples):\n",
    "            return action_based_approximation(key, transitions, tree, k=k, bins=bins, n_samples=n_samples)\n",
    "        return approx_func\n",
    "    \n",
    "    # Create a single figure for all datasets\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Iterate over each dataset\n",
    "    for label, keys, true_transitions in data_sets:\n",
    "        mmd_scores = []\n",
    "        for k in k_values:\n",
    "            # Create approximation function with current k\n",
    "            approx_func = make_action_based_approx(k)\n",
    "            # Compute MMD score\n",
    "            mmd = MMD_evaluation(approx_func, keys, true_transitions, train_transitions, tree, k, bins)\n",
    "            mmd_scores.append(mmd)\n",
    "            print(f\"MMD for {label}, k={k}: {mmd:.4f}\")\n",
    "        \n",
    "        # Plot the MMD scores for this dataset\n",
    "        plt.plot(k_values, mmd_scores, marker='o', linestyle='-', label=label)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('k (Number of Neighbors)')\n",
    "    plt.ylabel('MMD Score')\n",
    "    plt.title('MMD vs k for Action Based Approximation Across Datasets')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(k_values)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Define the datasets to visualize together\n",
    "# data_sets = [\n",
    "#     (\"Internal (Training)\", train_keys, train_transitions),\n",
    "#     (\"Internal (Seen Keys)\", test_seen_keys, test_transitions),\n",
    "#     (\"Internal (Unseen Keys)\", test_unseen_keys, test_transitions),\n",
    "#     (\"External (Seen Keys)\", external_common, eicu_mimic_iii_mimic_iv_train_trans),\n",
    "#     (\"External (Unseen Keys)\", external_not_mimic_iv, eicu_mimic_iii_trans)\n",
    "# ]\n",
    "data_sets = [\n",
    "    (\"Internal (Training)\", train_keys, train_transitions),\n",
    "    (\"Internal (Seen Keys)\", test_seen_keys, test_transitions),\n",
    "    (\"Internal (Unseen Keys)\", test_unseen_keys, test_transitions),\n",
    "    (\"External (Seen Keys)\", external_common, eicu_trans), # [cont_update] not sure if use eicu_trans will cause skip due to key not found in true keys list\n",
    "    (\"External (Unseen Keys)\", external_not_mimic_iv, eicu_trans)\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "k_values = [3, 10, 30, 50, 100, 150, 200]\n",
    "visualize_mmd_action_based_approximation(\n",
    "    k_values=k_values,\n",
    "    data_sets=data_sets,\n",
    "    train_transitions=train_transitions,\n",
    "    tree=train_tree,\n",
    "    bins=bins\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guideline to Bin for Python Gym environment Reward\n",
    "- rewards based on ventilation weaning (extubation) guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/mimic_iv/min_max_values_guidelines.json', 'r') as f:\n",
    "    min_max_values_guidelines = json.load(f)\n",
    "# Convert to DataFrame\n",
    "min_max_values_guidelines_raw_df = pd.DataFrame(min_max_values_guidelines)\n",
    "# Display the DataFrame\n",
    "min_max_values_guidelines_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_guideline_to_bins(min_max_df, bin_edges_dict, state_cols, action_cols):\n",
    "    \"\"\"\n",
    "    Convert guideline min and max values into bin values based on bin edges for state and action variables.\n",
    "\n",
    "    Args:\n",
    "        min_max_df (pd.DataFrame): Input DataFrame with raw guideline min and max values.\n",
    "        bin_edges_dict (dict): Dictionary containing bin edges for state and action variables.\n",
    "        state_cols (list): List of state variable columns.\n",
    "        action_cols (list): List of action variable columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with guideline min and max values converted to bin values.\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame\n",
    "    bin_df = min_max_df.copy()\n",
    "\n",
    "    # Process state variables\n",
    "    for col in state_cols:\n",
    "        if col in bin_edges_dict:\n",
    "            bin_edges = bin_edges_dict[col]\n",
    "            # Convert min and max values to bins\n",
    "            bin_df[col] = bin_df[col].apply(\n",
    "                lambda x: pd.cut(\n",
    "                    [x], bins=bin_edges, labels=False, include_lowest=True\n",
    "                )[0]\n",
    "            )\n",
    "\n",
    "    # Process action variables\n",
    "    for col in action_cols:\n",
    "        if col in bin_edges_dict:\n",
    "            bin_edges = bin_edges_dict[col]\n",
    "            # Convert min and max values to bins\n",
    "            bin_df[col] = bin_df[col].apply(\n",
    "                lambda x: pd.cut(\n",
    "                    [x], bins=bin_edges, labels=False, include_lowest=True\n",
    "                )[0]\n",
    "            )\n",
    "\n",
    "    return bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert guideline values into bins\n",
    "min_max_values_guidelines_bin_s1_df = convert_guideline_to_bins(min_max_values_guidelines_raw_df, bin_edges_dict_s1, state_cols, action_cols)\n",
    "min_max_values_guidelines_bin_s2_df = convert_guideline_to_bins(min_max_values_guidelines_raw_df, bin_edges_dict_s2, state_cols, action_cols)\n",
    "\n",
    "print(\"Guideline Bin Values (Strategy 1):\")\n",
    "min_max_values_guidelines_bin_s1_df[state_cols+action_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Guideline Bin Values (Strategy 2):\")\n",
    "min_max_values_guidelines_bin_s2_df[state_cols+action_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_min_max_df_to_dict(min_max_df):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame with 'min' and 'max' rows to a JSON-serializable dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    min_max_df : pandas.DataFrame\n",
    "        DataFrame with 'min' and 'max' rows and columns for each variable\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        JSON string representation of the min/max values\n",
    "    \"\"\"\n",
    "    # Convert the dataframe to a dictionary format\n",
    "    result = {}\n",
    "    for column in min_max_df.columns:\n",
    "        result[column] = {\n",
    "            'min': int(min_max_df.loc['min', column]),  # Convert to Python int\n",
    "            'max': int(min_max_df.loc['max', column])   # Convert to Python int\n",
    "        }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min_max_values_guidelines_bin_s1_df[state_cols+action_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min_max_values_guidelines_bin_s2_df[state_cols+action_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "min_max_values_guidelines_bin_s1_dict = convert_min_max_df_to_dict(min_max_values_guidelines_bin_s1_df)\n",
    "min_max_values_guidelines_bin_s2_dict = convert_min_max_df_to_dict(min_max_values_guidelines_bin_s2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bins_strategy == \"s1\":\n",
    "    bin_edges_dict = bin_edges_dict_s1\n",
    "    min_max_values_guidelines_bin_dict = min_max_values_guidelines_bin_s1_dict # TODO: need to set it as selected one\n",
    "elif bins_strategy == \"s2\":\n",
    "    bin_edges_dict = bin_edges_dict_s2\n",
    "    min_max_values_guidelines_bin_dict = min_max_values_guidelines_bin_s2_dict # TODO: need to set it as selected one\n",
    "min_max_values_guidelines_bin_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark of different severity init obs for different offline policy doctor agents\n",
    "(Finding Special Initial Observations for PatientEnvironment Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PatientEnvironment Evaluation Reward\n",
    "EVAL_ACTION_PENALTY_FLAG = False         # TODO: For Reward Design Experiment\n",
    "EVAL_ACTION_STABLE_PENALTY_FLAG = True   # Penalize if the action is the same as the previous one\n",
    "EVAL_WEANING_REWARD_FLAG = True          # TODO: For Reward Design Experiment\n",
    "EVAL_WEANING_REWARD = 10                 # Reward for weaning the patient off ventilation\n",
    "EVAL_MAX_TRAJ_LENGTH = 120               # [TODO] Maximum trajectory length for the environment, depends on the dataset distribution\n",
    "EVAL_ANALYSIS_REWARD_CLIP_MAX_TRAJ_LENGTH_FLAG = True  # Clip the reward to the maximum trajectory length for analysis\n",
    "\n",
    "ACTION_PENALTY_FLAG = False         # TODO: For Reward Design Experiment # [update] all exp don't use action penalty\n",
    "ACTION_STABLE_PENALTY_FLAG = True   # Penalize if the action is the same as the previous one\n",
    "STATE_REWARD_FLAG = True         # TODO: For Reward Design Experiment\n",
    "STATE_STABLE_PENALTY_FLAG = True   # Penalize if the action is the same as the previous one\n",
    "WEANING_REWARD_FLAG = True          # TODO: For Reward Design Experiment\n",
    "TIMESTAMP_PENALTY_FLAG = True  # Penalize if the timestamp is not the same as the previous one\n",
    "WEANING_REWARD = 10                 # Reward for weaning the patient off ventilation\n",
    "MAX_TRAJ_LENGTH = 120               # [TODO] Maximum trajectory length for the environment, depends on the dataset distribution\n",
    "ANALYSIS_REWARD_CLIP_MAX_TRAJ_LENGTH_FLAG = True  # Clip the reward to the maximum trajectory length for analysis\n",
    "ALL_USE_UNSEEN_FLAG = True\n",
    "EXP_FOLDER_PREFIX = \"preprocess_aggregate_update_all_use_unseen_without_action_penalty_with_action_stable_penalty_weaning_reward_10_lr_1e-5\" # \"without_internal_reward_weaning_reward_10_lr_1e-5\" # \"without_action_penalty_with_action_stable_penalty_weaning_reward_10_lr_1e-5\" # \"with_internal_reward_without_weaning_reward_lr_1e-5\" # \"without_action_penalty_without_action_stable_penalty_weaning_reward_10_lr_1e-5\"\n",
    "# \"preprocess_aggregate_update_all_use_unseen_without_action_penalty_with_action_stable_penalty_weaning_reward_10_lr_1e-5\"\n",
    "# \"preprocess_aggregate_update_all_use_unseen_without_internal_reward_weaning_reward_10_lr_1e-5\"\n",
    "# \"preprocess_aggregate_update_all_use_unseen_with_internal_reward_without_weaning_reward_lr_1e-5\"\n",
    "# mkdir if not exists\n",
    "import os\n",
    "if not os.path.exists(f\"../models/training_log/{EXP_FOLDER_PREFIX}\"):\n",
    "    os.makedirs(f\"../models/training_log/{EXP_FOLDER_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Initial Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_initial_obs(df, stay_id, state_cols, action_cols, baseline_cols, obs_hrs=3):\n",
    "    group = df[df['stay_id'] == stay_id].sort_values('hours_in')\n",
    "    if len(group) < obs_hrs:\n",
    "        return None\n",
    "    rows = group.iloc[:obs_hrs].to_dict('records')\n",
    "    # History: obs_hrs - 1 state-action pairs\n",
    "    history = [(tuple(rows[i][col] for col in state_cols), \n",
    "                tuple(rows[i][col] for col in action_cols)) \n",
    "               for i in range(obs_hrs - 1)]\n",
    "    # Current state at t = obs_hrs - 1\n",
    "    current_state = tuple(rows[obs_hrs - 1][col] for col in state_cols)\n",
    "    # Baseline (constant across rows)\n",
    "    baseline = tuple(rows[0][col] for col in baseline_cols)\n",
    "    # Flatten into a single array\n",
    "    flat_history = [item for sublist in history for item in (sublist[0] + sublist[1])]\n",
    "    obs = np.array(flat_history + list(current_state) + list(baseline), dtype=np.int32)\n",
    "    return obs\n",
    "\n",
    "# Example usage\n",
    "stay_id = 30003598\n",
    "init_obs = get_initial_obs(train_df, stay_id, state_cols, action_cols, baseline_cols)\n",
    "print(init_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizing Special Initial Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trajectory_metrics(df, state_cols, action_cols, baseline_cols, \n",
    "                               min_max_values_guidelines_bin_dict,\n",
    "                               obs_hrs=3, stable_hr=2, stable_threshold=2, \n",
    "                               timestamp_reward_step=1):\n",
    "    \"\"\"\n",
    "    Compute trajectory metrics per stay:\n",
    "      - Uses the same penalty logic as the environment.\n",
    "      - Clips trajectory when weaning (6 consecutive zero-penalty steps) occurs.\n",
    "      - Calculates total reward, steps to weaning for each stay.\n",
    "      - Prints summary: reward distribution, average steps, and proportion not weaned.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    metrics_list = []\n",
    "    total_rewards = []\n",
    "    steps_list = []\n",
    "    steps_to_weaning_list = []\n",
    "    num_stays = 0\n",
    "    num_not_weaned = 0\n",
    "    \n",
    "    for stay_id, group in df.groupby('stay_id'):\n",
    "        group = group.sort_values('hours_in')\n",
    "        if len(group) < obs_hrs + 1:\n",
    "            continue\n",
    "        \n",
    "        rows = group.to_dict('records')\n",
    "        # Seed history\n",
    "        history = []\n",
    "        for i in range(obs_hrs - 1):\n",
    "            history.append((tuple(rows[i][col] for col in state_cols),\n",
    "                            tuple(rows[i][col] for col in action_cols)))\n",
    "        \n",
    "        current_state = tuple(rows[obs_hrs-1][col] for col in state_cols)\n",
    "        previous_action = tuple(rows[obs_hrs-2][col] for col in action_cols) # TODO: [update reward]\n",
    "        current_action = tuple(rows[obs_hrs-2][col] for col in action_cols) # TODO: [update reward]\n",
    "        total_reward = 0\n",
    "        weaning_count = 0\n",
    "        weaning_met = False\n",
    "        steps_to_weaning = None\n",
    "        action_diversity_set = set()\n",
    "        \n",
    "        # TODO: [update reward Part 2]\n",
    "        # Iterate through trajectory\n",
    "        for t in range(obs_hrs - 1, len(rows) - 1):\n",
    "            if weaning_met:\n",
    "                break\n",
    "            step = t - (obs_hrs - 1)  # zero-index step\n",
    "            if step >= EVAL_MAX_TRAJ_LENGTH and EVAL_ANALYSIS_REWARD_CLIP_MAX_TRAJ_LENGTH_FLAG: # clip trajectory length\n",
    "                break\n",
    "            previous_action = current_action\n",
    "            current_action = tuple(rows[t][col] for col in action_cols)\n",
    "            next_state = tuple(rows[t+1][col] for col in state_cols)\n",
    "\n",
    "            action_diversity_set.add(current_action)\n",
    "            \n",
    "            # State penalty\n",
    "            state_penalty = -1 if any(\n",
    "                not (min_max_values_guidelines_bin_dict[col]['min'] <= next_state[i] <= \n",
    "                     min_max_values_guidelines_bin_dict[col]['max'])\n",
    "                for i, col in enumerate(state_cols)\n",
    "            ) else 0\n",
    "\n",
    "            state_reward = sum(\n",
    "                1 if min_max_values_guidelines_bin_dict[col]['min'] <= next_state[i] <= min_max_values_guidelines_bin_dict[col]['max'] else 0\n",
    "                for i, col in enumerate(state_cols)\n",
    "            ) / len(state_cols)\n",
    "            \n",
    "            # Action penalty\n",
    "            # action_penalty = -1 if any(\n",
    "            #     not (min_max_values_guidelines_bin_dict[col]['min'] <= a <= \n",
    "            #          min_max_values_guidelines_bin_dict[col]['max'])\n",
    "            #     for a, col in zip(current_action, action_cols)\n",
    "            # ) else 0\n",
    "            action_penalty = sum(\n",
    "                -1 if current_action[i] < min_max_values_guidelines_bin_dict[col]['min'] or current_action[i] > min_max_values_guidelines_bin_dict[col]['max'] else 0\n",
    "                for i, col in enumerate(action_cols)\n",
    "            ) / len(action_cols)\n",
    "            \n",
    "            # Stable penalty\n",
    "            # stable_penalty = 0\n",
    "            # if len(history) >= stable_hr:\n",
    "            #     stable = True\n",
    "            #     for i in range(-stable_hr + 1, 0):\n",
    "            #         for j in range(len(state_cols)):\n",
    "            #             diff = abs(history[i][0][j] - history[i-1][0][j])\n",
    "            #             if diff >= stable_threshold:\n",
    "            #                 stable = False\n",
    "            #                 break\n",
    "            #         if not stable:\n",
    "            #             break\n",
    "            #     if stable:\n",
    "            #         last_state = history[-1][0]\n",
    "            #         for j in range(len(state_cols)):\n",
    "            #             diff = abs(current_state[j] - last_state[j])\n",
    "            #             if diff >= stable_threshold:\n",
    "            #                 stable = False\n",
    "            #                 break\n",
    "            #     stable_penalty = -1 if not stable else 0\n",
    "\n",
    "            state_stable_penalty = sum(\n",
    "                -1 if abs(next_state[i] - current_state[i]) >= stable_threshold else 0\n",
    "                for i, col in enumerate(state_cols)\n",
    "            ) / len(state_cols)\n",
    "            \n",
    "            action_stable_penalty = sum(\n",
    "                -1 if abs(previous_action[i] - current_action[i]) >= stable_threshold * 2 else 0\n",
    "                for i, col in enumerate(action_cols)\n",
    "            ) / len(action_cols)\n",
    "\n",
    "            # Timestamp penalty\n",
    "            timestamp_penalty = -1 if (step + 1) % timestamp_reward_step == 0 else 0\n",
    "            \n",
    "            # Check weaning condition\n",
    "            # if state_penalty == 0 and action_penalty == 0 and state_stable_penalty == 0:\n",
    "            if state_penalty == 0 and action_penalty == 0 and state_stable_penalty == 0 and action_stable_penalty == 0: # TODO: try add action_stable_penalty\n",
    "                weaning_count += 1\n",
    "            else:\n",
    "                weaning_count = 0\n",
    "\n",
    "            # Accumulate reward\n",
    "            if not EVAL_ACTION_PENALTY_FLAG:\n",
    "                action_penalty = 0\n",
    "            if not EVAL_ACTION_STABLE_PENALTY_FLAG:\n",
    "                action_stable_penalty = 0\n",
    "            reward = state_reward + action_penalty + state_stable_penalty + action_stable_penalty + timestamp_penalty\n",
    "            total_reward += reward\n",
    "            \n",
    "            # if weaning_count >= 6: # TODO: 6 is set by my self\n",
    "            # if weaning_count >= 6 and step >= 23: # TODO: 6 is set by my self # TODO: [update reward]\n",
    "            if weaning_count >= 6: # TODO: 6 is set by my self\n",
    "                # total_reward += 10 # TODO: [update reward]\n",
    "                if EVAL_WEANING_REWARD_FLAG:\n",
    "                    total_reward += WEANING_REWARD\n",
    "                weaning_met = True\n",
    "                steps_to_weaning = step + 1\n",
    "                break\n",
    "            # [update reward] TODO: elif last step and weaning_cout >= 2\n",
    "            # elif step == len(rows) - obs_hrs - 1 and weaning_count >= 2:\n",
    "            #     total_reward += 100\n",
    "            #     weaning_met = True\n",
    "            #     steps_to_weaning = step + 1\n",
    "\n",
    "            # Update history and state for next iteration\n",
    "            history = history[1:] + [(current_state, current_action)]\n",
    "            current_state = next_state\n",
    "        \n",
    "        # If not weaned, mark steps_to_weaning as total steps\n",
    "        if not weaning_met:\n",
    "            if EVAL_ANALYSIS_REWARD_CLIP_MAX_TRAJ_LENGTH_FLAG:\n",
    "                steps_to_weaning = min(len(rows) - obs_hrs, EVAL_MAX_TRAJ_LENGTH)\n",
    "            else:\n",
    "                steps_to_weaning = len(rows) - obs_hrs\n",
    "            num_not_weaned += 1\n",
    "            total_reward -= EVAL_WEANING_REWARD\n",
    "        \n",
    "        metrics_list.append({\n",
    "            'stay_id': stay_id,\n",
    "            'total_reward': total_reward,\n",
    "            'meet_weaning': weaning_met,\n",
    "            'steps': steps_to_weaning,\n",
    "            'action_diversity': len(action_diversity_set),\n",
    "            'action': action_diversity_set\n",
    "        })\n",
    "        total_rewards.append(total_reward)\n",
    "        steps_list.append(steps_to_weaning)\n",
    "        if weaning_met:\n",
    "            steps_to_weaning_list.append(steps_to_weaning)\n",
    "        num_stays += 1\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    \n",
    "    # Summary statistics\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    std_reward = np.std(total_rewards)\n",
    "    avg_steps = np.mean(steps_list)\n",
    "    std_steps = np.std(steps_list)\n",
    "    avg_steps_to_weaning = np.mean(steps_to_weaning_list) if steps_to_weaning_list else 0\n",
    "    std_steps_to_weaning = np.std(steps_to_weaning_list) if steps_to_weaning_list else 0\n",
    "    prop_not_weaned = num_not_weaned / num_stays if num_stays > 0 else None\n",
    "    \n",
    "    print(f\"Total stays evaluated: {num_stays}\")\n",
    "    print(f\"Reward (mean  std): {avg_reward:.2f}  {std_reward:.2f}\")\n",
    "    print(f\"Average steps: {avg_steps:.2f}  {std_steps:.2f}\")\n",
    "    print(f\"Average steps to weaning: {avg_steps_to_weaning:.2f}  {std_steps_to_weaning:.2f}\")\n",
    "    print(f\"Proportion not weaned: {prop_not_weaned:.2%}\")\n",
    "    \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = compute_trajectory_metrics(train_df, state_cols, action_cols, baseline_cols, min_max_values_guidelines_bin_dict)\n",
    "metrics_train_df = compute_trajectory_metrics(train_df, state_cols, action_cols, baseline_cols, min_max_values_guidelines_bin_dict)\n",
    "metrics_test_df = compute_trajectory_metrics(test_df, state_cols, action_cols, baseline_cols, min_max_values_guidelines_bin_dict)\n",
    "metrics_eICU_df = compute_trajectory_metrics(eICU_disc, state_cols, action_cols, baseline_cols, min_max_values_guidelines_bin_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ANALYSIS_REWARD_CLIP_MAX_TRAJ_LENGTH_FLAG, analysis small, median, large reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward smallest 100\n",
    "# metrics_df[metrics_df[\"meet_weaning\"] == True].sort_values(by=\"total_reward\", ascending=True)[:100]\n",
    "metrics_df.sort_values(by=\"total_reward\", ascending=True)[:100]\n",
    "print(\"Smallest 100 rewards:\")\n",
    "print(f\"Mean of Total Reward: {metrics_df.sort_values(by='total_reward', ascending=True)[:100]['total_reward'].mean():.2f}\")\n",
    "print(f\"Std of Total Reward: {metrics_df.sort_values(by='total_reward', ascending=True)[:100]['total_reward'].std():.2f}\")\n",
    "print(f\"Meet Extubation: {metrics_df.sort_values(by='total_reward', ascending=True)[:100]['meet_weaning'].mean():.2%}\")\n",
    "print(f\"Mean of Steps: {metrics_df.sort_values(by='total_reward', ascending=True)[:100]['steps'].mean():.2f}\")\n",
    "print(f\"Std of Steps: {metrics_df.sort_values(by='total_reward', ascending=True)[:100]['steps'].std():.2f}\")\n",
    "print(f\"Mean of Steps to Weaning: {metrics_df.sort_values(by='total_reward', ascending=True)[:100][metrics_df.sort_values(by='total_reward', ascending=True)[:100]['meet_weaning'] == True]['steps'].mean():.2f}\")\n",
    "print(f\"Std of Steps to Weaning: {metrics_df.sort_values(by='total_reward', ascending=True)[:100][metrics_df.sort_values(by='total_reward', ascending=True)[:100]['meet_weaning'] == True]['steps'].std():.2f}\")\n",
    "print(f\"Action Diversity: {len(set.union(*metrics_df.sort_values(by='total_reward', ascending=True)[:100]['action']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_data_bemchmark_metrics(metrics_df, category, n_samples=100):\n",
    "    if category == \"small\":\n",
    "        ascending = True\n",
    "        subset = metrics_df.sort_values(by='total_reward', ascending=ascending)[:n_samples]\n",
    "    elif category == \"median\":\n",
    "        ascending = True\n",
    "        sorted_df = metrics_df.sort_values(by='total_reward', ascending=ascending)\n",
    "        start_idx = len(metrics_df)//2-(n_samples//2)\n",
    "        end_idx = len(metrics_df)//2+(n_samples//2)\n",
    "        subset = sorted_df[start_idx:end_idx]\n",
    "    elif category == \"large\":\n",
    "        ascending = False\n",
    "        subset = metrics_df.sort_values(by='total_reward', ascending=ascending)[:n_samples]\n",
    "    elif category == \"all\":\n",
    "        subset = metrics_df\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_reward_mean = subset['total_reward'].mean()\n",
    "    total_reward_std = subset['total_reward'].std()\n",
    "    meet_extubation = subset['meet_weaning'].mean()\n",
    "    avg_hours_mean = subset['steps'].mean()\n",
    "    avg_hours_std = subset['steps'].std()\n",
    "    \n",
    "    # Calculate steps to weaning for cases that meet weaning criteria\n",
    "    weaning_subset = subset[subset['meet_weaning'] == True]\n",
    "    if len(weaning_subset) > 0:\n",
    "        avg_hours_to_meet_mean = weaning_subset['steps'].mean()\n",
    "        avg_hours_to_meet_std = weaning_subset['steps'].std()\n",
    "    else:\n",
    "        avg_hours_to_meet_mean = np.nan\n",
    "        avg_hours_to_meet_std = np.nan\n",
    "    \n",
    "    action_diversity = len(set.union(*subset['action']))\n",
    "    \n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        'Category': category,\n",
    "        'N_Samples': len(subset) if category == \"all\" else n_samples,\n",
    "        'Total Reward (MeanStd)': f\"{total_reward_mean:.2f}{total_reward_std:.2f}\",\n",
    "        'Meet Extubation': f\"{meet_extubation:.2%}\",\n",
    "        'Avg Hours (MeanStd)': f\"{avg_hours_mean:.2f}{avg_hours_std:.2f}\",\n",
    "        'Avg Hours to Meet (MeanStd)': f\"{avg_hours_to_meet_mean:.2f}{avg_hours_to_meet_std:.2f}\" if not np.isnan(avg_hours_to_meet_mean) else \"N/A\",\n",
    "        'Action Diversity': action_diversity\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame([result])\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# If you want to combine multiple categories into one DataFrame:\n",
    "def get_all_metrics(metrics_df, n_samples=100):\n",
    "    categories = [\"small\", \"median\", \"large\", \"all\"]\n",
    "    all_results = []\n",
    "    \n",
    "    for category in categories:\n",
    "        if category == \"all\":\n",
    "            result_df = real_data_bemchmark_metrics(metrics_df, category)\n",
    "        else:\n",
    "            result_df = real_data_bemchmark_metrics(metrics_df, category, n_samples)\n",
    "        all_results.append(result_df)\n",
    "    \n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a single category\n",
    "# real_data_bemchmark_metrics_result_df = real_data_bemchmark_metrics(metrics_df, \"small\", 100)\n",
    "# real_data_bemchmark_metrics_result_df\n",
    "\n",
    "# For all categories combined\n",
    "n_samples = 100\n",
    "all_metrics_df = get_all_metrics(metrics_df, n_samples)\n",
    "all_metrics_train_df = get_all_metrics(metrics_train_df, n_samples)\n",
    "all_metrics_test_df = get_all_metrics(metrics_test_df, n_samples)\n",
    "all_metrics_eICU_df = get_all_metrics(metrics_eICU_df, n_samples)\n",
    "all_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_eICU_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_samples as 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "all_metrics_df = get_all_metrics(metrics_df, n_samples)\n",
    "all_metrics_test_df = get_all_metrics(metrics_test_df, n_samples)\n",
    "all_metrics_eICU_df = get_all_metrics(metrics_eICU_df, n_samples)\n",
    "all_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_eICU_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_combined_metrics_csv(metrics_train_df, metrics_test_df, metrics_eICU_df, filename='combined_metrics.csv'):\n",
    "    \"\"\"\n",
    "    Create a single CSV file combining all metrics for different datasets and sample sizes\n",
    "    \"\"\"\n",
    "    \n",
    "    all_combined_results = []\n",
    "    \n",
    "    # Define sample sizes\n",
    "    sample_sizes = [100, 1000]\n",
    "    \n",
    "    # Define datasets with their corresponding dataframes\n",
    "    datasets = {\n",
    "        'Train': metrics_train_df,\n",
    "        'Test': metrics_test_df,\n",
    "        'eICU': metrics_eICU_df\n",
    "    }\n",
    "    \n",
    "    # Process each dataset and sample size combination\n",
    "    for dataset_name, df in datasets.items():\n",
    "        for n_samples in sample_sizes:\n",
    "            # Get metrics for current dataset and sample size\n",
    "            metrics_df = get_all_metrics(df, n_samples)\n",
    "            \n",
    "            # Add dataset name and sample size columns\n",
    "            metrics_df['Dataset'] = dataset_name\n",
    "            metrics_df['Sample_Size'] = n_samples\n",
    "            \n",
    "            # Reorder columns to put Dataset and Sample_Size first\n",
    "            cols = ['Dataset', 'Sample_Size'] + [col for col in metrics_df.columns if col not in ['Dataset', 'Sample_Size']]\n",
    "            metrics_df = metrics_df[cols]\n",
    "            \n",
    "            all_combined_results.append(metrics_df)\n",
    "    \n",
    "    # Combine all results\n",
    "    final_combined_df = pd.concat(all_combined_results, ignore_index=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    final_combined_df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"Combined metrics saved to {filename}\")\n",
    "    print(f\"Total rows: {len(final_combined_df)}\")\n",
    "    print(\"\\nPreview of the combined data:\")\n",
    "    print(final_combined_df.head(10))\n",
    "    \n",
    "    return final_combined_df\n",
    "\n",
    "# Usage:\n",
    "combined_metrics = create_combined_metrics_csv(\n",
    "    metrics_train_df, \n",
    "    metrics_test_df, \n",
    "    metrics_eICU_df, \n",
    "    filename='../models/real_data_benchmark_metrics.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_df[metrics_df[\"meet_weaning\"] == True]\n",
    "# metrics_df[metrics_df[\"meet_weaning\"] == True].sort_values(by=\"total_reward\", ascending=True).head(50)\n",
    "# metrics_df[(metrics_df[\"steps_to_weaning\"] <= 48)].sort_values(by=\"total_reward\", ascending=True).head(50)\n",
    "# metrics_df.sort_values(by=\"total_reward\", ascending=True)[int(len(metrics_df)/2)-10:int(len(metrics_df)/2)+11]\n",
    "# metrics_df[metrics_df[\"meet_weaning\"] == True].sort_values(by=\"steps_to_weaning\", ascending=True).describe()\n",
    "metrics_df[metrics_df[\"meet_weaning\"] == True].sort_values(by=\"total_reward\", ascending=True).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward smallest 100\n",
    "metrics_df[metrics_df[\"meet_weaning\"] == True].sort_values(by=\"total_reward\", ascending=True)[:100].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward largest 100\n",
    "metrics_eICU_df[metrics_eICU_df[\"meet_weaning\"] == True].sort_values(by=\"total_reward\", ascending=False)[:100].describe()\n",
    "# metrics_df[metrics_df[\"meet_weaning\"] == True].sort_values(by=\"total_reward\", ascending=False)[40:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward median 100\n",
    "metrics_test_df[metrics_test_df[\"meet_weaning\"] == True].sort_values(by=\"total_reward\", ascending=True)[int(len(metrics_test_df)/2)-50:int(len(metrics_test_df)/2)+50].describe()\n",
    "# metrics_df[metrics_df[\"meet_weaning\"] == True].sort_values(by=\"total_reward\", ascending=False)[40:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train_df[metrics_train_df[\"steps_to_weaning\"] > 96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train_df[metrics_train_df[\"steps_to_weaning\"]==1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics_train_df[metrics_train_df[\"steps_to_weaning\"]==1])/len(metrics_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics_train_df[metrics_train_df[\"steps_to_weaning\"]==6])/len(metrics_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train_df[metrics_train_df[\"steps_to_weaning\"]==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_df, title):\n",
    "    # relationship between total reward and steps to weaning\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Separate data based on meet_weaning status\n",
    "    meet_weaning_true = metrics_df[metrics_df['meet_weaning'] == True]\n",
    "    meet_weaning_false = metrics_df[metrics_df['meet_weaning'] == False]\n",
    "    # Plot with different colors\n",
    "    plt.scatter(meet_weaning_true['total_reward'], meet_weaning_true['steps'], \n",
    "                alpha=0.5, color='#1f77b4', label='Meet Extubation: True')\n",
    "                # alpha=0.5, color='#1f77b4', label='Meet Weaning: True')\n",
    "    plt.scatter(meet_weaning_false['total_reward'], meet_weaning_false['steps'], \n",
    "                alpha=0.5, color='#d62728', label='Meet Extubation: False')\n",
    "                # alpha=0.5, color='#d62728', label='Meet Weaning: False')\n",
    "    \n",
    "    plt.title(f'{title} Total Reward vs Trajectory Length')\n",
    "    plt.xlabel('Total Reward')\n",
    "    # plt.ylabel('Steps to Extubation')\n",
    "    plt.ylabel('Trajectory Length')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(metrics_train_df, 'Train')\n",
    "plot_metrics(metrics_test_df, 'Test')\n",
    "plot_metrics(metrics_eICU_df, 'eICU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clip by MAX_TRAJ_LENGTH = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics_train_df, 'Train')\n",
    "plot_metrics(metrics_test_df, 'Test')\n",
    "plot_metrics(metrics_eICU_df, 'eICU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics_train_df, 'Train')\n",
    "plot_metrics(metrics_test_df, 'Test')\n",
    "plot_metrics(metrics_eICU_df, 'eICU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics_train_df, 'Train')\n",
    "plot_metrics(metrics_test_df, 'Test')\n",
    "plot_metrics(metrics_eICU_df, 'eICU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Evaluation Initial Observation Categories (7 categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_init_obs_categories(train_df, metrics_df, state_cols, action_cols, baseline_cols, n_samples=100):\n",
    "    # Total Reward Categories\n",
    "    n = len(metrics_df)\n",
    "    \n",
    "    # Use min(n_samples, available samples) to handle cases where dataset is smaller than n_samples\n",
    "    small_sample_size = min(n_samples, int(0.1 * n))\n",
    "    large_sample_size = min(n_samples, int(0.1 * n))\n",
    "    \n",
    "    small_reward_patients = metrics_df.nsmallest(small_sample_size, 'total_reward')['stay_id']\n",
    "    \n",
    "    # For median, take samples from the middle 10% range\n",
    "    # median_idx = int(0.45 * n), int(0.55 * n)\n",
    "    median_idx = int(n//2 - n_samples//2), int(n//2 + n_samples//2)\n",
    "    median_candidates = metrics_df.sort_values('total_reward').iloc[median_idx[0]:median_idx[1]]\n",
    "    median_sample_size = min(n_samples, len(median_candidates))\n",
    "    median_reward_patients = median_candidates.sample(n=median_sample_size, random_state=42)['stay_id']\n",
    "    \n",
    "    large_reward_patients = metrics_df.nlargest(large_sample_size, 'total_reward')['stay_id']\n",
    "\n",
    "    small_reward_init_obs = [get_initial_obs(train_df, sid, state_cols, action_cols, baseline_cols) for sid in small_reward_patients]\n",
    "    median_reward_init_obs = [get_initial_obs(train_df, sid, state_cols, action_cols, baseline_cols) for sid in median_reward_patients]\n",
    "    large_reward_init_obs = [get_initial_obs(train_df, sid, state_cols, action_cols, baseline_cols) for sid in large_reward_patients]\n",
    "\n",
    "    # Steps to Weaning Categories\n",
    "    short_sample_size = min(n_samples, int(0.1 * n))\n",
    "    long_sample_size = min(n_samples, int(0.1 * n))\n",
    "    \n",
    "    short_to_wean_patients = metrics_df.nsmallest(short_sample_size, 'steps')['stay_id']\n",
    "    \n",
    "    # median_wean_idx = int(0.45 * n), int(0.55 * n)\n",
    "    median_wean_idx = int(n//2 - n_samples//2), int(n//2 + n_samples//2)\n",
    "    median_wean_candidates = metrics_df.sort_values('steps').iloc[median_wean_idx[0]:median_wean_idx[1]]\n",
    "    median_wean_sample_size = min(n_samples, len(median_wean_candidates))\n",
    "    median_to_wean_patients = median_wean_candidates.sample(n=median_wean_sample_size, random_state=42)['stay_id']\n",
    "    \n",
    "    long_to_wean_patients = metrics_df.nlargest(long_sample_size, 'steps')['stay_id']\n",
    "\n",
    "    short_to_wean_init_obs = [get_initial_obs(train_df, sid, state_cols, action_cols, baseline_cols) for sid in short_to_wean_patients]\n",
    "    median_to_wean_init_obs = [get_initial_obs(train_df, sid, state_cols, action_cols, baseline_cols) for sid in median_to_wean_patients]\n",
    "    long_to_wean_init_obs = [get_initial_obs(train_df, sid, state_cols, action_cols, baseline_cols) for sid in long_to_wean_patients]\n",
    "\n",
    "    # Common Initial Observations\n",
    "    from collections import Counter\n",
    "    initial_obs_list = [tuple(get_initial_obs(train_df, sid, state_cols, action_cols, baseline_cols)) \n",
    "                        for sid in train_df['stay_id'].unique() if get_initial_obs(train_df, sid, state_cols, action_cols, baseline_cols) is not None]\n",
    "    obs_counter = Counter(initial_obs_list)\n",
    "    common_init_obs = [obs for obs, count in obs_counter.items() if count > 1]  # Appears in multiple patients\n",
    "\n",
    "    # Example usage\n",
    "    # TODO: update as only small, median, and large reward\n",
    "    categories = {\n",
    "        'small_reward': small_reward_init_obs,\n",
    "        'median_reward': median_reward_init_obs,\n",
    "        'large_reward': large_reward_init_obs,\n",
    "        # 'long_to_wean': long_to_wean_init_obs,\n",
    "        # 'median_to_wean': median_to_wean_init_obs,\n",
    "        # 'short_to_wean': short_to_wean_init_obs,\n",
    "        # 'common': common_init_obs\n",
    "    }\n",
    "\n",
    "    # Filter out None values from init_obs lists\n",
    "    categories = {k: [obs for obs in v if obs is not None] for k, v in categories.items()}\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_init_obs_categories = get_eval_init_obs_categories(train_df, metrics_train_df, state_cols, action_cols, baseline_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eval_init_obs_categories[\"median_reward\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real data Reward on different Evaluation Initial Observation Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std_of_reward_and_steps(metrics_df):\n",
    "    # 1. Mean and standard deviation of `total_reward`\n",
    "    mean_reward = metrics_df['total_reward'].mean()\n",
    "    std_reward = metrics_df['total_reward'].std()\n",
    "    # 2. Mean and standard deviation of `steps_to_weaning`\n",
    "    mean_steps_to_weaning = metrics_df['steps'].mean()\n",
    "    std_steps_to_weaning = metrics_df['steps'].std()\n",
    "    # 3. Action diversity across all 100 rows\n",
    "    # Combine all action sets into one set\n",
    "    all_actions = set().union(*metrics_df['action'])\n",
    "    action_diversity = len(all_actions)\n",
    "\n",
    "    return {\n",
    "        'mean_reward': mean_reward,\n",
    "        'std_reward': std_reward,\n",
    "        'mean_steps_to_weaning': mean_steps_to_weaning,\n",
    "        'std_steps_to_weaning': std_steps_to_weaning,\n",
    "        'action_diversity': action_diversity\n",
    "    }\n",
    "\n",
    "def real_data_reward_across_eval_init_obs_categories(metrics_df, n_samples=100):\n",
    "    n = len(metrics_df)\n",
    "    small_reward_patients_n_samples_rows = metrics_df.nsmallest(min(n_samples, int(0.1 * n)), 'total_reward')[:n_samples]\n",
    "    median_idx = int(n//2 - n_samples//2), int(n//2 + n_samples//2)\n",
    "    median_reward_patients_n_samples_rows = metrics_df.sort_values('total_reward').iloc[median_idx[0]:median_idx[1]][:n_samples]\n",
    "    large_reward_patients_n_samples_rows = metrics_df.nlargest(min(n_samples, int(0.1 * n)), 'total_reward')[:n_samples]\n",
    "    small_reward_stats_results = get_mean_std_of_reward_and_steps(small_reward_patients_n_samples_rows)\n",
    "    median_reward_stats_results = get_mean_std_of_reward_and_steps(median_reward_patients_n_samples_rows)\n",
    "    large_reward_stats_results = get_mean_std_of_reward_and_steps(large_reward_patients_n_samples_rows)\n",
    "\n",
    "    short_to_wean_patients_n_samples_rows = metrics_df.nsmallest(min(n_samples, int(0.1 * n)), 'steps')[:n_samples]\n",
    "    median_wean_idx = int(n//2 - n_samples//2), int(n//2 + n_samples//2)\n",
    "    median_to_wean_patients_n_samples_rows = metrics_df.sort_values('steps').iloc[median_wean_idx[0]:median_wean_idx[1]][:n_samples]\n",
    "    long_to_wean_patients_n_samples_rows = metrics_df.nlargest(min(n_samples, int(0.1 * n)), 'steps')[:n_samples]\n",
    "    short_to_wean_stats_results = get_mean_std_of_reward_and_steps(short_to_wean_patients_n_samples_rows)\n",
    "    median_to_wean_stats_results = get_mean_std_of_reward_and_steps(median_to_wean_patients_n_samples_rows)\n",
    "    long_to_wean_stats_results = get_mean_std_of_reward_and_steps(long_to_wean_patients_n_samples_rows)\n",
    "\n",
    "    # Combine results into a single dictionary\n",
    "    results = {\n",
    "        'small_reward': small_reward_stats_results,\n",
    "        'median_reward': median_reward_stats_results,\n",
    "        'large_reward': large_reward_stats_results,\n",
    "        'long_to_wean': long_to_wean_stats_results,\n",
    "        'median_to_wean': median_to_wean_stats_results,\n",
    "        'short_to_wean': short_to_wean_stats_results\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def plot_reward_steps(metrics_df, title):\n",
    "    # Plotting the distribution of total reward and steps to weaning\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Total Reward Distribution\n",
    "    ax[0].hist(metrics_df['total_reward'], bins=30, color='blue', alpha=0.7)\n",
    "    ax[0].set_title(f'{title} Total Reward Distribution')\n",
    "    ax[0].set_xlabel('Total Reward')\n",
    "    ax[0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Steps to Weaning Distribution\n",
    "    ax[1].hist(metrics_df['steps_to_weaning'], bins=30, color='green', alpha=0.7)\n",
    "    ax[1].set_title(f'{title} Steps to Weaning Distribution')\n",
    "    ax[1].set_xlabel('Steps to Weaning')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_reward_stats = real_data_reward_across_eval_init_obs_categories(metrics_train_df, n_samples=100)\n",
    "test_df_reward_stats = real_data_reward_across_eval_init_obs_categories(metrics_test_df, n_samples=100)\n",
    "eICU_df_reward_stats = real_data_reward_across_eval_init_obs_categories(metrics_eICU_df, n_samples=100)\n",
    "train_df_reward_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_reward_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eICU_df_reward_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert the dictionary data to a more convenient format for visualization\n",
    "def extract_metric_data(stats_dict, dataset_name):\n",
    "    rows = []\n",
    "    for category, metrics in stats_dict.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            # Convert numpy.float64 to regular float for better compatibility \n",
    "            if isinstance(value, np.float64):\n",
    "                value = float(value)\n",
    "            rows.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Category': category,\n",
    "                'Metric': metric_name,\n",
    "                'Value': value\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Create the datasets\n",
    "train_data = extract_metric_data(train_df_reward_stats, 'Train')\n",
    "test_data = extract_metric_data(test_df_reward_stats, 'Test')\n",
    "eicu_data = extract_metric_data(eICU_df_reward_stats, 'eICU')\n",
    "\n",
    "# Combine the datasets\n",
    "combined_data = pd.concat([train_data, test_data, eicu_data], ignore_index=True)\n",
    "\n",
    "# Handling different metrics appropriately\n",
    "metrics_to_plot = ['mean_reward', 'std_reward', 'mean_steps_to_weaning', \n",
    "                  'std_steps_to_weaning', 'action_diversity']\n",
    "\n",
    "# Create subplots for each metric\n",
    "fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(14, 20))\n",
    "fig.suptitle('Comparison of Metrics Across Datasets and Categories', fontsize=16)\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    # Filter for this metric\n",
    "    metric_data = combined_data[combined_data['Metric'] == metric].copy()\n",
    "    \n",
    "    # Pivot data for heatmap\n",
    "    pivot_data = metric_data.pivot_table(\n",
    "        index='Category', \n",
    "        columns='Dataset', \n",
    "        values='Value'\n",
    "    )\n",
    "    \n",
    "    # Sort categories for better visualization\n",
    "    # You can modify this sorting logic as needed\n",
    "    category_order = [\n",
    "        'large_reward', 'median_reward', 'small_reward',\n",
    "        'short_to_wean', 'median_to_wean', 'long_to_wean'\n",
    "    ]\n",
    "    pivot_data = pivot_data.reindex(category_order)\n",
    "    \n",
    "    # Determine colormap based on metric\n",
    "    # Green is good for rewards, red is good for fewer steps\n",
    "    if metric == 'mean_reward':\n",
    "        cmap = 'RdYlGn'  # Red-Yellow-Green: Red for negative, Green for positive rewards\n",
    "    elif 'steps' in metric:\n",
    "        cmap = 'RdYlGn_r'  # Reversed: Green for fewer steps, Red for more steps\n",
    "    elif metric == 'action_diversity':\n",
    "        cmap = 'viridis'  # Neutral colormap for diversity\n",
    "    else:\n",
    "        cmap = 'coolwarm'  # Default colormap\n",
    "    \n",
    "    # Plot heatmap\n",
    "    ax = axes[i]\n",
    "    sns.heatmap(pivot_data, annot=True, fmt=\".2f\", cmap=cmap, ax=ax, linewidths=0.5)\n",
    "    \n",
    "    # Format the heatmap\n",
    "    ax.set_title(f'{metric}', fontsize=14)\n",
    "    ax.set_ylabel('')\n",
    "    if i < len(metrics_to_plot) - 1:\n",
    "        ax.set_xlabel('')\n",
    "    else:\n",
    "        ax.set_xlabel('Dataset')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()\n",
    "\n",
    "# Let's also create a radar chart to visualize the multidimensional aspects\n",
    "# Just for the large_reward, median_reward, and small_reward categories\n",
    "\n",
    "# Function to create radar chart\n",
    "def create_radar_chart(categories, metrics, data, title):\n",
    "    num_metrics = len(metrics)\n",
    "    angles = np.linspace(0, 2*np.pi, num_metrics, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Close the polygon\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Add metrics labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics)\n",
    "    \n",
    "    # Add data for each category and dataset\n",
    "    for dataset in ['Train', 'Test', 'eICU']:\n",
    "        for category in categories:\n",
    "            values = []\n",
    "            for metric in metrics:\n",
    "                value = data[(data['Dataset'] == dataset) & \n",
    "                             (data['Category'] == category) & \n",
    "                             (data['Metric'] == metric)]['Value'].values[0]\n",
    "                values.append(value)\n",
    "            values += values[:1]  # Close the polygon\n",
    "            \n",
    "            ax.plot(angles, values, linewidth=2, label=f\"{dataset} - {category}\")\n",
    "            ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    ax.set_title(title, fontsize=15)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Define categories and metrics for radar chart\n",
    "radar_categories = ['large_reward', 'median_reward', 'small_reward']\n",
    "radar_metrics = ['mean_reward', 'mean_steps_to_weaning', 'action_diversity']\n",
    "\n",
    "# Normalize data for radar chart (different scales)\n",
    "radar_data = combined_data.copy()\n",
    "for metric in radar_metrics:\n",
    "    metric_subset = radar_data[radar_data['Metric'] == metric]\n",
    "    min_val = metric_subset['Value'].min()\n",
    "    max_val = metric_subset['Value'].max()\n",
    "    radar_data.loc[radar_data['Metric'] == metric, 'Value'] = (\n",
    "        (radar_data.loc[radar_data['Metric'] == metric, 'Value'] - min_val) / (max_val - min_val)\n",
    "    )\n",
    "\n",
    "# Create radar chart\n",
    "radar_fig = create_radar_chart(\n",
    "    radar_categories, \n",
    "    radar_metrics,\n",
    "    radar_data,\n",
    "    'Normalized Comparison of Reward Categories Across Datasets'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Let's also create a bar chart for mean_reward across all categories and datasets\n",
    "plt.figure(figsize=(14, 8))\n",
    "reward_data = combined_data[combined_data['Metric'] == 'mean_reward']\n",
    "sns.barplot(x='Category', y='Value', hue='Dataset', data=reward_data)\n",
    "plt.title('Mean Reward Comparison Across Categories and Datasets', fontsize=15)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# And a bar chart for mean_steps_to_weaning across all categories and datasets\n",
    "plt.figure(figsize=(14, 8))\n",
    "steps_data = combined_data[combined_data['Metric'] == 'mean_steps_to_weaning']\n",
    "sns.barplot(x='Category', y='Value', hue='Dataset', data=steps_data)\n",
    "plt.title('Mean Steps to Weaning Comparison Across Categories and Datasets', fontsize=15)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_reward_stats = real_data_reward_across_eval_init_obs_categories(metrics_train_df, top_n=100)\n",
    "test_df_reward_stats = real_data_reward_across_eval_init_obs_categories(metrics_test_df, top_n=100)\n",
    "eICU_df_reward_stats = real_data_reward_across_eval_init_obs_categories(metrics_eICU_df, top_n=100)\n",
    "train_df_reward_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_reward_stats = real_data_reward_across_eval_init_obs_categories(metrics_train_df, top_n=100)\n",
    "test_df_reward_stats = real_data_reward_across_eval_init_obs_categories(metrics_test_df, top_n=100)\n",
    "eICU_df_reward_stats = real_data_reward_across_eval_init_obs_categories(metrics_eICU_df, top_n=100)\n",
    "train_df_reward_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym Patient Environment (with d3rlpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KDTree\n",
    "import itertools\n",
    "import d3rlpy\n",
    "from d3rlpy.algos import DQNConfig\n",
    "from d3rlpy.models.q_functions import MeanQFunctionFactory\n",
    "from d3rlpy.models import DefaultEncoderFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transform_to_reset_format(data, state_cols, action_cols, baseline_cols):\n",
    "    # Calculate lengths of each section\n",
    "    state_len = len(state_cols)\n",
    "    action_len = len(action_cols)\n",
    "    baseline_len = len(baseline_cols)\n",
    "\n",
    "    # Split the data into states, actions, and baselines\n",
    "    num_states = (len(data) - baseline_len) // (state_len + action_len)\n",
    "    states = []\n",
    "    actions = []\n",
    "    for i in range(num_states):\n",
    "        start_idx = i * (state_len + action_len)\n",
    "        states.append(tuple(data[start_idx:start_idx + state_len]))\n",
    "        actions.append(tuple(data[start_idx + state_len:start_idx + state_len + action_len]))\n",
    "    # last state is the current state\n",
    "    current_state = tuple(data[-(state_len + baseline_len):-baseline_len])\n",
    "    states.append(current_state)\n",
    "    # Extract baselines\n",
    "    baselines = tuple(data[-baseline_len:])\n",
    "\n",
    "    # Create the final format\n",
    "    result = []\n",
    "    for i in range(num_states+1):\n",
    "        # Use the last action if the current action is missing\n",
    "        action = actions[i] if i < len(actions) else actions[-1]\n",
    "        result.append((states[i], action))\n",
    "    \n",
    "    # Add the baselines\n",
    "    result.append(baselines)\n",
    "\n",
    "    return tuple(result)\n",
    "\n",
    "test_data = (\n",
    "    np.int32(1), np.int32(2), np.int32(3),  # state 1\n",
    "    np.int32(4), np.int32(5),               # action 1\n",
    "    np.int32(6), np.int32(7), np.int32(8),  # state 2\n",
    "    np.int32(9), np.int32(0),               # action 2\n",
    "    np.int32(1), np.int32(2), np.int32(3),  # state 3 (current state)\n",
    "    np.int32(4), np.int32(5)                # baselines\n",
    ")\n",
    "\n",
    "result = transform_to_reset_format(test_data, state_cols, action_cols, baseline_cols)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PatientEnvironment Class ===\n",
    "class PatientEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    A Gym environment for patient simulation using historical medical data.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs=3, stable_hr=2, stable_threshold=2, timestamp_reward_step=1):\n",
    "        super(PatientEnvironment, self).__init__()\n",
    "        \n",
    "        # Configuration\n",
    "        self.train_transitions = train_transitions\n",
    "        self.state_cols = state_cols\n",
    "        self.action_cols = action_cols\n",
    "        self.baseline_cols = baseline_cols\n",
    "        self.num_bins_dict = num_bins_dict\n",
    "        self.min_max_values_guidelines_bin_dict = min_max_values_guidelines_bin_dict\n",
    "        self.obs_hrs = obs_hrs\n",
    "        self.stable_hr = stable_hr\n",
    "        self.stable_threshold = stable_threshold\n",
    "        self.timestamp_reward_step = timestamp_reward_step\n",
    "        \n",
    "        # Define observation space\n",
    "        state_bins = [num_bins_dict[col] for col in state_cols]\n",
    "        action_bins = [num_bins_dict[col] for col in action_cols]\n",
    "        baseline_bins = [num_bins_dict[col] for col in baseline_cols]\n",
    "        nvec = []\n",
    "        for _ in range(obs_hrs - 1):  # History of obs_hrs-1 state-action pairs\n",
    "            nvec.extend(state_bins)\n",
    "            nvec.extend(action_bins)\n",
    "        nvec.extend(state_bins)  # Current state\n",
    "        nvec.extend(baseline_bins)  # Baseline\n",
    "        self.observation_space = spaces.MultiDiscrete(nvec)\n",
    "        \n",
    "        # Define action space\n",
    "        self.action_bins = action_bins\n",
    "        num_actions = np.prod(action_bins)\n",
    "        self.action_space = spaces.Discrete(num_actions)\n",
    "        \n",
    "        # Action mapping\n",
    "        self.id_to_action = {i: a for i, a in enumerate(itertools.product(*[range(b) for b in action_bins]))}\n",
    "        self.action_to_id = {a: i for i, a in self.id_to_action.items()}\n",
    "        \n",
    "        # State variables\n",
    "        self.history = []  # List of (state, action) tuples\n",
    "        self.current_state = None\n",
    "        self.previous_action = None\n",
    "        self.current_action = None\n",
    "        self.current_baseline = None\n",
    "        self.meet_weaning_history = []\n",
    "        self.steps = 0\n",
    "        \n",
    "        # KDTree for approximation\n",
    "        self.train_keys_list = list(train_transitions.keys())\n",
    "        self.train_X = np.array([self._flatten_key(key) for key in self.train_keys_list])\n",
    "        self.tree = KDTree(self.train_X, metric='manhattan')\n",
    "\n",
    "    # def reset(self):\n",
    "    #     \"\"\"Reset the environment to an initial state.\"\"\"\n",
    "    #     initial_key = random.choice(self.train_keys_list)\n",
    "    #     self.history = list(initial_key[:self.obs_hrs - 1])  # obs_hrs-1 state-action pairs\n",
    "    #     self.current_state = initial_key[self.obs_hrs - 1][0]  # Current state\n",
    "    #     self.current_baseline = initial_key[-1]  # Baseline\n",
    "    #     self.meet_weaning_history = []\n",
    "    #     self.steps = 0\n",
    "    #     return self._get_obs()\n",
    "\n",
    "    def reset(self, initial_key=None):\n",
    "        \"\"\"Reset the environment to an initial state.\"\"\"\n",
    "        if initial_key is None:\n",
    "            initial_key = random.choice(self.train_keys_list)\n",
    "        else:\n",
    "            # for i in range(len(initial_key)):\n",
    "            #     initial_key[i] = int(initial_key[i])\n",
    "            state_len = len(state_cols)\n",
    "            action_len = len(action_cols)\n",
    "            baseline_len = len(baseline_cols)\n",
    "\n",
    "            # Split the data into states, actions, and baselines\n",
    "            num_states = (len(initial_key) - baseline_len) // (state_len + action_len)\n",
    "            states = []\n",
    "            actions = []\n",
    "            for i in range(num_states):\n",
    "                start_idx = i * (state_len + action_len)\n",
    "                states.append(tuple(initial_key[start_idx:start_idx + state_len]))\n",
    "                actions.append(tuple(initial_key[start_idx + state_len:start_idx + state_len + action_len]))\n",
    "            # last state is the current state\n",
    "            current_state = tuple(initial_key[-(state_len + baseline_len):-baseline_len])\n",
    "            states.append(current_state)\n",
    "            # Extract baselines\n",
    "            baselines = tuple(initial_key[-baseline_len:])\n",
    "\n",
    "            # Create the final format\n",
    "            result = []\n",
    "            for i in range(num_states+1):\n",
    "                # Use the last action if the current action is missing\n",
    "                action = actions[i] if i < len(actions) else actions[-1]\n",
    "                result.append((states[i], action))\n",
    "            \n",
    "            # Add the baselines\n",
    "            result.append(baselines)\n",
    "\n",
    "            initial_key = tuple(result)\n",
    "\n",
    "        # print(f\"initial_key: {initial_key}\")\n",
    "        self.history = list(initial_key[:self.obs_hrs - 1])  # obs_hrs-1 state-action pairs\n",
    "        # print(f\"self.history: {self.history}\")\n",
    "        self.current_state = initial_key[self.obs_hrs - 1][0]  # Current state\n",
    "        # print(f\"self.current_state: {self.current_state}\")\n",
    "        self.current_action = initial_key[self.obs_hrs - 2][1]  # Previous action\n",
    "        # print(f\"self.previous_action: {self.previous_action}\")\n",
    "        self.current_baseline = initial_key[-1]  # Baseline\n",
    "        # print(f\"self.current_baseline: {self.current_baseline}\")\n",
    "        self.meet_weaning_history = []\n",
    "        self.steps = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment.\"\"\"\n",
    "        self.previous_action = self.current_action\n",
    "        self.current_action = self.id_to_action[action]\n",
    "        key = tuple(self.history) + ((self.current_state, self.current_action),) + (self.current_baseline,) # TODO: self debug\n",
    "        if key in self.train_transitions and not ALL_USE_UNSEEN_FLAG: # [cont_update]\n",
    "            next_state = random.choice(self.train_transitions[key])\n",
    "            nonsense_action = False\n",
    "        else:\n",
    "            next_state, nonsense_action = self._action_based_approximation_filter_nonsense_action(key)\n",
    "        \n",
    "        # Update state\n",
    "        self.history = self.history[1:] + [(self.current_state, self.current_action)]\n",
    "        self.steps += 1\n",
    "        reward = self._calculate_reward(self.current_action, next_state) # TODO: since we need to get the current state before update\n",
    "        self.current_state = next_state\n",
    "        done = self._check_termination() or nonsense_action\n",
    "        meet_weaning = False\n",
    "\n",
    "        # termination have three cases:\n",
    "        # 1. meet weaning condition\n",
    "        # 2. nonsense action\n",
    "        # 3. steps exceed max trajectory length\n",
    "\n",
    "        # 1. meet weaning condition\n",
    "        if (len(self.meet_weaning_history) >= 6 and all(self.meet_weaning_history[-6:])):\n",
    "            # print(\"meet weaning condition\")\n",
    "            meet_weaning = True\n",
    "            reward += EVAL_WEANING_REWARD\n",
    "        # 2. nonsense action\n",
    "        elif nonsense_action:\n",
    "            # print(\"nonsense action\")\n",
    "            reward -= EVAL_WEANING_REWARD\n",
    "        # 3. steps exceed max trajectory length\n",
    "        elif self.steps >= EVAL_MAX_TRAJ_LENGTH:\n",
    "            # print(\"steps exceed max trajectory length\")\n",
    "            reward -= EVAL_WEANING_REWARD\n",
    "\n",
    "        return self._get_obs(), reward, done, {\"nonsense_action\": nonsense_action, \"meet_weaning\": meet_weaning} # TODO: add info dict for nonsense action\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Return the current observation as a flattened array.\"\"\"\n",
    "        flat = []\n",
    "        for s, a in self.history:\n",
    "            flat.extend(s)\n",
    "            flat.extend(a)\n",
    "        flat.extend(self.current_state)\n",
    "        flat.extend(self.current_baseline)\n",
    "        return np.array(flat, dtype=np.int32)\n",
    "\n",
    "    def _calculate_reward(self, current_action, next_state):\n",
    "        \"\"\"\n",
    "        Calculate the reward based on next_state, action, and history.\n",
    "\n",
    "        Penalties:\n",
    "        - state_penalty: -1 if any next_state value is outside its guideline range (else 0)\n",
    "        - action_penalty: -1 if any action is outside its guideline range (else 0)\n",
    "        - stable_penalty: -1 if any jump >= stable_threshold in the last stable_hr steps (else 0)\n",
    "        - timestamp_penalty: -1 every timestamp_reward_step steps (else 0)\n",
    "\n",
    "        A one-time +100 reward (weaning bonus) is granted once the state_penalty,\n",
    "        action_penalty, and stable_penalty are all zero for 6 consecutive steps,\n",
    "        after step >= 23.  The episode then terminates via _check_termination().\n",
    "        \"\"\"\n",
    "        # TODO: [update reward Part 1]\n",
    "        # TODO: [update reward] reward shaping\n",
    "        # State penalty: -1 if any vital sign is outside guidelines\n",
    "        state_penalty = -1 if any(\n",
    "            not (self.min_max_values_guidelines_bin_dict[col]['min'] <= next_state[i] <=\n",
    "                self.min_max_values_guidelines_bin_dict[col]['max'])\n",
    "            for i, col in enumerate(self.state_cols)\n",
    "        ) else 0\n",
    "\n",
    "        state_reward = sum(\n",
    "            1 if min_max_values_guidelines_bin_dict[col]['min'] <= next_state[i] <= min_max_values_guidelines_bin_dict[col]['max'] else 0\n",
    "            for i, col in enumerate(self.state_cols)\n",
    "        ) / len(self.state_cols)\n",
    "\n",
    "        \n",
    "        action_penalty = sum(\n",
    "            -1 if current_action[i] < min_max_values_guidelines_bin_dict[col]['min'] or current_action[i] > min_max_values_guidelines_bin_dict[col]['max'] else 0\n",
    "            for i, col in enumerate(action_cols)\n",
    "        ) / len(action_cols)\n",
    "\n",
    "\n",
    "        state_stable_penalty = sum(\n",
    "            -1 if abs(next_state[i] - self.current_state[i]) >= self.stable_threshold else 0\n",
    "            for i, col in enumerate(state_cols)\n",
    "        ) / len(state_cols)\n",
    "\n",
    "        action_stable_penalty = sum(\n",
    "            -1 if abs(self.current_action[i] - self.previous_action[i]) >= self.stable_threshold * 2 else 0 # TODO: [update reward] *2 not sure, hope can guide not (8, 5), (8, 5), (1, 0)\n",
    "            for i, col in enumerate(action_cols)\n",
    "        ) / len(action_cols)\n",
    "\n",
    "\n",
    "        # Timestamp penalty: -1 every timestamp_reward_step steps\n",
    "        timestamp_penalty = -1 if (self.steps + 1) % self.timestamp_reward_step == 0 else 0\n",
    "\n",
    "        # Record whether all three penalties are zero (weaning condition) at this step\n",
    "        meet_condition = (state_penalty == 0 and action_penalty == 0 and state_stable_penalty == 0 and action_stable_penalty == 0) # TODO: [update reward] add action_stable_penalty\n",
    "        # meet_condition = (state_penalty == 0 and action_penalty == 0 and state_stable_penalty == 0)\n",
    "        # Total reward is sum of all penalties (non-positive unless all are zero)\n",
    "        \n",
    "        if not EVAL_ACTION_PENALTY_FLAG:\n",
    "            action_penalty = 0\n",
    "        if not EVAL_ACTION_STABLE_PENALTY_FLAG:\n",
    "            action_stable_penalty = 0\n",
    "\n",
    "        reward = state_reward + action_penalty + state_stable_penalty + action_stable_penalty + timestamp_penalty\n",
    "\n",
    "        self.meet_weaning_history.append(1 if meet_condition else 0)\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def _check_termination(self):\n",
    "        \"\"\"Check if the episode should terminate.\"\"\"\n",
    "        return (len(self.meet_weaning_history) >= 6 and all(self.meet_weaning_history[-6:])) or self.steps >= EVAL_MAX_TRAJ_LENGTH # TODO: [update reward] add and self.steps >= 23 s.t. it at least 24 hr (not sure reasonable)\n",
    "        # return (len(self.meet_weaning_history) >= 6 and all(self.meet_weaning_history[-6:])) or self.steps >= 120 # TODO: [update reward] add and self.steps >= 23 s.t. it at least 24 hr (not sure reasonable)\n",
    "        # return (len(self.meet_weaning_history) >= 6 and all(self.meet_weaning_history[-6:])) and self.steps >= 23 or self.steps >= 120 # TODO: [update reward] add and self.steps >= 23 s.t. it at least 24 hr (not sure reasonable)\n",
    "\n",
    "    def _action_based_approximation(self, key):\n",
    "        \"\"\"Approximate next state when key is not in transitions.\"\"\"\n",
    "        desired_action = key[-2][1]\n",
    "        flat_key = self._flatten_key(key)\n",
    "        distances, indices = self.tree.query(flat_key.reshape(1, -1), k=100) # TODO: number of neighbors, should be selected by experiment\n",
    "        candidates = [self.train_keys_list[i] for i in indices[0] if self.train_keys_list[i][-2][1] == desired_action]\n",
    "        # print(f\"len of candidates: {len(candidates)}\")\n",
    "        # print(f\"key: {key}\")\n",
    "        # print(f\"candidates: {candidates}\")\n",
    "        if not candidates:\n",
    "            return key[-2][0]\n",
    "        all_deltas = [tuple(np.array(s_next) - np.array(cand_key[-2][0])) \n",
    "                      for cand_key in candidates for s_next in self.train_transitions[cand_key]]\n",
    "        delta = random.choice(all_deltas) if all_deltas else (0,) * len(self.state_cols)\n",
    "        s_t = key[-2][0]\n",
    "        return tuple(np.clip(s_t[i] + delta[i], 0, self.num_bins_dict[self.state_cols[i]] - 1) \n",
    "                    for i in range(len(s_t)))\n",
    "    \n",
    "    def _action_based_approximation_filter_nonsense_action(self, key):\n",
    "        \"\"\"Approximate next state when key is not in transitions.\"\"\"\n",
    "        desired_action = key[-2][1]\n",
    "        flat_key = self._flatten_key(key)\n",
    "        distances, indices = self.tree.query(flat_key.reshape(1, -1), k=100) # TODO: number of neighbors, should be selected by experiment\n",
    "        candidates = [self.train_keys_list[i] for i in indices[0] if self.train_keys_list[i][-2][1] == desired_action]\n",
    "        # print(f\"len of candidates: {len(candidates)}\")\n",
    "        # print(f\"key: {key}\")\n",
    "        # print(f\"candidates: {candidates}\")\n",
    "        if not candidates:\n",
    "            return key[-2][0], True # TODO: currently return that last state, but should penalty reward since this pair not seen in dataset with 100 around neighbors\n",
    "        all_deltas = [tuple(np.array(s_next) - np.array(cand_key[-2][0])) \n",
    "                      for cand_key in candidates for s_next in self.train_transitions[cand_key]]\n",
    "        delta = random.choice(all_deltas) if all_deltas else (0,) * len(self.state_cols)\n",
    "        s_t = key[-2][0]\n",
    "        return tuple(np.clip(s_t[i] + delta[i], 0, self.num_bins_dict[self.state_cols[i]] - 1) \n",
    "                    for i in range(len(s_t))), False\n",
    "    \n",
    "    def _transformer_approximation(self, key, transitions, n_samples=1):\n",
    "        \"\"\"Transformer-based approximation of next state.\"\"\"\n",
    "        \n",
    "        # Load probabilistic Transformer model\n",
    "        if 'probabilistic_transformer_model' not in globals():\n",
    "            model_dir = os.path.join(mimic_iv_prefix_path, \"models\")\n",
    "            # model_path = os.path.join(model_dir, 'probabilistic_probabilistictransformernextstatepredictor_best.pth')\n",
    "            model_path = os.path.join(model_dir, 'probabilistic_probabilistictransformernextstatepredictor_best_ground_truth_with_neighbors.pth') # [gt_with_neighbors_update]\n",
    "            \n",
    "            try:\n",
    "                # Load the checkpoint\n",
    "                checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "                \n",
    "                # Use the saved output_dim_dict from checkpoint if available\n",
    "                if 'output_dim_dict' in checkpoint:\n",
    "                    output_dim_dict = checkpoint['output_dim_dict']\n",
    "                else:\n",
    "                    output_dim_dict = {var_name: num_bins_dict[var_name] for var_name in state_cols}\n",
    "                \n",
    "                # Initialize the model\n",
    "                probabilistic_transformer = ProbabilisticTransformerNextStatePredictor(\n",
    "                    # input_dim=StateTransitionDataset(transitions).features.shape[1],\n",
    "                    input_dim=input_dim,\n",
    "                    hidden_dim=128,\n",
    "                    num_layers=2,\n",
    "                    output_dim_dict=output_dim_dict,\n",
    "                    dropout=0.2,\n",
    "                    nhead=4  # Add the nhead parameter that was used in training\n",
    "                )\n",
    "                \n",
    "                # Load the state dict using the correct key\n",
    "                probabilistic_transformer.load_state_dict(checkpoint['model_state_dict'])\n",
    "                print(\"Successfully loaded Transformer model from:\", model_path)\n",
    "                globals()['probabilistic_transformer_model'] = probabilistic_transformer\n",
    "                \n",
    "            except (FileNotFoundError, KeyError) as e:\n",
    "                print(f\"Error loading Transformer model: {e}\")\n",
    "                print(\"Training a new probabilistic Transformer model...\")\n",
    "                probabilistic_transformer = train_probabilistic_model(\n",
    "                    ProbabilisticTransformerNextStatePredictor,\n",
    "                    train_transitions, \n",
    "                    state_cols, \n",
    "                    hidden_dim=128, \n",
    "                    nhead=4,\n",
    "                    num_layers=2,\n",
    "                    dropout=0.2\n",
    "                )\n",
    "                globals()['probabilistic_transformer_model'] = probabilistic_transformer\n",
    "        \n",
    "        # Use the cached model for prediction\n",
    "        transformer_samples = probabilistic_next_state_prediction(\n",
    "            globals()['probabilistic_transformer_model'], \n",
    "            key, \n",
    "            n_samples=n_samples\n",
    "        )\n",
    "        \n",
    "        return transformer_samples[0]\n",
    "\n",
    "    def _flatten_key(self, key):\n",
    "        \"\"\"Flatten a key for KDTree.\"\"\"\n",
    "        flat = []\n",
    "        for s, a in key[:-1]:\n",
    "            flat.extend(s)\n",
    "            flat.extend(a)\n",
    "        flat.extend(key[-1])\n",
    "        return np.array(flat, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_state(history, current_state, baseline):\n",
    "    flat = []\n",
    "    for s, a in history:\n",
    "        flat.extend(s)\n",
    "        flat.extend(a)\n",
    "    flat.extend(current_state)\n",
    "    flat.extend(baseline)\n",
    "    return np.array(flat, dtype=np.int32)\n",
    "\n",
    "\n",
    "def preprocess_data(df, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs=3, stable_hr=2, stable_threshold=2, timestamp_reward_step=1, satisfy_weaning_reward=10):\n",
    "    \"\"\"\n",
    "    Preprocess dataframe into a D3RLPY MDPDataset, using the consistent reward logic:\n",
    "      - state_penalty = -1 if any vital out of guideline range (else 0)\n",
    "      - action_penalty = -1 if any action out of guideline range (else 0)\n",
    "      - state_stable_penalty = -1 if any jump >= threshold over last stable_hr states (else 0)\n",
    "      - timestamp_penalty = -1 every timestamp_reward_step steps (else 0)\n",
    "      - satisfy_weaning_reward = +100 granted once when all three penalties are 0 for 6 consecutive steps (then terminal).\n",
    "    \"\"\"\n",
    "    from itertools import product\n",
    "    import numpy as np\n",
    "    import d3rlpy\n",
    "    \n",
    "    # Build action -> ID map\n",
    "    action_bins = [num_bins_dict[col] for col in action_cols]\n",
    "    action_to_id = {tuple(a): i\n",
    "                    for i, a in enumerate(product(*[range(b) for b in action_bins]))}\n",
    "    \n",
    "    transitions = []\n",
    "    not_met_weaning_count = 0\n",
    "    met_weaning_count = 0\n",
    "    for stay_id, group in df.groupby('stay_id'):\n",
    "        group = group.sort_values('hours_in')\n",
    "        if len(group) < obs_hrs + 1:\n",
    "            continue\n",
    "        \n",
    "        history = []\n",
    "        rows = group.to_dict('records')\n",
    "        # Seed history with first (obs_hrs-1) state-action pairs\n",
    "        for i in range(obs_hrs - 1):\n",
    "            s = tuple(rows[i][col] for col in state_cols)\n",
    "            a = tuple(rows[i][col] for col in action_cols)\n",
    "            current_action = a\n",
    "            history.append((s, a))\n",
    "        baseline = tuple(rows[0][col] for col in baseline_cols)\n",
    "        \n",
    "        weaning_met = False\n",
    "        weaning_count = 0\n",
    "        \n",
    "        transition = []\n",
    "        # Iterate from obs_hrs-1 to the penultimate row\n",
    "        # TODO: [update reward Part 3]\n",
    "        for t in range(obs_hrs - 1, len(rows) - 1):\n",
    "            # if weaning_met: # TODO: [update reward] if want clip at the first time met weaning condition\n",
    "            #     break  # Stop once weaning is achieved (trajectory clipped)\n",
    "            previous_action = current_action\n",
    "            step = t - (obs_hrs - 1)  # zero-indexed step count\n",
    "            current_state = tuple(rows[t][col] for col in state_cols)\n",
    "            current_action = tuple(rows[t][col] for col in action_cols)\n",
    "            next_state    = tuple(rows[t+1][col] for col in state_cols)\n",
    "            \n",
    "            # 1) State penalty\n",
    "            state_penalty = -1 if any(\n",
    "                next_state[i] < min_max_values_guidelines_bin_dict[col]['min'] or\n",
    "                next_state[i] > min_max_values_guidelines_bin_dict[col]['max']\n",
    "                for i, col in enumerate(state_cols)\n",
    "            ) else 0\n",
    "\n",
    "            state_reward = sum(\n",
    "                1 if min_max_values_guidelines_bin_dict[col]['min'] <= next_state[i] <= min_max_values_guidelines_bin_dict[col]['max'] else 0\n",
    "                for i, col in enumerate(state_cols)\n",
    "            ) / len(state_cols)\n",
    "            \n",
    "            # 2) Action penalty\n",
    "            \n",
    "            action_penalty = sum(\n",
    "                -1 if current_action[i] < min_max_values_guidelines_bin_dict[col]['min'] or current_action[i] > min_max_values_guidelines_bin_dict[col]['max'] else 0\n",
    "                for i, col in enumerate(action_cols)\n",
    "            ) / len(action_cols)\n",
    "            \n",
    "            # 3) Stable penalty # TODO: [update reward] make sure it only check next state - current state\n",
    "            # for each of the state_cols, next_state - current_state >= stable_threshold then state_stable_penalty = -1, don't need to check history\n",
    "            state_stable_penalty = sum(\n",
    "                -1 if abs(next_state[i] - current_state[i]) >= stable_threshold else 0\n",
    "                for i, col in enumerate(state_cols)\n",
    "            ) / len(state_cols)\n",
    "\n",
    "            action_stable_penalty = sum(\n",
    "                -1 if abs(previous_action[i] - current_action[i]) >= stable_threshold * 2 else 0\n",
    "                for i, col in enumerate(action_cols)\n",
    "            ) / len(action_cols)\n",
    "            \n",
    "            # 4) Timestamp penalty (every timestamp_reward_step)\n",
    "            if TIMESTAMP_PENALTY_FLAG:\n",
    "                timestamp_penalty = 1 if (step + 1) % timestamp_reward_step == 0 else 0 # TODO: [update reward] --> set 0 due to transition MDP don't have sequential concept\n",
    "            else:\n",
    "                timestamp_penalty = 0 if (step + 1) % timestamp_reward_step == 0 else 0 # TODO: [update reward] --> set 0 due to transition MDP don't have sequential concept\n",
    "                                                                                    # TODO: update --> to align with the Patient Environment\n",
    "            \n",
    "            # 5) Check if all penalties are zero (weaning condition)\n",
    "            # if state_penalty == 0 and action_penalty == 0 and state_stable_penalty == 0:\n",
    "            if state_penalty == 0 and action_penalty == 0 and state_stable_penalty == 0 and action_stable_penalty == 0:\n",
    "                weaning_count += 1\n",
    "            else:\n",
    "                weaning_count = 0\n",
    "\n",
    "            # Base reward (sum of penalties)\n",
    "            # reward = state_penalty + action_penalty + state_stable_penalty + timestamp_penalty\n",
    "            if not ACTION_PENALTY_FLAG:\n",
    "                action_penalty = 0\n",
    "            if not ACTION_STABLE_PENALTY_FLAG:\n",
    "                action_stable_penalty = 0\n",
    "            if not STATE_REWARD_FLAG:\n",
    "                state_reward = 0\n",
    "            if not STATE_STABLE_PENALTY_FLAG:\n",
    "                state_stable_penalty = 0\n",
    "\n",
    "            reward = state_reward + action_penalty + state_stable_penalty + action_stable_penalty + timestamp_penalty\n",
    "            \n",
    "            # 6) Grant +100 and clip if sustained for 6 steps (with min step 23)\n",
    "            terminal = False\n",
    "            # if weaning_count >= 6 and step >= 23: # TODO: [update reward]\n",
    "            if weaning_count >= 6: # TODO: [update reward]\n",
    "                weaning_met = True\n",
    "                if WEANING_REWARD_FLAG:\n",
    "                    reward += WEANING_REWARD\n",
    "            #     reward += satisfy_weaning_reward\n",
    "            #     terminal = True\n",
    "            if t == len(rows) - 2:\n",
    "                terminal = True\n",
    "                if weaning_count < 6 and WEANING_REWARD_FLAG: # TODO: [update reward] not sure will cause problem\n",
    "                    reward -= WEANING_REWARD\n",
    "            # [update reward] TODO: else if the traj end with last step satisfy weaning condition, then reward += satisfy_weaning_reward\n",
    "            # since the real data might wean without keep 6 steps stable, but the trajectory path may be good\n",
    "            # elif t == len(rows) - 2 and weaning_count >= 2: # TODO: not sure - 2 or - 1?\n",
    "            #     reward += satisfy_weaning_reward\n",
    "            #     terminal = True\n",
    "            #     weaning_met = True\n",
    "            \n",
    "            # Flatten observation and next_observation\n",
    "            obs = flatten_state(history, current_state, baseline)\n",
    "            next_history = history[-(stable_hr-2):] if len(history) >= stable_hr - 1 else history[1:]\n",
    "            next_history = next_history + [(current_state, current_action)]\n",
    "            next_obs = flatten_state(next_history, next_state, baseline)\n",
    "            \n",
    "            # TODO: [update reward] if the traj end without satisfy weaning condition, maybe not add into the MDP dataset\n",
    "            transition.append({\n",
    "                'observation':       obs,\n",
    "                'action':            action_to_id[current_action],\n",
    "                'reward':            reward,\n",
    "                'next_observation':  next_obs,\n",
    "                'terminal':          terminal\n",
    "            })\n",
    "            \n",
    "            # Update history\n",
    "            history = history[1:] + [(current_state, current_action)]\n",
    "        # Append transitions if weaning condition met\n",
    "        if weaning_met:\n",
    "            transitions.extend(transition)\n",
    "            met_weaning_count += 1\n",
    "        else:\n",
    "            transitions.extend(transition) # TODO: [update reward] since the MDP is \"transition\", so met or not doesn't matter\n",
    "            not_met_weaning_count += 1\n",
    "    print(f\"Number of trajectories meeting weaning condition: {met_weaning_count}\")\n",
    "    print(f\"Number of trajectories not meeting weaning condition: {not_met_weaning_count}\")\n",
    "    print(f\"Total number of trajectories: {met_weaning_count + not_met_weaning_count}\")\n",
    "    print(f\"percentage of trajectories meeting weaning condition: {met_weaning_count / (met_weaning_count + not_met_weaning_count) * 100:.2f}%\")\n",
    "    print(f\"percentage of trajectories not meeting weaning condition: {not_met_weaning_count / (met_weaning_count + not_met_weaning_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Collate into arrays for MDPDataset\n",
    "    observations      = np.stack([t['observation'] for t in transitions])\n",
    "    actions           = np.array([t['action']           for t in transitions], dtype=np.int32)\n",
    "    rewards           = np.array([t['reward']           for t in transitions], dtype=np.float32)\n",
    "    next_observations = np.stack([t['next_observation'] for t in transitions])\n",
    "    terminals         = np.array([t['terminal']         for t in transitions], dtype=bool)\n",
    "    \n",
    "    return d3rlpy.dataset.MDPDataset(observations, actions, rewards, terminals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KDTree\n",
    "import itertools\n",
    "import d3rlpy\n",
    "import torch\n",
    "from d3rlpy.algos import DQNConfig, DoubleDQNConfig, DiscreteSACConfig, DecisionTransformerConfig\n",
    "from d3rlpy.algos import DiscreteDecisionTransformerConfig, DiscreteDecisionTransformer\n",
    "from d3rlpy.models.encoders import DefaultEncoderFactory\n",
    "from d3rlpy.models.q_functions import MeanQFunctionFactory\n",
    "from d3rlpy.models import DefaultEncoderFactory\n",
    "from d3rlpy import PositionEncodingType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KDTree\n",
    "import itertools\n",
    "import d3rlpy\n",
    "import torch\n",
    "from d3rlpy.algos import DQNConfig, DoubleDQNConfig, DiscreteSACConfig, DecisionTransformerConfig, NFQ, NFQConfig\n",
    "from d3rlpy.algos import DiscreteDecisionTransformerConfig, DiscreteDecisionTransformer\n",
    "from d3rlpy.algos.transformer.inputs import TorchTransformerInput\n",
    "from d3rlpy.algos import DiscreteBC, DiscreteBCConfig\n",
    "from d3rlpy.models.encoders import DefaultEncoderFactory\n",
    "from d3rlpy.models.q_functions import MeanQFunctionFactory\n",
    "from d3rlpy.models import DefaultEncoderFactory\n",
    "from d3rlpy import PositionEncodingType\n",
    "from d3rlpy.algos import DiscreteBCQ, DiscreteCQL\n",
    "from d3rlpy.algos import DiscreteBCQConfig, DiscreteCQLConfig\n",
    "from d3rlpy.optimizers import OptimizerFactory\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def get_newest_json(algo_name):\n",
    "    # Construct the search pattern for the algorithm's JSON files\n",
    "    search_pattern = f\"./d3rlpy_logs/{algo_name}_*/params.json\"\n",
    "    # Use glob to find all matching files\n",
    "    json_files = glob.glob(search_pattern)\n",
    "    # Sort the files by their modification time in descending order\n",
    "    json_files.sort(key=os.path.getmtime, reverse=True)\n",
    "    # Return the newest file (if any)\n",
    "    return json_files[0] if json_files else None\n",
    "\n",
    "def get_newest_loss(algo_name):\n",
    "    # Construct the search pattern for the algorithm's csv files\n",
    "    search_pattern = f\"./d3rlpy_logs/{algo_name}_*/loss.csv\"\n",
    "    # Use glob to find all matching files\n",
    "    loss_csv_files = glob.glob(search_pattern)\n",
    "    # Sort the files by their modification time in descending order\n",
    "    loss_csv_files.sort(key=os.path.getmtime, reverse=True)\n",
    "    # Return the newest file (if any)\n",
    "    return loss_csv_files[0] if loss_csv_files else None\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(df, dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs=3, algo='DQN', data_source='test', retrain=False):\n",
    "    \"\"\"\n",
    "    Train or load a D3RLPY agent, then evaluate it.\n",
    "    \"\"\"\n",
    "    # Device selection\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Prepare model directory\n",
    "    model_dir = f\"../models/training_log/{EXP_FOLDER_PREFIX}/models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, f\"{algo}_{data_source}.pkl\")\n",
    "\n",
    "\n",
    "    # Create encoder factory with batch normalization and dropout\n",
    "    encoder_factory = DefaultEncoderFactory(use_batch_norm=True, dropout_rate=0.5)\n",
    "    # Create optimizer factory with Adam optimizer\n",
    "    # optim_factory = OptimizerFactory(Adam)\n",
    "    \n",
    "    # Define algorithm mapping\n",
    "    # [2025/06/09] Current used settings\n",
    "    \"\"\" algo_map = {\n",
    "        'DQN': (d3rlpy.algos.DQN, DQNConfig(learning_rate=1e-4, gamma=0.98, target_update_interval=1000, batch_size=2048, encoder_factory=encoder_factory)),\n",
    "        'DoubleDQN': (d3rlpy.algos.DoubleDQN, DoubleDQNConfig(learning_rate=1e-4, gamma=0.98, target_update_interval=1000, batch_size=2048, encoder_factory=encoder_factory)),\n",
    "        'DiscreteSAC': (d3rlpy.algos.DiscreteSAC, DiscreteSACConfig(actor_learning_rate=3e-4, critic_learning_rate=3e-4, batch_size=2048, gamma=0.98, actor_encoder_factory=encoder_factory, critic_encoder_factory=encoder_factory, target_update_interval=1000)), # [2025/05/21] current best\n",
    "        'DecisionTransformer': (d3rlpy.algos.DiscreteDecisionTransformer, DiscreteDecisionTransformerConfig( context_size=1, batch_size=128, learning_rate=6e-4, max_timestep=int(df.groupby('stay_id')['hours_in'].max().max() + 1), num_heads=8, num_layers=6)), # context_size=obs_hrs, # TODO: Prior sequence length. \n",
    "        'DiscreteBCQ': (d3rlpy.algos.DiscreteBCQ, d3rlpy.algos.DiscreteBCQConfig(batch_size=128, learning_rate=1e-5, gamma=0.99, target_update_interval=1000, encoder_factory=encoder_factory)),\n",
    "        'DiscreteCQL': (d3rlpy.algos.DiscreteCQL, d3rlpy.algos.DiscreteCQLConfig(batch_size=1024, learning_rate=1e-5, gamma=0.75, target_update_interval=1000, encoder_factory=encoder_factory, alpha=0.1)), # TODO: this performance is good\n",
    "        'DiscreteBC': (d3rlpy.algos.DiscreteBC, d3rlpy.algos.DiscreteBCConfig(batch_size=1024, learning_rate=1e-6, gamma=0.99, encoder_factory=encoder_factory, beta=0.1)),\n",
    "        'NFQ': (d3rlpy.algos.NFQ, d3rlpy.algos.NFQConfig(batch_size=1024, learning_rate=1e-6, gamma=0.99, encoder_factory=encoder_factory)),\n",
    "    } \"\"\"\n",
    "    # same parameter settings v1\n",
    "    learning_rate = 1e-5\n",
    "    # learning_rate = 1e-6\n",
    "    gamma = 0.98\n",
    "    target_update_interval = 1000\n",
    "    batch_size = 1024 \n",
    "    algo_map = {\n",
    "        'DQN': (d3rlpy.algos.DQN, DQNConfig(learning_rate=learning_rate, gamma=gamma, target_update_interval=target_update_interval, batch_size=batch_size, encoder_factory=encoder_factory)),\n",
    "        'DoubleDQN': (d3rlpy.algos.DoubleDQN, DoubleDQNConfig(learning_rate=learning_rate, gamma=gamma, target_update_interval=target_update_interval, batch_size=batch_size, encoder_factory=encoder_factory)),\n",
    "        'DiscreteSAC': (d3rlpy.algos.DiscreteSAC, DiscreteSACConfig(actor_learning_rate=learning_rate, critic_learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, actor_encoder_factory=encoder_factory, critic_encoder_factory=encoder_factory, target_update_interval=target_update_interval)), # [2025/05/21] current best\n",
    "        'DecisionTransformer': (d3rlpy.algos.DiscreteDecisionTransformer, DiscreteDecisionTransformerConfig( context_size=1, batch_size=batch_size, learning_rate=learning_rate, max_timestep=int(df.groupby('stay_id')['hours_in'].max().max() + 1), num_heads=8, num_layers=6)), # context_size=obs_hrs, # TODO: Prior sequence length. \n",
    "        'DiscreteBCQ': (d3rlpy.algos.DiscreteBCQ, d3rlpy.algos.DiscreteBCQConfig(batch_size=batch_size, learning_rate=learning_rate, gamma=gamma, target_update_interval=target_update_interval, encoder_factory=encoder_factory)),\n",
    "        'DiscreteCQL': (d3rlpy.algos.DiscreteCQL, d3rlpy.algos.DiscreteCQLConfig(batch_size=batch_size, learning_rate=learning_rate, gamma=gamma, target_update_interval=target_update_interval, encoder_factory=encoder_factory, alpha=0.1)), # TODO: this performance is good\n",
    "        'DiscreteBC': (d3rlpy.algos.DiscreteBC, d3rlpy.algos.DiscreteBCConfig(batch_size=batch_size, learning_rate=learning_rate, gamma=gamma, encoder_factory=encoder_factory, beta=0.1)),\n",
    "        'NFQ': (d3rlpy.algos.NFQ, d3rlpy.algos.NFQConfig(batch_size=batch_size, learning_rate=learning_rate, gamma=gamma, encoder_factory=encoder_factory)),\n",
    "    }\n",
    "\n",
    "    \"\"\" # D3RLPY default settings for baseline\n",
    "    target_update_interval = 1000\n",
    "    algo_map = {\n",
    "        'DQN': (d3rlpy.algos.DQN, DQNConfig(target_update_interval=target_update_interval)),\n",
    "        'DoubleDQN': (d3rlpy.algos.DoubleDQN, DoubleDQNConfig(target_update_interval=target_update_interval)),\n",
    "        'DiscreteSAC': (d3rlpy.algos.DiscreteSAC, DiscreteSACConfig(target_update_interval=target_update_interval)), # [2025/05/21] current best\n",
    "        # 'DecisionTransformer': (d3rlpy.algos.DiscreteDecisionTransformer, DiscreteDecisionTransformerConfig( context_size=1, batch_size=batch_size, learning_rate=learning_rate, max_timestep=int(df.groupby('stay_id')['hours_in'].max().max() + 1), num_heads=8, num_layers=6)), # context_size=obs_hrs, # TODO: Prior sequence length. \n",
    "        'DiscreteBCQ': (d3rlpy.algos.DiscreteBCQ, d3rlpy.algos.DiscreteBCQConfig(target_update_interval=target_update_interval)),\n",
    "        'DiscreteCQL': (d3rlpy.algos.DiscreteCQL, d3rlpy.algos.DiscreteCQLConfig(target_update_interval=target_update_interval)), # TODO: this performance is good\n",
    "        'DiscreteBC': (d3rlpy.algos.DiscreteBC, d3rlpy.algos.DiscreteBCConfig()),\n",
    "        'NFQ': (d3rlpy.algos.NFQ, d3rlpy.algos.NFQConfig()),\n",
    "    } \"\"\"\n",
    "\n",
    "    \"\"\" algo_map = {\n",
    "        # 'DQN': (d3rlpy.algos.DQN, DQNConfig(learning_rate=1e-4, gamma=0.98, target_update_interval=1000, batch_size=2048)), # TODO: [update reward] add L2 regularization\n",
    "        # 'DQN': (d3rlpy.algos.DQN, DQNConfig(gamma=0.99, target_update_interval=1000, batch_size=1024)), # TODO: [update reward] not sure target_update_interval\n",
    "        # 'DQN': (d3rlpy.algos.DQN, DQNConfig(learning_rate=1e-3, gamma=0.99, target_update_interval=1000, batch_size=1024)), # TODO: [update reward] not sure target_update_interval\n",
    "        # 'DQN': (d3rlpy.algos.DQN, DQNConfig( learning_rate=1e-4, gamma=0.98, target_update_interval=1000, batch_size=2048, encoder_factory=encoder_factory, optim_factory=optim_factory)),\n",
    "        'DQN': (d3rlpy.algos.DQN, DQNConfig(learning_rate=1e-4, gamma=0.98, target_update_interval=1000, batch_size=2048, encoder_factory=encoder_factory)),\n",
    "        # 'DoubleDQN': (d3rlpy.algos.DoubleDQN, DoubleDQNConfig(learning_rate=1e-3, gamma=0.99, target_update_interval=1000, batch_size=1024)),\n",
    "        'DoubleDQN': (d3rlpy.algos.DoubleDQN, DoubleDQNConfig(learning_rate=1e-4, gamma=0.98, target_update_interval=1000, batch_size=2048, encoder_factory=encoder_factory)),\n",
    "        # 'DiscreteSAC': (d3rlpy.algos.DiscreteSAC, DiscreteSACConfig(actor_learning_rate=3e-4, critic_learning_rate=3e-4, batch_size=1024, gamma=0.99)),\n",
    "        'DiscreteSAC': (d3rlpy.algos.DiscreteSAC, DiscreteSACConfig(actor_learning_rate=3e-4, critic_learning_rate=3e-4, batch_size=2048, gamma=0.98, actor_encoder_factory=encoder_factory, critic_encoder_factory=encoder_factory, target_update_interval=1000)), # [2025/05/21] current best\n",
    "        # 'DiscreteSAC': (d3rlpy.algos.DiscreteSAC, DiscreteSACConfig(actor_learning_rate=1e-5, critic_learning_rate=1e-5, batch_size=2048, gamma=0.98, actor_encoder_factory=encoder_factory, critic_encoder_factory=encoder_factory, target_update_interval=1000)),\n",
    "        'DecisionTransformer': (d3rlpy.algos.DiscreteDecisionTransformer, DiscreteDecisionTransformerConfig(\n",
    "                context_size=1,\n",
    "                # context_size=obs_hrs, # TODO: Prior sequence length.\n",
    "                batch_size=128,\n",
    "                learning_rate=6e-4,\n",
    "                max_timestep=int(df.groupby('stay_id')['hours_in'].max().max() + 1),\n",
    "                num_heads=8,\n",
    "                num_layers=6\n",
    "            )\n",
    "        ),\n",
    "        # 'DiscreteBCQ': (d3rlpy.algos.DiscreteBCQ, d3rlpy.algos.DiscreteBCQConfig(learning_rate=1e-4, batch_size=2048, gamma=0.98, encoder_factory=encoder_factory, target_update_interval=1000)),\n",
    "        # 'DiscreteBCQ': (d3rlpy.algos.DiscreteBCQ, d3rlpy.algos.DiscreteBCQConfig(batch_size=2048, gamma=0.98)),\n",
    "        # 'DiscreteBCQ': (d3rlpy.algos.DiscreteBCQ, d3rlpy.algos.DiscreteBCQConfig(batch_size=1024, gamma=0.99)),\n",
    "        # 'DiscreteBCQ': (d3rlpy.algos.DiscreteBCQ, d3rlpy.algos.DiscreteBCQConfig(batch_size=32, gamma=0.99, target_update_interval=1000, encoder_factory=encoder_factory)), # [2025/05/21] this is current best, but batch size 32 like not reasonable\n",
    "        'DiscreteBCQ': (d3rlpy.algos.DiscreteBCQ, d3rlpy.algos.DiscreteBCQConfig(batch_size=128, learning_rate=1e-5, gamma=0.99, target_update_interval=1000, encoder_factory=encoder_factory)),\n",
    "        # 'DiscreteBCQ': (d3rlpy.algos.DiscreteBCQ, d3rlpy.algos.DiscreteBCQConfig(batch_size=1024, learning_rate=1e-5, gamma=0.99, target_update_interval=1000, encoder_factory=encoder_factory)),\n",
    "        # 'DiscreteCQL': (d3rlpy.algos.DiscreteCQL, d3rlpy.algos.DiscreteCQLConfig(learning_rate=1e-4, batch_size=2048, gamma=0.98, encoder_factory=encoder_factory, target_update_interval=1000)),\n",
    "        # 'DiscreteCQL': (d3rlpy.algos.DiscreteCQL, d3rlpy.algos.DiscreteCQLConfig(batch_size=2048, gamma=0.98)),\n",
    "        # 'DiscreteCQL': (d3rlpy.algos.DiscreteCQL, d3rlpy.algos.DiscreteCQLConfig(batch_size=1024, gamma=0.99)),\n",
    "        # 'DiscreteCQL': (d3rlpy.algos.DiscreteCQL, d3rlpy.algos.DiscreteCQLConfig(batch_size=128, gamma=0.99, target_update_interval=1000, encoder_factory=encoder_factory)),\n",
    "        # 'DiscreteBC': (d3rlpy.algos.DiscreteBC, d3rlpy.algos.DiscreteBCConfig(learning_rate=1e-4, batch_size=2048, gamma=0.98, encoder_factory=encoder_factory)),\n",
    "        # 'DiscreteCQL': (d3rlpy.algos.DiscreteCQL, d3rlpy.algos.DiscreteCQLConfig(batch_size=128, gamma=0.99, target_update_interval=10000, encoder_factory=encoder_factory)), # TODO: seems like using larger target_update_interval (default 8000), before target network update the reward getting better\n",
    "        # 'DiscreteCQL': (d3rlpy.algos.DiscreteCQL, d3rlpy.algos.DiscreteCQLConfig(batch_size=1024, learning_rate=1e-5, gamma=0.99, target_update_interval=1000, encoder_factory=encoder_factory, alpha=0.1)), # TODO: gamma = 0.75 is batter than 0.99\n",
    "        'DiscreteCQL': (d3rlpy.algos.DiscreteCQL, d3rlpy.algos.DiscreteCQLConfig(batch_size=1024, learning_rate=1e-5, gamma=0.75, target_update_interval=1000, encoder_factory=encoder_factory, alpha=0.1)), # TODO: this performance is good\n",
    "        # 'DiscreteCQL': (d3rlpy.algos.DiscreteCQL, d3rlpy.algos.DiscreteCQLConfig(batch_size=32, gamma=0.98, target_update_interval=1000, encoder_factory=encoder_factory, alpha=0.7)),\n",
    "        'DiscreteBC': (d3rlpy.algos.DiscreteBC, d3rlpy.algos.DiscreteBCConfig(batch_size=1024, learning_rate=1e-6, gamma=0.99, encoder_factory=encoder_factory, beta=0.1)),\n",
    "        # 'DiscreteBC': (d3rlpy.algos.DiscreteBC, d3rlpy.algos.DiscreteBCConfig(learning_rate=1e-4, batch_size=64, gamma=0.98, encoder_factory=encoder_factory)),\n",
    "        # 'DiscreteBC': (d3rlpy.algos.DiscreteBC, d3rlpy.algos.DiscreteBCConfig(learning_rate=1e-4, batch_size=2048, gamma=0.98)),\n",
    "        # 'NFQ': (d3rlpy.algos.NFQ, d3rlpy.algos.NFQConfig(batch_size=1024, gamma=0.99, encoder_factory=encoder_factory)),\n",
    "        'NFQ': (d3rlpy.algos.NFQ, d3rlpy.algos.NFQConfig(batch_size=1024, learning_rate=1e-6, gamma=0.99, encoder_factory=encoder_factory)),\n",
    "    } \"\"\"\n",
    "\n",
    "    if algo not in algo_map:\n",
    "        raise ValueError(f\"Unknown algorithm: {algo}\")\n",
    "\n",
    "    AlgClass, config = algo_map[algo]\n",
    "\n",
    "    # Instantiate agent\n",
    "    agent = AlgClass(config=config, device=device, enable_ddp=False)\n",
    "    env = PatientEnvironment(train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs)\n",
    "\n",
    "    # Load existing or train new\n",
    "    if not retrain and os.path.exists(model_path):\n",
    "        print(f\"Loading existing model from {model_path}\")\n",
    "        json_path = get_newest_json(algo)\n",
    "        if json_path:\n",
    "            if algo == 'DQN':\n",
    "                agent = d3rlpy.algos.DQN.from_json(json_path)\n",
    "            elif algo == 'DoubleDQN':\n",
    "                agent = d3rlpy.algos.DoubleDQN.from_json(json_path)\n",
    "            elif algo == 'DiscreteSAC':\n",
    "                agent = d3rlpy.algos.DiscreteSAC.from_json(json_path)\n",
    "            elif algo == 'DecisionTransformer':\n",
    "                agent = d3rlpy.algos.DiscreteDecisionTransformer.from_json(json_path)\n",
    "            elif algo == 'DiscreteBCQ':\n",
    "                agent = d3rlpy.algos.DiscreteBCQ.from_json(json_path)\n",
    "            elif algo == 'DiscreteCQL':\n",
    "                agent = d3rlpy.algos.DiscreteCQL.from_json(json_path)\n",
    "            elif algo == 'DiscreteBC':\n",
    "                agent = d3rlpy.algos.DiscreteBC.from_json(json_path)\n",
    "            elif algo == 'NFQ':\n",
    "                agent = d3rlpy.algos.NFQ.from_json(json_path)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown algorithm: {algo}\")\n",
    "            print(f\"Loading model from {json_path}\")\n",
    "            agent.load_model(model_path)\n",
    "    else:\n",
    "        # Train agent\n",
    "        # agent.fit(dataset, n_steps=10, n_steps_per_epoch=5)\n",
    "        # agent.fit(dataset, n_steps=1000000, n_steps_per_epoch=100000)\n",
    "        # agent.fit(dataset, n_steps=50000, n_steps_per_epoch=10000)\n",
    "        # agent.fit(dataset, n_steps=10000, n_steps_per_epoch=1000)\n",
    "        # agent.fit(dataset, n_steps=100000, n_steps_per_epoch=10000)\n",
    "        # n_steps = 10000 # TODO: increase # TODO: [update reward] not sure\n",
    "        \"\"\" n_steps = 20 # TODO: increase # TODO: [update reward] not sure\n",
    "        n_steps_per_epoch = 10 \"\"\"\n",
    "        n_steps = 20000 # TODO: increase # TODO: [update reward] not sure\n",
    "        n_steps_per_epoch = 1000\n",
    "        \n",
    "        cur_best_reward = -np.inf\n",
    "        rewards_summary = []\n",
    "        action_diversity_summary = []\n",
    "        for epoch in range(int(n_steps/n_steps_per_epoch)):\n",
    "            print(f\"Epoch {epoch+1}/{n_steps/n_steps_per_epoch}\")\n",
    "            agent.fit(dataset, n_steps=n_steps_per_epoch, n_steps_per_epoch=n_steps_per_epoch)\n",
    "            # evaluate the agent\n",
    "            action_diversity_set = set()\n",
    "            evaluation_trajectories = []  # List to store all trajectory steps\n",
    "\n",
    "            for category, init_obs_list in eval_init_obs_categories.items():\n",
    "                if not init_obs_list:\n",
    "                    print(f\"Skipping {category}: No valid initial observations\")\n",
    "                    continue\n",
    "\n",
    "                # Full evaluation for reward statistics and trajectories\n",
    "                episode_returns = []\n",
    "                traj_len = []\n",
    "                traj_len_without_nonsense_actions = []\n",
    "                nonsense_actions = []\n",
    "                meet_weaning_list = []\n",
    "                meet_weaning_traj_len_list = []\n",
    "                action_diversity_in_category_set = set()\n",
    "                for idx in range(min(100, len(init_obs_list))): # TODO: set back to 100\n",
    "                # for idx in range(min(100, len(init_obs_list))):\n",
    "                    obs = env.reset(init_obs_list[idx])\n",
    "                    done = False\n",
    "                    total_reward = 0\n",
    "                    trajectory = []  # Trajectory for this episode\n",
    "                    stay_id = f'{epoch}_{category}_{idx}'\n",
    "                    # stay_id = f'eval_{algo}_{data_source}_epoch{epoch}_{category}_{idx}'\n",
    "\n",
    "                    # Extract sizes\n",
    "                    state_size = len(env.state_cols)  # e.g., 3 for heart_rate, resp_rate, spo2\n",
    "                    action_size = len(env.action_cols)  # e.g., 2 for fio2, respiratory_rate_set\n",
    "                    baseline_size = len(env.baseline_cols)  # e.g., 2 for gender_M, age\n",
    "                    obs_hrs = env.obs_hrs  # Assuming env has obs_hrs attribute\n",
    "\n",
    "                    # At t=0, include historical states and current state from initial obs\n",
    "                    for h in range(obs_hrs):  # h=0 for s_1, h=1 for s_2, ..., h=obs_hrs-1 for s_current\n",
    "                        state_start = h * (state_size + action_size)\n",
    "                        state = obs[state_start:state_start + state_size]  # Extract state\n",
    "                        # Action: use a_h for s_h, last historical action for s_current # TODO: this is not correct\n",
    "                        if h < obs_hrs - 1:\n",
    "                            action_start = state_start + state_size\n",
    "                            action = obs[action_start:action_start + action_size]\n",
    "                        else:\n",
    "                            action_idx = agent.predict(np.expand_dims(obs, axis=0))[0]  # Predict action for s_current\n",
    "                            action = env.id_to_action[action_idx]  # Assuming this maps action index to tuple\n",
    "                            # q_value = agent.predict_value(np.expand_dims(obs, axis=0), action_idx) # TODO: for tracking q_value\n",
    "                            # print(f\"action: {action} q_value: {q_value}\")\n",
    "                        # Baseline variables\n",
    "                        baseline_start = len(obs) - baseline_size\n",
    "                        baseline = obs[baseline_start:baseline_start + baseline_size]\n",
    "\n",
    "                        trajectory.append({\n",
    "                            'stay_id': stay_id,\n",
    "                            'hours_in': h,\n",
    "                            **{col: state[i] for i, col in enumerate(env.state_cols)},\n",
    "                            **{col: action[i] for i, col in enumerate(env.action_cols)},\n",
    "                            **{col: baseline[i] for i, col in enumerate(env.baseline_cols)}\n",
    "                        })\n",
    "\n",
    "                    # Continue for remaining steps\n",
    "                    t = obs_hrs\n",
    "                    while not done:\n",
    "                        action_idx = agent.predict(np.expand_dims(obs, axis=0))[0] # TODO: or use \"sample_action\" -> since some mtehod will be not deterministic\n",
    "                        action_tuple = env.id_to_action[action_idx]  # Assuming this maps action index to tuple\n",
    "                        action_diversity_set.add(action_idx)\n",
    "                        action_diversity_in_category_set.add(action_idx)\n",
    "                        next_obs, reward, done, info = env.step(action_idx)\n",
    "                        total_reward += reward\n",
    "\n",
    "                        # Extract current state and baseline from next_obs\n",
    "                        state_start = (obs_hrs - 1) * (state_size + action_size)\n",
    "                        state = next_obs[state_start:state_start + state_size]  # s_current\n",
    "                        baseline_start = len(next_obs) - baseline_size\n",
    "                        baseline = next_obs[baseline_start:baseline_start + baseline_size]\n",
    "\n",
    "                        # Store trajectory\n",
    "                        trajectory.append({\n",
    "                            'stay_id': stay_id,\n",
    "                            'hours_in': t,\n",
    "                            **{col: state[i] for i, col in enumerate(env.state_cols)},\n",
    "                            **{col: action_tuple[i] for i, col in enumerate(env.action_cols)},\n",
    "                            **{col: baseline[i] for i, col in enumerate(env.baseline_cols)}\n",
    "                        })\n",
    "\n",
    "                        obs = next_obs\n",
    "                        t += 1\n",
    "\n",
    "                    episode_returns.append(total_reward)\n",
    "                    evaluation_trajectories.extend(trajectory)  # Add this episode's steps to the full list\n",
    "                    traj_len.append(len(trajectory))\n",
    "                    nonsense_actions.append(info['nonsense_action'])\n",
    "                    if not info['nonsense_action']:\n",
    "                        traj_len_without_nonsense_actions.append(len(trajectory))\n",
    "                    meet_weaning_list.append(info['meet_weaning'])\n",
    "                    if info['meet_weaning']:\n",
    "                        meet_weaning_traj_len_list.append(len(trajectory))\n",
    "\n",
    "                if len(episode_returns) == 0:\n",
    "                    print(f\"Skipping {category}: No valid episode returns\")\n",
    "                    continue\n",
    "\n",
    "                mean_reward = np.mean(episode_returns)\n",
    "                std_reward = np.std(episode_returns)\n",
    "                rewards_summary.append({\n",
    "                    'algo': algo,\n",
    "                    'data_source': data_source,\n",
    "                    'epoch': epoch,\n",
    "                    'category': category,\n",
    "                    'mean_reward': mean_reward,\n",
    "                    'std_reward': std_reward,\n",
    "                    'meet_weaning_percentage': np.mean(meet_weaning_list) if meet_weaning_list else 0,\n",
    "                    'mean_traj_len': np.mean(traj_len),\n",
    "                    'std_traj_len': np.std(traj_len),\n",
    "                    'mean_traj_len_without_nonsense_actions': np.mean(traj_len_without_nonsense_actions) if traj_len_without_nonsense_actions else 0,\n",
    "                    'std_traj_len_without_nonsense_actions': np.std(traj_len_without_nonsense_actions) if traj_len_without_nonsense_actions else 0,\n",
    "                    'mean_traj_len_to_meet_weaning': np.mean(meet_weaning_traj_len_list) if meet_weaning_traj_len_list else 0,\n",
    "                    'std_traj_len_to_meet_weaning': np.std(meet_weaning_traj_len_list) if meet_weaning_traj_len_list else 0,\n",
    "                    'action_diversity': len(action_diversity_in_category_set), # TODO: action_diversity_set will be \"cumulative\" across all categories\n",
    "                    'mean_nonsense_actions': np.mean(nonsense_actions) # not sure mean of boolean\n",
    "                })\n",
    "                print(f\"{algo} {data_source} - {category}: Average Total Reward = {mean_reward}\")\n",
    "                # print(f\"Mean trajectory length: {np.mean(traj_len)}\")\n",
    "                # print(f\"Mean trajectory length without nonsense actions: {np.mean(traj_len_without_nonsense_actions)}\")\n",
    "                # print(f\"Mean nonsense actions: {np.mean(nonsense_actions)}\")\n",
    "\n",
    "                if category == 'small_reward' and mean_reward > cur_best_reward:\n",
    "                    cur_best_reward = mean_reward\n",
    "                    print(f\"Saving model to {model_path}\")\n",
    "                    agent.save_model(model_path)\n",
    "\n",
    "            # Record action diversity for the epoch\n",
    "            action_diversity = len(action_diversity_set)\n",
    "            action_diversity_summary.append({\n",
    "                'algo': algo,\n",
    "                'data_source': data_source,\n",
    "                'epoch': epoch,\n",
    "                'action_diversity': action_diversity\n",
    "            })\n",
    "            print(f\"Action diversity: {action_diversity}\")\n",
    "            print(f\"Actions taken: {action_diversity_set}\")\n",
    "\n",
    "            # Save all evaluation trajectories to a CSV file\n",
    "            df_trajectories = pd.DataFrame(evaluation_trajectories)\n",
    "            df_trajectories.to_csv(f'../models/training_log/{EXP_FOLDER_PREFIX}/eval_trajectories_{algo}_{data_source}_epoch{epoch}.csv', index=False)\n",
    "            print(f\"Saved evaluation trajectories to 'eval_trajectories_{algo}_{data_source}_epoch{epoch}.csv'\")\n",
    "            # Save rewards summary to a CSV file\n",
    "            df_rewards = pd.DataFrame(rewards_summary)\n",
    "            df_rewards.to_csv(f'../models/training_log/{EXP_FOLDER_PREFIX}/rewards_summary_{algo}_{data_source}.csv', index=False)\n",
    "            # Save action diversity summary to a CSV file\n",
    "            df_action_diversity = pd.DataFrame(action_diversity_summary)\n",
    "            # TODO: can merge rewards_summary and action_diversity_summary\n",
    "            df_action_diversity.to_csv(f'../models/training_log/{EXP_FOLDER_PREFIX}/action_diversity_summary_{algo}_{data_source}.csv', index=False)\n",
    "            # save model\n",
    "            # agent.save_model(f\"../models/{algo}_{data_source}_epoch{epoch}.pkl\") # TODO: save if needed\n",
    "            \"\"\" if mean_reward > cur_best_reward: # TODO: only use common reward for finding the best model, since it is the last one to be eval\n",
    "                cur_best_reward = mean_reward\n",
    "                print(f\"Saving model to {model_path}\")\n",
    "                agent.save_model(model_path) \"\"\"\n",
    "        # # Save model\n",
    "        # print(f\"Saving model to {model_path}\")\n",
    "        # agent.save_model(model_path)\n",
    "\n",
    "    return agent, env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprare data set for train_df, test_df, and eICU_disc for train d3rlpy agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_dataset = preprocess_data(train_df, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs)\n",
    "test_df_dataset = preprocess_data(test_df, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs)\n",
    "eICU_disc_dataset = preprocess_data(eICU_disc, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physician Policy (Naive Agent action transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "class NaiveAgent:\n",
    "    \"\"\"\n",
    "    A naive agent that interacts with the PatientEnvironment.\n",
    "    It uses a dictionary to map state-action-baseline keys to a list of possible actions.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, obs_hrs, state_cols, action_cols, baseline_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the naive agent by creating its internal dictionary.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe containing the data.\n",
    "            obs_hrs (int): Number of observation hours to include in the state history.\n",
    "            state_cols (list): List of columns to include in the state.\n",
    "            action_cols (list): List of columns to include in the action.\n",
    "            baseline_cols (list, optional): List of baseline columns (e.g., age, gender) to include. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.naive_agent_dict = self._create_naive_agent_dict(df, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "\n",
    "    def _create_naive_agent_dict(self, df, obs_hrs, state_cols, action_cols, baseline_cols=None):\n",
    "        \"\"\"\n",
    "        Create the naive agent dictionary.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe containing the data.\n",
    "            obs_hrs (int): Number of observation hours to include in the state history.\n",
    "            state_cols (list): List of columns to include in the state.\n",
    "            action_cols (list): List of columns to include in the action.\n",
    "            baseline_cols (list, optional): List of baseline columns (e.g., age, gender) to include. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are tuples of (state, action) history and baseline,\n",
    "                  and values are lists of possible actions.\n",
    "        \"\"\"\n",
    "        naive_agent = defaultdict(list)\n",
    "        \n",
    "        # Group by stay_id\n",
    "        for stay_id, group in df.groupby('stay_id'):\n",
    "            group = group.sort_values('hours_in')\n",
    "            rows = group.to_dict('records')\n",
    "            \n",
    "            # Sliding window with length obs_hrs+1 to get (state/action history, current_action)\n",
    "            for i in range(len(rows) - obs_hrs):\n",
    "                window = rows[i:i+obs_hrs+1]\n",
    "                \n",
    "                # Build state_with_pre_state_action_baseline: for each of the first obs_hrs rows\n",
    "                key = []\n",
    "                for j in range(obs_hrs):\n",
    "                    state = tuple(window[j][col] for col in state_cols)  # Extract state variables\n",
    "                    if j < obs_hrs - 1:\n",
    "                        action = tuple(window[j][col] for col in action_cols)  # Extract action variables\n",
    "                        # key.append((state, action))\n",
    "                        key.append(state)\n",
    "                        key.append(action)\n",
    "                    else:\n",
    "                        # key.append((state))\n",
    "                        key.append(state)\n",
    "                \n",
    "                # Add baseline variables (if provided) to the key\n",
    "                if baseline_cols:\n",
    "                    baseline = tuple(window[0][col] for col in baseline_cols)  # Use the first row for baseline variables\n",
    "                    key.append(baseline)\n",
    "\n",
    "                # The current action for the last row\n",
    "                curr_action = tuple(window[obs_hrs][col] for col in action_cols)\n",
    "                \n",
    "                # Store action: key -> action_list\n",
    "                # naive_agent[tuple(key)].append(curr_action)\n",
    "                naive_agent[tuple(itertools.chain.from_iterable(key))].append(curr_action)\n",
    "                \n",
    "                \n",
    "        \n",
    "        return naive_agent\n",
    "\n",
    "    def sample_action(self, state_with_pre_state_action_baseline, last_action):\n",
    "        \"\"\"\n",
    "        Sample an action from the naive agent dictionary for a given key.\n",
    "        If the key does not exist, return the last action.\n",
    "\n",
    "        Args:\n",
    "            state_with_pre_state_action_baseline (tuple): The state-action-baseline key.\n",
    "            last_action (tuple): The last action to return if the key does not exist.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A sampled action or the last action if the key does not exist.\n",
    "        \"\"\"\n",
    "        if tuple(state_with_pre_state_action_baseline) in self.naive_agent_dict:\n",
    "            return random.choice(self.naive_agent_dict[state_with_pre_state_action_baseline])\n",
    "        return last_action\n",
    "    \n",
    "    def predict(self, state_with_pre_state_action_baseline):\n",
    "        action_bins = [num_bins_dict[col] for col in action_cols]\n",
    "        action_to_id = {a: i for i, a in enumerate(itertools.product(*[range(b) for b in action_bins]))}\n",
    "        \n",
    "        state_with_pre_state_action_baseline = tuple(state_with_pre_state_action_baseline[0])\n",
    "        if tuple(state_with_pre_state_action_baseline) in self.naive_agent_dict:\n",
    "            return [action_to_id[random.choice(self.naive_agent_dict[state_with_pre_state_action_baseline])]]\n",
    "        else:\n",
    "            state_len = len(state_cols)\n",
    "            action_len = len(action_cols)\n",
    "            baseline_len = len(baseline_cols)\n",
    "\n",
    "            # Split the data into states, actions, and baselines\n",
    "            num_states = (len(state_with_pre_state_action_baseline) - baseline_len) // (state_len + action_len)\n",
    "            states = []\n",
    "            actions = []\n",
    "            for i in range(num_states):\n",
    "                start_idx = i * (state_len + action_len)\n",
    "                states.append(tuple(state_with_pre_state_action_baseline[start_idx:start_idx + state_len]))\n",
    "                actions.append(tuple(state_with_pre_state_action_baseline[start_idx + state_len:start_idx + state_len + action_len]))\n",
    "            # last state is the current state\n",
    "            current_state = tuple(state_with_pre_state_action_baseline[-(state_len + baseline_len):-baseline_len])\n",
    "            states.append(current_state)\n",
    "\n",
    "            # Create the final format\n",
    "            result = []\n",
    "            for i in range(num_states + 1):\n",
    "                # Use the last action if the current action is missing\n",
    "                if i < len(actions):\n",
    "                    action = actions[i]\n",
    "                else:\n",
    "                    last_action = actions[-1]\n",
    "                    # Convert np.int64 (or any NumPy integer) to plain Python integers\n",
    "                    last_action = tuple(int(x) if isinstance(x, np.integer) else x for x in last_action)\n",
    "                    # print(\"last_action\")\n",
    "                    return [action_to_id[last_action]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_agent = NaiveAgent(train_df, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "# naive_agent = NaiveAgent(test_df, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "# naive_agent = NaiveAgent(eICU_disc, obs_hrs, state_cols, action_cols, baseline_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_agent.naive_agent_dict[tuple([5, 7, 5, 5, 6, 5, 7, 5, 5, 6, 5, 7, 5, 0, 7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_agent.predict(tuple(tuple([5, 7, 5, 5, 6, 5, 7, 5, 5, 6, 5, 7, 5, 0, 7])))\n",
    "# naive_agent.predict(np.expand_dims([5, 7, 5, 5, 6, 5, 7, 5, 5, 6, 5, 7, 5, 0, 7], axis=0))\n",
    "env = PatientEnvironment(train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs)\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "naive_agent.predict(np.expand_dims(obs, axis=0))\n",
    "# naive_agent.predict(tuple(env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D3rlpy Offline Policy Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_bc_train, env = train_and_evaluate(train_df, train_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteBC', data_source='train', retrain=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_bc_train, env = train_and_evaluate(train_df, train_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteBC', data_source='train', retrain=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_bc_train, env = train_and_evaluate(train_df, train_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteBC', data_source='train')\n",
    "agent_bc_test, env = train_and_evaluate(test_df, train_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteBC', data_source='test')\n",
    "agent_bc_eICU, env = train_and_evaluate(eICU_disc, train_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteBC', data_source='eICU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_nfq_train, env = train_and_evaluate(train_df, train_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='NFQ', data_source='train')\n",
    "agent_nfq_test, env = train_and_evaluate(test_df, test_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='NFQ', data_source='test')\n",
    "agent_nfq_eICU, env = train_and_evaluate(eICU_disc, eICU_disc_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='NFQ', data_source='eICU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dqn_train, env = train_and_evaluate(train_df, train_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DQN', data_source='train')\n",
    "agent_dqn_test, env = train_and_evaluate(test_df, test_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DQN', data_source='test')\n",
    "agent_dqn_eICU, env = train_and_evaluate(eICU_disc, eICU_disc_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DQN', data_source='eICU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_ddqn_train, env = train_and_evaluate(train_df, train_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DoubleDQN', data_source='train')\n",
    "agent_ddqn_test, env = train_and_evaluate(test_df, test_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DoubleDQN', data_source='test')\n",
    "agent_ddqn_eICU, env = train_and_evaluate(eICU_disc, eICU_disc_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DoubleDQN', data_source='eICU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DiscreteSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_sac_train, env = train_and_evaluate(train_df, train_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteSAC', data_source='train')\n",
    "agent_sac_test, env = train_and_evaluate(test_df, test_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteSAC', data_source='test')\n",
    "agent_sac_eICU, env = train_and_evaluate(eICU_disc, eICU_disc_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteSAC', data_source='eICU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_bcq_train, env = train_and_evaluate(train_df, train_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteBCQ', data_source='train')\n",
    "agent_bcq_test, env = train_and_evaluate(test_df, test_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteBCQ', data_source='test')\n",
    "agent_bcq_eICU, env = train_and_evaluate(eICU_disc, eICU_disc_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteBCQ', data_source='eICU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_cql_train, env = train_and_evaluate(train_df, train_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteCQL', data_source='train')\n",
    "agent_cql_test, env = train_and_evaluate(test_df, test_df_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteCQL', data_source='test')\n",
    "agent_cql_eICU, env = train_and_evaluate(eICU_disc, eICU_disc_dataset, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, algo='DiscreteCQL', data_source='eICU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Policy Evaluation (FQE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import d3rlpy\n",
    "from d3rlpy.algos import DQNConfig, DoubleDQNConfig, DiscreteSACConfig, DiscreteBCQConfig, DiscreteCQLConfig\n",
    "from d3rlpy.ope import DiscreteFQE\n",
    "\n",
    "def load_agent(algo, data_source, dataset):\n",
    "    \"\"\"\n",
    "    Load a trained agent from a saved model file.\n",
    "    \n",
    "    Args:\n",
    "        algo (str): Algorithm name (e.g., 'DQN', 'DoubleDQN').\n",
    "        data_source (str): Data source the agent was trained on ('train', 'test', 'eICU').\n",
    "        dataset (MDPDataset): Dataset to build the agent's model structure.\n",
    "    \n",
    "    Returns:\n",
    "        Agent object loaded with the saved model.\n",
    "    \"\"\"\n",
    "    model_path = f\"../models/{algo}_{data_source}.pkl\"\n",
    "    \n",
    "    # Define configurations matching those in train_and_evaluate\n",
    "    algo_configs = {\n",
    "        'DQN': DQNConfig(learning_rate=1e-3, gamma=0.99, target_update_interval=1000, batch_size=32),\n",
    "        'DoubleDQN': DoubleDQNConfig(learning_rate=1e-3, gamma=0.99, target_update_interval=1000, batch_size=32),\n",
    "        'DiscreteSAC': DiscreteSACConfig(actor_learning_rate=3e-4, critic_learning_rate=3e-4, batch_size=32, gamma=0.99),\n",
    "        'DiscreteBCQ': DiscreteBCQConfig(batch_size=32, gamma=0.99),\n",
    "        'DiscreteBC': DiscreteBCConfig(batch_size=32, learning_rate=1e-3),\n",
    "        'DiscreteCQL': DiscreteCQLConfig(batch_size=32, gamma=0.99)\n",
    "    }\n",
    "    \n",
    "    if algo not in algo_configs:\n",
    "        raise ValueError(f\"Unknown algorithm: {algo}\")\n",
    "    \n",
    "    config = algo_configs[algo]\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Instantiate the agent\n",
    "    if algo == 'DQN':\n",
    "        agent = d3rlpy.algos.DQN(config=config, device=device, enable_ddp=False)\n",
    "    elif algo == 'DoubleDQN':\n",
    "        agent = d3rlpy.algos.DoubleDQN(config=config, device=device, enable_ddp=False)\n",
    "    elif algo == 'DiscreteSAC':\n",
    "        agent = d3rlpy.algos.DiscreteSAC(config=config, device=device, enable_ddp=False)\n",
    "    elif algo == 'DiscreteBCQ':\n",
    "        agent = d3rlpy.algos.DiscreteBCQ(config=config, device=device, enable_ddp=False)\n",
    "    elif algo == 'DiscreteCQL':\n",
    "        agent = d3rlpy.algos.DiscreteCQL(config=config, device=device, enable_ddp=False)\n",
    "    elif algo == 'DiscreteBC':\n",
    "        agent = d3rlpy.algos.DiscreteBC(config=config, device=device, enable_ddp=False)\n",
    "    \n",
    "    # Build with dataset to set observation and action spaces\n",
    "    agent.build_with_dataset(dataset)\n",
    "    \n",
    "    # Load the saved model\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "    agent.load_model(model_path)\n",
    "    print(f\"Loaded {algo} agent trained on {data_source} from {model_path}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "def get_terminals_from_dataset(dataset):\n",
    "    terminals = []\n",
    "    for episode in dataset.episodes:\n",
    "        for transition in episode.transitions:\n",
    "            # Assume transition is a Transition object with a terminal attribute\n",
    "            terminals.append(getattr(transition, 'terminal', False))\n",
    "    return np.array(terminals, dtype=bool)\n",
    "\n",
    "# def evaluate_with_fqe(agent, dataset, fqe_config, n_steps=10000):\n",
    "#     \"\"\"\n",
    "#     Evaluate an agent on a dataset using DiscreteFQE.\n",
    "    \n",
    "#     Args:\n",
    "#         agent: Trained agent with a predict method.\n",
    "#         dataset (MDPDataset): Dataset to evaluate on.\n",
    "#         fqe_config: Configuration for the FQE Q-function.\n",
    "#         n_steps (int): Number of training steps for FQE.\n",
    "    \n",
    "#     Returns:\n",
    "#         float: Estimated policy value.\n",
    "#     \"\"\"\n",
    "#     # Initialize FQE\n",
    "#     fqe = DiscreteFQE(algo=agent, config=fqe_config)\n",
    "    \n",
    "#     # Fit FQE on the dataset\n",
    "#     fqe.fit(dataset, n_steps=n_steps)\n",
    "    \n",
    "#     # Identify initial observations (start of episodes)\n",
    "#     initial_indices = np.where(dataset.terminals[:-1] == 1)[0] + 1\n",
    "#     if len(initial_indices) == 0:\n",
    "#         initial_indices = [0]\n",
    "#     else:\n",
    "#         initial_indices = np.concatenate(([0], initial_indices))\n",
    "#     initial_obs = dataset.observations[initial_indices]\n",
    "    \n",
    "#     # Get actions chosen by the agent's policy\n",
    "#     actions = agent.predict(initial_obs)\n",
    "    \n",
    "#     # Predict Q-values and select those for the chosen actions\n",
    "#     q_values = fqe.predict(initial_obs)  # Shape: (n_samples, n_actions)\n",
    "#     selected_q_values = q_values[np.arange(len(actions)), actions]\n",
    "    \n",
    "#     # Compute average Q-value as the estimated policy value\n",
    "#     estimated_value = selected_q_values.mean()\n",
    "    \n",
    "#     return estimated_value\n",
    "def evaluate_with_fqe(agent, dataset, eval_init_obs_categories, fqe_config, n_steps=10000):\n",
    "    \"\"\"\n",
    "    Evaluate an agent on a dataset using DiscreteFQE.\n",
    "    \n",
    "    Args:\n",
    "        agent: Trained agent with a predict method.\n",
    "        dataset (MDPDataset): Dataset to evaluate on.\n",
    "        episode_starts (list): List of indices indicating the start of each episode.\n",
    "        fqe_config: Configuration for the FQE Q-function.\n",
    "        n_steps (int): Number of training steps for FQE.\n",
    "    \n",
    "    Returns:\n",
    "        float: Estimated policy value.\n",
    "    \"\"\"\n",
    "    # Initialize FQE\n",
    "    fqe = DiscreteFQE(algo=agent, config=fqe_config)\n",
    "    \n",
    "    # Fit FQE on the dataset\n",
    "    fqe.fit(dataset, n_steps=n_steps)\n",
    "    \n",
    "    # obs = env.reset()\n",
    "    # q_value = fqe.predict(obs)\n",
    "    # # Identify initial observations using episode_starts\n",
    "    # initial_obs = dataset.observations[episode_starts]\n",
    "    \n",
    "    # # Get actions chosen by the agent's policy\n",
    "    # actions = agent.predict(initial_obs)\n",
    "    \n",
    "    # # Predict Q-values and select those for the chosen actions\n",
    "    # q_values = fqe.predict(initial_obs)  # Shape: (n_samples, n_actions)\n",
    "    # selected_q_values = q_values[np.arange(len(actions)), actions]\n",
    "    \n",
    "    # # Compute average Q-value as the estimated policy value\n",
    "    # estimated_value = selected_q_values.mean()\n",
    "    \n",
    "    return fqe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets (assumed to be predefined MDPDataset objects)\n",
    "# datasets = {\n",
    "#     'train': (train_df_dataset, train_episode_starts),\n",
    "#     'test': (test_df_dataset, test_episode_starts),\n",
    "#     'eICU': (eICU_disc_dataset, eICU_episode_starts)\n",
    "# }\n",
    "datasets = {\n",
    "    'train': train_df_dataset,\n",
    "    'test': test_df_dataset,\n",
    "    'eICU': eICU_disc_dataset\n",
    "}\n",
    "\n",
    "# FQE Configuration\n",
    "fqe_config = DQNConfig(\n",
    "    learning_rate=1e-3,\n",
    "    gamma=0.99,\n",
    "    target_update_interval=1000,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Algorithms and data sources\n",
    "algorithms = ['DQN', 'DoubleDQN', 'DiscreteSAC', 'DiscreteBCQ', 'DiscreteCQL', 'DiscreteBC']\n",
    "data_sources = ['train', 'test', 'eICU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "\n",
    "for algo in algorithms:\n",
    "    for train_data_source in data_sources:\n",
    "        # Load the agent trained on train_data_source\n",
    "        try:\n",
    "            agent = load_agent(algo, train_data_source, train_df_dataset) # TODO: train_df_dataset -> it's only for building the model structure\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "        # Evaluate on each dataset\n",
    "        for eval_data_source in data_sources:\n",
    "            dataset = datasets[eval_data_source]\n",
    "            # dataset, episode_starts = datasets[eval_data_source]\n",
    "            # estimated_value = evaluate_with_fqe(agent, dataset, episode_starts, fqe_config, n_steps=10000)\n",
    "            # fqe_agent = evaluate_with_fqe(agent, dataset, episode_starts, fqe_config, n_steps=10000)\n",
    "            fqe_agent = DiscreteFQE(algo=agent, config=fqe_config)\n",
    "            # fqe_agent.fit(dataset, n_steps=100, n_steps_per_epoch=10)\n",
    "            fqe_agent.fit(dataset, n_steps=10000, n_steps_per_epoch=1000)\n",
    "            # estimated_value = fqe_agent.predict(env.reset())\n",
    "            print(f\"Estimated policy value for {eval_data_source}:\")\n",
    "            q_value_dict = {}\n",
    "            action_diversity = set()\n",
    "            for category, init_obs_list in eval_init_obs_categories.items():\n",
    "                q_value_list = []\n",
    "                for obs in init_obs_list:\n",
    "                    action = fqe_agent.predict(np.expand_dims(obs, axis=0))\n",
    "                    action_diversity.add(tuple(action))\n",
    "                    q_value = fqe_agent.predict_value(np.expand_dims(obs, axis=0), action)\n",
    "                    q_value_list.append(q_value)\n",
    "                q_value_mean = np.mean(q_value_list)\n",
    "                print(f\"mean Q of {category}: {q_value_mean}\")\n",
    "                q_value_dict[category] = q_value_mean\n",
    "            # print(f\"Estimated policy value for {eval_data_source}: {estimated_value}\")\n",
    "            results.append({\n",
    "                'algo': algo,\n",
    "                'train_data_source': train_data_source,\n",
    "                'eval_data_source': eval_data_source,\n",
    "                'q_value_dict': q_value_dict,\n",
    "                'action_diversity': len(action_diversity)\n",
    "            })\n",
    "            print(f\"{algo} trained on {train_data_source}, evaluated on {eval_data_source}, action diversity {len(action_diversity)}: {q_value_dict}\")\n",
    "            # save fqe model\n",
    "            fqe_agent.save_model(f\"../models/fqe_{algo}_{train_data_source}_{eval_data_source}.pkl\")\n",
    "            print(f\"FQE model saved to '../models/fqe_{algo}_{train_data_source}_{eval_data_source}.pkl'\")\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('../models/fqe_evaluation_results.csv', index=False)\n",
    "print(\"Results saved to 'fqe_evaluation_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize fqe results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqe_evaluation_results_df = pd.read_csv('../models/fqe_evaluation_results.csv')\n",
    "fqe_evaluation_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqe_evaluation_results_df[\"q_value_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqe_evaluation_results_df = pd.read_csv('../models/fqe_evaluation_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "fqe_evaluation_results_df = pd.read_csv('../models/fqe_evaluation_results.csv')\n",
    "\n",
    "# Convert 'q_value_dict' from string to dictionary\n",
    "fqe_evaluation_results_df['q_value_dict'] = fqe_evaluation_results_df['q_value_dict'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Define the reward categories\n",
    "reward_categories = ['small_reward', 'median_reward', 'large_reward', 'long_to_wean', 'median_to_wean', 'short_to_wean', 'common']\n",
    "\n",
    "# Aggregate Q-values by algorithm and evaluation data source\n",
    "def aggregate_q_values(df):\n",
    "    aggregated_data = {}\n",
    "    for algo in df['algo'].unique():\n",
    "        for eval_data in df['eval_data_source'].unique():\n",
    "            subset = df[(df['algo'] == algo) & (df['eval_data_source'] == eval_data)]\n",
    "            q_values = {reward: np.mean([entry[reward] for entry in subset['q_value_dict']]) for reward in reward_categories}\n",
    "            aggregated_data[(algo, eval_data)] = q_values\n",
    "    return aggregated_data\n",
    "\n",
    "aggregated_q_values = aggregate_q_values(fqe_evaluation_results_df)\n",
    "\n",
    "# Convert to DataFrame for heatmap plotting\n",
    "heatmap_data = pd.DataFrame(aggregated_q_values).T\n",
    "heatmap_data.index = pd.MultiIndex.from_tuples(heatmap_data.index, names=['Algorithm', 'Eval Data'])\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(heatmap_data, cmap='coolwarm', annot=True, fmt='.2f', cbar=True)\n",
    "plt.title('Q-Value Heatmap by Algorithm and Eval Data Source')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1) Load the CSV\n",
    "\n",
    "# 2) Parse the q_value_dict column into actual dicts\n",
    "def parse_q_dict(s):\n",
    "    # safely evaluate the string representation of the dict\n",
    "    return {k: float(v) for k, v in ast.literal_eval(s).items()}\n",
    "\n",
    "fqe_evaluation_results_df['q_dict'] = fqe_evaluation_results_df['q_value_dict'].apply(parse_q_dict)\n",
    "\n",
    "# 3) Unpack into long form\n",
    "rows = []\n",
    "for _, row in fqe_evaluation_results_df.iterrows():\n",
    "    algo = row['algo']\n",
    "    train_src = row['train_data_source']\n",
    "    eval_src = row['eval_data_source']\n",
    "    diversity = float(row['action_diversity'])\n",
    "    qdict = row['q_dict']\n",
    "    for metric_name, value in qdict.items():\n",
    "        rows.append({\n",
    "            'Algo': algo,\n",
    "            'Train Source': train_src,\n",
    "            'Eval Source': eval_src,\n",
    "            'Metric': metric_name,\n",
    "            'Value': value\n",
    "        })\n",
    "    # add action_diversity as its own metric\n",
    "    rows.append({\n",
    "        'Algo': algo,\n",
    "        'Train Source': train_src,\n",
    "        'Eval Source': eval_src,\n",
    "        'Metric': 'action_diversity',\n",
    "        'Value': diversity\n",
    "    })\n",
    "\n",
    "long_fqe_evaluation_results_df = pd.DataFrame(rows)\n",
    "\n",
    "# 4) Decide which metrics to plot and their order\n",
    "metrics_to_plot = [\n",
    "    'small_reward', 'median_reward', 'large_reward',\n",
    "    'short_to_wean', 'median_to_wean', 'long_to_wean',\n",
    "    'action_diversity'\n",
    "]\n",
    "\n",
    "# 5) Plot one heatmap per metric, comparing Train vs Eval for each algorithm\n",
    "for algo in long_fqe_evaluation_results_df['Algo'].unique():\n",
    "    algo_fqe_evaluation_results_df = long_fqe_evaluation_results_df[long_fqe_evaluation_results_df['Algo'] == algo]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(12, 4 * len(metrics_to_plot)))\n",
    "    fig.suptitle(f'{algo}: FQE Evaluation Metrics\\n', fontsize=18)\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        sub = algo_fqe_evaluation_results_df[algo_fqe_evaluation_results_df['Metric'] == metric]\n",
    "        pivot = sub.pivot_table(\n",
    "            index='Train Source',\n",
    "            columns='Eval Source',\n",
    "            values='Value'\n",
    "        ).reindex(index=['train', 'test', 'eICU'], columns=['train', 'test', 'eICU'])\n",
    "        \n",
    "        # pick colormap\n",
    "        if 'reward' in metric:\n",
    "            cmap = 'RdYlGn'\n",
    "        elif 'to_wean' in metric:\n",
    "            cmap = 'RdYlGn_r'\n",
    "        else:\n",
    "            cmap = 'viridis'\n",
    "        \n",
    "        ax = axes[i]\n",
    "        sns.heatmap(pivot, annot=True, fmt=\".2f\", cmap=cmap,\n",
    "                    linewidths=0.5, cbar=i == 0, ax=ax)\n",
    "        ax.set_title(metric, fontsize=14)\n",
    "        ax.set_ylabel('Train on')\n",
    "        ax.set_xlabel('Eval on')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PatientEnvironment(train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_init_obs_categories[\"small_reward\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_list = []\n",
    "for category, init_obs_list in eval_init_obs_categories.items():\n",
    "    for obs in init_obs_list:\n",
    "        action = fqe_agent.predict(np.expand_dims(obs, axis=0))\n",
    "        q_value = fqe_agent.predict_value(np.expand_dims(obs, axis=0), action)\n",
    "        q_value_list.append(q_value)\n",
    "    print(f\"mean Q of {category}: {np.mean(q_value_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_obs = env.reset()\n",
    "fix_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_obs = eval_init_obs_categories[\"small_reward\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = fqe_agent.predict(np.expand_dims(obs, axis=0))\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = fqe_agent.predict(np.expand_dims(env.reset(), axis=0))\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqe_agent.predict_value(np.expand_dims(obs, axis=0), action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"obs: {fix_obs}\")\n",
    "for i in range(30):\n",
    "    action = fqe_agent.predict(np.expand_dims(env.reset(), axis=0))\n",
    "    value = fqe_agent.predict_value(np.expand_dims(fix_obs, axis=0), action)\n",
    "    print(f\"action: {env.id_to_action[action[0]]}, q_value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"obs: {fix_obs}\")\n",
    "for i in range(5):\n",
    "    action = fqe_agent.sample_action(np.expand_dims(fix_obs, axis=0))\n",
    "    # action = fqe_agent.predict(np.expand_dims(fix_obs, axis=0))\n",
    "    value = fqe_agent.predict_value(np.expand_dims(fix_obs, axis=0), action)\n",
    "    print(f\"action: {env.id_to_action[action[0]]}, q_value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"obs: {fix_obs}\")\n",
    "for category, init_obs_list in eval_init_obs_categories.items():\n",
    "    for i in range(5):\n",
    "        print(f\"obs: {init_obs_list[i]}\")\n",
    "        action = fqe_agent.predict(np.expand_dims(init_obs_list[i], axis=0))\n",
    "        # action = fqe_agent.predict(np.expand_dims(fix_obs, axis=0))\n",
    "        value = fqe_agent.predict_value(np.expand_dims(fix_obs, axis=0), action)\n",
    "        print(f\"category: {category} action: {env.id_to_action[action[0]]}, q_value: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Agent (action transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_agent = NaiveAgent(train_df, obs_hrs, state_cols, action_cols, baseline_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_agent.naive_agent_dict[tuple([5, 7, 5, 5, 6, 5, 7, 5, 5, 6, 5, 7, 5, 0, 7])]\n",
    "# naive_agent.naive_agent_dict[tuple(env.reset())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_agent.predict(tuple(tuple([5, 7, 5, 5, 6, 5, 7, 5, 5, 6, 5, 7, 5, 0, 7])))\n",
    "# naive_agent.predict(np.expand_dims([5, 7, 5, 5, 6, 5, 7, 5, 5, 6, 5, 7, 5, 0, 7], axis=0))\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "naive_agent.predict(np.expand_dims(obs, axis=0))\n",
    "# naive_agent.predict(tuple(env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Naive Agent on Patient Environment with Random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_agent_evaluate_with_random(df, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs=3, algo='DQN', data_source='test', retrain=False):\n",
    "    \"\"\"\n",
    "    Train or load a D3RLPY agent, then evaluate it.\n",
    "    \"\"\"\n",
    "    agent = NaiveAgent(df, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "    env = PatientEnvironment(train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs)\n",
    "    cur_best_reward = -np.inf\n",
    "    rewards_summary = []\n",
    "    action_diversity_summary = []\n",
    "    action_diversity_set = set()\n",
    "    evaluation_trajectories = []  # List to store all trajectory steps\n",
    "\n",
    "    # for category, init_obs_list in eval_init_obs_categories.items():\n",
    "    #     if not init_obs_list:\n",
    "    #         print(f\"Skipping {category}: No valid initial observations\")\n",
    "    #         continue\n",
    "\n",
    "    # Full evaluation for reward statistics and trajectories\n",
    "    episode_returns = []\n",
    "    # for idx in range(min(100, len(init_obs_list))):\n",
    "    for idx in range(int(df[\"stay_id\"].nunique())):\n",
    "        # obs = env.reset(init_obs_list[idx])\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        trajectory = []  # Trajectory for this episode\n",
    "        stay_id = f'{category}_{idx}'\n",
    "        # stay_id = f'eval_{algo}_{data_source}_epoch{epoch}_{category}_{idx}'\n",
    "\n",
    "        # Extract sizes\n",
    "        state_size = len(env.state_cols)  # e.g., 3 for heart_rate, resp_rate, spo2\n",
    "        action_size = len(env.action_cols)  # e.g., 2 for fio2, respiratory_rate_set\n",
    "        baseline_size = len(env.baseline_cols)  # e.g., 2 for gender_M, age\n",
    "        obs_hrs = env.obs_hrs  # Assuming env has obs_hrs attribute\n",
    "\n",
    "        # At t=0, include historical states and current state from initial obs\n",
    "        for h in range(obs_hrs):  # h=0 for s_1, h=1 for s_2, ..., h=obs_hrs-1 for s_current\n",
    "            state_start = h * (state_size + action_size)\n",
    "            state = obs[state_start:state_start + state_size]  # Extract state\n",
    "            # Action: use a_h for s_h, last historical action for s_current\n",
    "            if h < obs_hrs - 1:\n",
    "                action_start = state_start + state_size\n",
    "                action = obs[action_start:action_start + action_size]\n",
    "            else:\n",
    "                # For s_current, use the last historical action (a_(obs_hrs-1))\n",
    "                last_action_start = (obs_hrs - 1) * (state_size + action_size) - action_size\n",
    "                action = obs[last_action_start:last_action_start + action_size]\n",
    "            # Baseline variables\n",
    "            baseline_start = len(obs) - baseline_size\n",
    "            baseline = obs[baseline_start:baseline_start + baseline_size]\n",
    "\n",
    "            trajectory.append({\n",
    "                'stay_id': stay_id,\n",
    "                'hours_in': h,\n",
    "                **{col: state[i] for i, col in enumerate(env.state_cols)},\n",
    "                **{col: action[i] for i, col in enumerate(env.action_cols)},\n",
    "                **{col: baseline[i] for i, col in enumerate(env.baseline_cols)}\n",
    "            })\n",
    "\n",
    "        # Continue for remaining steps\n",
    "        t = obs_hrs\n",
    "        while not done:\n",
    "            action_idx = agent.predict(np.expand_dims(obs, axis=0))[0]\n",
    "            action_tuple = env.id_to_action[action_idx]  # Assuming this maps action index to tuple\n",
    "            action_diversity_set.add(action_idx)\n",
    "            next_obs, reward, done, _ = env.step(action_idx)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Extract current state and baseline from next_obs\n",
    "            state_start = (obs_hrs - 1) * (state_size + action_size)\n",
    "            state = next_obs[state_start:state_start + state_size]  # s_current\n",
    "            baseline_start = len(next_obs) - baseline_size\n",
    "            baseline = next_obs[baseline_start:baseline_start + baseline_size]\n",
    "\n",
    "            # Store trajectory\n",
    "            trajectory.append({\n",
    "                'stay_id': stay_id,\n",
    "                'hours_in': t,\n",
    "                **{col: state[i] for i, col in enumerate(env.state_cols)},\n",
    "                **{col: action_tuple[i] for i, col in enumerate(env.action_cols)},\n",
    "                **{col: baseline[i] for i, col in enumerate(env.baseline_cols)}\n",
    "            })\n",
    "\n",
    "            obs = next_obs\n",
    "            t += 1\n",
    "\n",
    "        episode_returns.append(total_reward)\n",
    "        evaluation_trajectories.extend(trajectory)  # Add this episode's steps to the full list\n",
    "\n",
    "    # if len(episode_returns) == 0:\n",
    "    #     print(f\"Skipping {category}: No valid episode returns\")\n",
    "    #     continue\n",
    "\n",
    "    mean_reward = np.mean(episode_returns)\n",
    "    std_reward = np.std(episode_returns)\n",
    "    rewards_summary.append({\n",
    "        'algo': algo,\n",
    "        'data_source': data_source,\n",
    "        'category': category,\n",
    "        'mean_reward': mean_reward,\n",
    "        'std_reward': std_reward\n",
    "    })\n",
    "    # rewards_summary.append({  # TODO: save more info of navie agent, also can add action diversity\n",
    "    #     'algo': algo,\n",
    "    #     'data_source': data_source,\n",
    "    #     'category': category,\n",
    "    #     'mean_reward': mean_reward,\n",
    "    #     'std_reward': std_reward,\n",
    "    #     'mean_traj_len': np.mean(traj_len),\n",
    "    #     'std_traj_len': np.std(traj_len),\n",
    "    #     'mean_traj_len_without_nonsense_actions': np.mean(traj_len_without_nonsense_actions) if traj_len_without_nonsense_actions else 0,\n",
    "    #     'std_traj_len_without_nonsense_actions': np.std(traj_len_without_nonsense_actions) if traj_len_without_nonsense_actions else 0,\n",
    "    #     'mean_nonsense_actions': np.mean(nonsense_actions) # not sure mean of boolean\n",
    "    # })\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"{algo} {data_source} - {category}: Average Total Reward = {mean_reward}\")\n",
    "\n",
    "    # Record action diversity for the epoch\n",
    "    action_diversity = len(action_diversity_set)\n",
    "    action_diversity_summary.append({\n",
    "        'algo': algo,\n",
    "        'data_source': data_source,\n",
    "        'action_diversity': action_diversity\n",
    "    })\n",
    "    print(f\"Action diversity: {action_diversity}\")\n",
    "    print(f\"Actions taken: {action_diversity_set}\")\n",
    "\n",
    "    # Save all evaluation trajectories to a CSV file\n",
    "    df_trajectories = pd.DataFrame(evaluation_trajectories)\n",
    "    # df_trajectories.to_csv(f'../models/training_log/eval_trajectories_{algo}_{data_source}.csv', index=False)\n",
    "    print(f\"Saved evaluation trajectories to 'eval_trajectories_{algo}_{data_source}.csv'\")\n",
    "    # Save rewards summary to a CSV file\n",
    "    df_rewards = pd.DataFrame(rewards_summary)\n",
    "    # df_rewards.to_csv(f'../models/training_log/rewards_summary_{algo}_{data_source}.csv', index=False)\n",
    "    # Save action diversity summary to a CSV file\n",
    "    df_action_diversity = pd.DataFrame(action_diversity_summary)\n",
    "    # df_action_diversity.to_csv(f'../models/training_log/action_diversity_summary_{algo}_{data_source}.csv', index=False)\n",
    "\n",
    "    return df_trajectories, df_rewards, df_action_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_agent_gen_trajectories, naive_agent_gen_reward, naive_agent_gen_action_diversity = naive_agent_evaluate_with_random(train_df, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs=3, algo='naive_agent', data_source='train', retrain=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_agent_gen_trajectories[naive_agent_gen_trajectories[\"hours_in\"] == 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1118/7443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Naive Agent on Patient Environment with different severity categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_agent_evaluate(df, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs=3, algo='DQN', data_source='test', retrain=False):\n",
    "    agent = NaiveAgent(df, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "    env = PatientEnvironment(train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs)\n",
    "    cur_best_reward = -np.inf\n",
    "    rewards_summary = []\n",
    "    action_diversity_summary = []\n",
    "    action_diversity_set = set()\n",
    "    evaluation_trajectories = []  # List to store all trajectory steps\n",
    "\n",
    "    for category, init_obs_list in eval_init_obs_categories.items():\n",
    "        if not init_obs_list:\n",
    "            print(f\"Skipping {category}: No valid initial observations\")\n",
    "            continue\n",
    "\n",
    "        # Full evaluation for reward statistics and trajectories\n",
    "        episode_returns = []\n",
    "        meet_weaning_list = []\n",
    "        steps_list = []\n",
    "        steps_to_weaning_list = []\n",
    "        nonsense_action_list = []\n",
    "        action_diversity_category_set = set()\n",
    "        \n",
    "        for idx in range(min(100, len(init_obs_list))):\n",
    "            obs = env.reset(init_obs_list[idx])\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            trajectory = []  # Trajectory for this episode\n",
    "            stay_id = f'{category}_{idx}'\n",
    "            # stay_id = f'eval_{algo}_{data_source}_epoch{epoch}_{category}_{idx}'\n",
    "\n",
    "            # Extract sizes\n",
    "            state_size = len(env.state_cols)  # e.g., 3 for heart_rate, resp_rate, spo2\n",
    "            action_size = len(env.action_cols)  # e.g., 2 for fio2, respiratory_rate_set\n",
    "            baseline_size = len(env.baseline_cols)  # e.g., 2 for gender_M, age\n",
    "            obs_hrs = env.obs_hrs  # Assuming env has obs_hrs attribute\n",
    "\n",
    "            # At t=0, include historical states and current state from initial obs\n",
    "            for h in range(obs_hrs):  # h=0 for s_1, h=1 for s_2, ..., h=obs_hrs-1 for s_current\n",
    "                state_start = h * (state_size + action_size)\n",
    "                state = obs[state_start:state_start + state_size]  # Extract state\n",
    "                # Action: use a_h for s_h, last historical action for s_current\n",
    "                if h < obs_hrs - 1:\n",
    "                    action_start = state_start + state_size\n",
    "                    action = obs[action_start:action_start + action_size]\n",
    "                else:\n",
    "                    # For s_current, use the last historical action (a_(obs_hrs-1))\n",
    "                    last_action_start = (obs_hrs - 1) * (state_size + action_size) - action_size\n",
    "                    action = obs[last_action_start:last_action_start + action_size]\n",
    "                # Baseline variables\n",
    "                baseline_start = len(obs) - baseline_size\n",
    "                baseline = obs[baseline_start:baseline_start + baseline_size]\n",
    "\n",
    "                trajectory.append({\n",
    "                    'stay_id': stay_id,\n",
    "                    'hours_in': h,\n",
    "                    **{col: state[i] for i, col in enumerate(env.state_cols)},\n",
    "                    **{col: action[i] for i, col in enumerate(env.action_cols)},\n",
    "                    **{col: baseline[i] for i, col in enumerate(env.baseline_cols)}\n",
    "                })\n",
    "\n",
    "            # Continue for remaining steps\n",
    "            t = obs_hrs\n",
    "            while not done:\n",
    "                action_idx = agent.predict(np.expand_dims(obs, axis=0))[0]\n",
    "                action_tuple = env.id_to_action[action_idx]  # Assuming this maps action index to tuple\n",
    "                action_diversity_set.add(action_idx)\n",
    "                action_diversity_category_set.add(action_idx)\n",
    "                next_obs, reward, done, info = env.step(action_idx)\n",
    "                total_reward += reward\n",
    "\n",
    "                # Extract current state and baseline from next_obs\n",
    "                state_start = (obs_hrs - 1) * (state_size + action_size)\n",
    "                state = next_obs[state_start:state_start + state_size]  # s_current\n",
    "                baseline_start = len(next_obs) - baseline_size\n",
    "                baseline = next_obs[baseline_start:baseline_start + baseline_size]\n",
    "\n",
    "                # Store trajectory\n",
    "                trajectory.append({\n",
    "                    'stay_id': stay_id,\n",
    "                    'hours_in': t,\n",
    "                    **{col: state[i] for i, col in enumerate(env.state_cols)},\n",
    "                    **{col: action_tuple[i] for i, col in enumerate(env.action_cols)},\n",
    "                    **{col: baseline[i] for i, col in enumerate(env.baseline_cols)}\n",
    "                })\n",
    "\n",
    "                obs = next_obs\n",
    "                t += 1\n",
    "            # {\"nonsense_action\": nonsense_action, \"meet_weaning\": meet_weaning}\n",
    "            meet_weaning_list.append(info[\"meet_weaning\"])\n",
    "            nonsense_action_list.append(info[\"nonsense_action\"])\n",
    "            steps_list.append(t)\n",
    "            if info[\"meet_weaning\"]:\n",
    "                steps_to_weaning_list.append(t)\n",
    "\n",
    "            episode_returns.append(total_reward)\n",
    "            evaluation_trajectories.extend(trajectory)  # Add this episode's steps to the full list\n",
    "\n",
    "        if len(episode_returns) == 0:\n",
    "            print(f\"Skipping {category}: No valid episode returns\")\n",
    "            continue\n",
    "\n",
    "        mean_reward = np.mean(episode_returns)\n",
    "        std_reward = np.std(episode_returns)\n",
    "        rewards_summary.append({\n",
    "            'algo': algo,\n",
    "            'data_source': data_source,\n",
    "            'category': category,\n",
    "            'mean_reward': mean_reward,\n",
    "            'std_reward': std_reward,\n",
    "            'meet_weaning_percentage': np.mean(meet_weaning_list),\n",
    "            'mean_traj_len': np.mean(steps_list),\n",
    "            'std_traj_len': np.std(steps_list),\n",
    "            'mean_traj_len_to_meet_weaning': np.mean(steps_to_weaning_list),\n",
    "            'std_traj_len_to_meet_weaning': np.std(steps_to_weaning_list),\n",
    "            'action_diversity': len(action_diversity_category_set),\n",
    "            'mean_nonsense_actions': np.mean(nonsense_action_list)\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "        \n",
    "        print(f\"{algo} {data_source} - {category}: Average Total Reward = {mean_reward}\")\n",
    "\n",
    "        # Record action diversity for the epoch\n",
    "        action_diversity = len(action_diversity_set)\n",
    "        action_diversity_summary.append({\n",
    "            'algo': algo,\n",
    "            'data_source': data_source,\n",
    "            'action_diversity': action_diversity\n",
    "        })\n",
    "        print(f\"Action diversity: {action_diversity}\")\n",
    "        print(f\"Actions taken: {action_diversity_set}\")\n",
    "\n",
    "    # Save all evaluation trajectories to a CSV file\n",
    "    df_trajectories = pd.DataFrame(evaluation_trajectories)\n",
    "    df_trajectories.to_csv(f'../models/training_log/naive_agent/eval_trajectories_{algo}_{data_source}.csv', index=False)\n",
    "    print(f\"Saved evaluation trajectories to 'eval_trajectories_{algo}_{data_source}.csv'\")\n",
    "    # Save rewards summary to a CSV file\n",
    "    df_rewards = pd.DataFrame(rewards_summary)\n",
    "    df_rewards.to_csv(f'../models/training_log/naive_agent/rewards_summary_{algo}_{data_source}.csv', index=False)\n",
    "    # Save action diversity summary to a CSV file\n",
    "    df_action_diversity = pd.DataFrame(action_diversity_summary)\n",
    "    df_action_diversity.to_csv(f'../models/training_log/naive_agent/action_diversity_summary_{algo}_{data_source}.csv', index=False)\n",
    "        # save model\n",
    "        # if mean_reward > cur_best_reward: # TODO: only use common reward for finding the best model, since it is the last one to be eval\n",
    "        #     cur_best_reward = mean_reward\n",
    "        #     print(f\"Saving model to {model_path}\")\n",
    "        #     agent.save_model(model_path)\n",
    "    # # Save model\n",
    "    # print(f\"Saving model to {model_path}\")\n",
    "    # agent.save_model(model_path)\n",
    "\n",
    "    return agent, env, df_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_naive_train, env, reward_summary_train = naive_agent_evaluate(train_df, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs=3, algo='naive_agent', data_source='train', retrain=False)\n",
    "agent_naive_test, env, reward_summary_test = naive_agent_evaluate(test_df, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs=3, algo='naive_agent', data_source='test', retrain=False)\n",
    "agent_naive_eICU, env, reward_summary_eICU = naive_agent_evaluate(eICU_disc, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs=3, algo='naive_agent', data_source='eICU', retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_naive_test, env, reward_summary_test = naive_agent_evaluate(test_df, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs=3, algo='naive_agent', data_source='test', retrain=False)\n",
    "# agent_naive_eICU, env, reward_summary_eICU = naive_agent_evaluate(eICU_disc, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs=3, algo='naive_agent', data_source='eICU', retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_summary_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_summary_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_summary_eICU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your original dataframe is called 'df'\n",
    "# Create the transformed dataframe\n",
    "def turn_into_benchmark_table_format(df):\n",
    "    transformed_data = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Calculate formatted values\n",
    "        total_reward = f\"{row['mean_reward']:.2f}{row['std_reward']:.2f}\"\n",
    "        meet_extubation = f\"{row['meet_weaning_percentage']*100:.0f}%\"\n",
    "        nonsense_action = f\"{row['mean_nonsense_actions']*100:.0f}%\"\n",
    "        avg_hours = f\"{row['mean_traj_len']:.2f}{row['std_traj_len']:.2f}\"\n",
    "        \n",
    "        # Handle avg hours to meet - only show if weaning rate > 0\n",
    "        if row['meet_weaning_percentage'] > 0:\n",
    "            avg_hours_to_meet = f\"{row['mean_traj_len_to_meet_weaning']:.2f}{row['std_traj_len_to_meet_weaning']:.2f}\"\n",
    "        else:\n",
    "            avg_hours_to_meet = \"N/A\"\n",
    "        \n",
    "        transformed_row = {\n",
    "            'Category': row['category'].replace('_reward', ''),\n",
    "            'N_Samples': 100,  # You may need to adjust this based on your actual sample sizes\n",
    "            'Total Reward (MeanStd)': total_reward,\n",
    "            'Meet Extubation': meet_extubation,\n",
    "            'Avg Hours (MeanStd)': avg_hours,\n",
    "            'Avg Hours to Meet (MeanStd)': avg_hours_to_meet,\n",
    "            'Action Diversity': int(row['action_diversity']),\n",
    "            'Anomaly Action': nonsense_action\n",
    "        }\n",
    "        \n",
    "        transformed_data.append(transformed_row)\n",
    "\n",
    "    # Create the new dataframe\n",
    "    new_df = pd.DataFrame(transformed_data)\n",
    "\n",
    "    # If you want to add an \"all\" row (you'll need to calculate these values from your full dataset)\n",
    "    # all_row = {\n",
    "    #     'Category': 'all',\n",
    "    #     'N_Samples': 7443,\n",
    "    #     'Total Reward (MeanStd)': '6.058.87',\n",
    "    #     'Meet Extubation': '91.94%',\n",
    "    #     'Avg Hours (MeanStd)': '19.3920.62',\n",
    "    #     'Avg Hours to Meet (MeanStd)': '16.0214.42',\n",
    "    #     'Action Diversity': 89\n",
    "    # }\n",
    "    # new_df = pd.concat([new_df, pd.DataFrame([all_row])], ignore_index=True)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_into_benchmark_table_format(reward_summary_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_into_benchmark_table_format(reward_summary_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_into_benchmark_table_format(reward_summary_eICU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_agent_train_rewards = pd.read_csv('../models/training_log/naive_agent/rewards_summary_naive_agent_train.csv')\n",
    "naive_agent_test_rewards = pd.read_csv('../models/training_log/naive_agent/rewards_summary_naive_agent_test.csv')\n",
    "naive_agent_eICU_rewards = pd.read_csv('../models/training_log/naive_agent/rewards_summary_naive_agent_eICU.csv')\n",
    "naive_agent_train_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from the provided CSVs\n",
    "naive_agent_train_rewards = pd.read_csv('../models/training_log/naive_agentrewards_summary_naive_agent_train.csv')\n",
    "naive_agent_test_rewards = pd.read_csv('../models/training_log/naive_agentrewards_summary_naive_agent_test.csv')\n",
    "naive_agent_eICU_rewards = pd.read_csv('../models/training_log/naive_agentrewards_summary_naive_agent_eICU.csv')\n",
    "\n",
    "# Add dataset column to each dataframe\n",
    "naive_agent_train_rewards['dataset'] = 'Train'\n",
    "naive_agent_test_rewards['dataset'] = 'Test'\n",
    "naive_agent_eICU_rewards['dataset'] = 'eICU'\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_data = pd.concat([naive_agent_train_rewards, naive_agent_test_rewards, naive_agent_eICU_rewards])\n",
    "\n",
    "# Create a pivot table for the heatmap\n",
    "heatmap_data = combined_data.pivot_table(\n",
    "    index='category', \n",
    "    columns='dataset', \n",
    "    values='mean_reward',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "sns.heatmap(heatmap_data, annot=True, cmap=\"RdYlGn\", center=0, fmt=\".2f\", linewidths=.5)\n",
    "\n",
    "plt.title('Mean Rewards by Category and Dataset', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('mean_rewards_heatmap.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create another heatmap for standard deviation\n",
    "std_heatmap_data = combined_data.pivot_table(\n",
    "    index='category', \n",
    "    columns='dataset', \n",
    "    values='std_reward',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(std_heatmap_data, annot=True, cmap=\"Blues\", fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Standard Deviation of Rewards by Category and Dataset', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('std_rewards_heatmap.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Let's also create a more comprehensive visualization\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "# Plot mean rewards heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, cmap=\"RdYlGn\", center=0, fmt=\".2f\", \n",
    "            linewidths=.5, ax=axes[0], cbar_kws={'label': 'Mean Reward'})\n",
    "axes[0].set_title('Mean Rewards', fontsize=14)\n",
    "\n",
    "# Plot std deviation heatmap\n",
    "sns.heatmap(std_heatmap_data, annot=True, cmap=\"Blues\", fmt=\".2f\", \n",
    "            linewidths=.5, ax=axes[1], cbar_kws={'label': 'Standard Deviation'})\n",
    "axes[1].set_title('Standard Deviation of Rewards', fontsize=14)\n",
    "\n",
    "plt.suptitle('Reward Statistics by Category and Dataset', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.savefig('combined_rewards_heatmap.png', dpi=300)\n",
    "\n",
    "# Create a barplot for comparison across categories\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.barplot(x='category', y='mean_reward', hue='dataset', data=combined_data)\n",
    "plt.title('Mean Rewards by Category Across Datasets', fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rewards_barplot.png', dpi=300)\n",
    "\n",
    "print(\"Visualizations created successfully!\")\n",
    "\n",
    "# Display the summary statistics for each dataset\n",
    "print(\"\\nSummary Statistics:\")\n",
    "for dataset in ['Train', 'Test', 'eICU']:\n",
    "    subset = combined_data[combined_data['dataset'] == dataset]\n",
    "    print(f\"\\n{dataset} Dataset:\")\n",
    "    print(f\"Average Mean Reward: {subset['mean_reward'].mean():.2f}\")\n",
    "    print(f\"Average Std Reward: {subset['std_reward'].mean():.2f}\")\n",
    "    print(f\"Best Category: {subset.loc[subset['mean_reward'].idxmax(), 'category']} ({subset['mean_reward'].max():.2f})\")\n",
    "    print(f\"Worst Category: {subset.loc[subset['mean_reward'].idxmin(), 'category']} ({subset['mean_reward'].min():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from the provided CSVs\n",
    "naive_agent_train_rewards = pd.read_csv('../models/training_log/rewards_summary_naive_agent_train.csv')\n",
    "naive_agent_test_rewards = pd.read_csv('../models/training_log/rewards_summary_naive_agent_test.csv')\n",
    "naive_agent_eICU_rewards = pd.read_csv('../models/training_log/rewards_summary_naive_agent_eICU.csv')\n",
    "\n",
    "# Add dataset column to each dataframe\n",
    "naive_agent_train_rewards['Dataset'] = 'Train'\n",
    "naive_agent_test_rewards['Dataset'] = 'Test'\n",
    "naive_agent_eICU_rewards['Dataset'] = 'eICU'\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_rewards = pd.concat([naive_agent_train_rewards, naive_agent_test_rewards, naive_agent_eICU_rewards])\n",
    "\n",
    "# Rename columns to be consistent with the first part if needed\n",
    "combined_rewards = combined_rewards.rename(columns={\n",
    "    'category': 'Category',\n",
    "    'mean_reward': 'mean_reward',\n",
    "    'std_reward': 'std_reward',\n",
    "    'mean_steps': 'mean_steps_to_weaning',\n",
    "    'std_steps': 'std_steps_to_weaning'\n",
    "})\n",
    "\n",
    "# Define the metrics to plot (matching the first part)\n",
    "metrics_to_plot = ['mean_reward', 'std_reward', 'mean_steps_to_weaning', \n",
    "                   'std_steps_to_weaning']\n",
    "\n",
    "# Check if action_diversity exists in columns, if not we'll handle it separately\n",
    "if 'action_diversity' in combined_rewards.columns:\n",
    "    metrics_to_plot.append('action_diversity')\n",
    "\n",
    "# Create subplots for each metric\n",
    "fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(14, 20))\n",
    "fig.suptitle('Naive Agent: Comparison of Metrics Across Datasets and Categories', fontsize=16)\n",
    "\n",
    "# Get the category order - keep it consistent with part 1 if possible\n",
    "# You might need to adjust this based on what categories are in your data\n",
    "category_order = combined_rewards['Category'].unique().tolist()\n",
    "# Try to match the original order if those categories exist\n",
    "preferred_order = [\n",
    "    'large_reward', 'median_reward', 'small_reward',\n",
    "    'short_to_wean', 'median_to_wean', 'long_to_wean'\n",
    "]\n",
    "category_order = [cat for cat in preferred_order if cat in category_order]\n",
    "# Add any missing categories\n",
    "for cat in combined_rewards['Category'].unique():\n",
    "    if cat not in category_order:\n",
    "        category_order.append(cat)\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    if metric not in combined_rewards.columns and metric == 'action_diversity':\n",
    "        # Skip action_diversity if it doesn't exist in the dataset\n",
    "        continue\n",
    "    elif metric not in combined_rewards.columns:\n",
    "        print(f\"Warning: Metric '{metric}' not found in data. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Create pivot table for this metric\n",
    "    pivot_data = combined_rewards.pivot_table(\n",
    "        index='Category', \n",
    "        columns='Dataset', \n",
    "        values=metric\n",
    "    )\n",
    "    \n",
    "    # Reorder categories for better visualization\n",
    "    pivot_data = pivot_data.reindex(category_order)\n",
    "    \n",
    "    # Determine colormap based on metric (matching the original)\n",
    "    if metric == 'mean_reward':\n",
    "        cmap = 'RdYlGn'  # Red-Yellow-Green: Red for negative, Green for positive rewards\n",
    "    elif 'steps' in metric:\n",
    "        cmap = 'RdYlGn_r'  # Reversed: Green for fewer steps, Red for more steps\n",
    "    elif metric == 'action_diversity':\n",
    "        cmap = 'viridis'  # Neutral colormap for diversity\n",
    "    elif metric == 'std_reward':\n",
    "        cmap = 'coolwarm'  # Default colormap for std\n",
    "    else:\n",
    "        cmap = 'coolwarm'  # Default colormap\n",
    "    \n",
    "    # Plot heatmap\n",
    "    ax = axes[i]\n",
    "    sns.heatmap(pivot_data, annot=True, fmt=\".2f\", cmap=cmap, ax=ax, linewidths=0.5)\n",
    "    \n",
    "    # Format the heatmap\n",
    "    ax.set_title(f'{metric}', fontsize=14)\n",
    "    ax.set_ylabel('')\n",
    "    if i < len(metrics_to_plot) - 1:\n",
    "        ax.set_xlabel('')\n",
    "    else:\n",
    "        ax.set_xlabel('Dataset')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.savefig('naive_agent_metrics_heatmaps.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Let's also create a bar chart for mean_reward across all categories and datasets\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='Category', y='mean_reward', hue='Dataset', data=combined_rewards)\n",
    "plt.title('Naive Agent: Mean Reward Comparison Across Categories and Datasets', fontsize=15)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('naive_agent_mean_rewards_barplot.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# If mean_steps_to_weaning exists, create a bar chart for it\n",
    "if 'mean_steps_to_weaning' in combined_rewards.columns:\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(x='Category', y='mean_steps_to_weaning', hue='Dataset', data=combined_rewards)\n",
    "    plt.title('Naive Agent: Mean Steps to Weaning Comparison Across Categories and Datasets', fontsize=15)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('naive_agent_mean_steps_barplot.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# If your dataset has the necessary columns for a radar chart, include it\n",
    "# This requires mean_reward, mean_steps_to_weaning, and action_diversity\n",
    "radar_metrics = ['mean_reward']\n",
    "if 'mean_steps_to_weaning' in combined_rewards.columns:\n",
    "    radar_metrics.append('mean_steps_to_weaning')\n",
    "if 'action_diversity' in combined_rewards.columns:\n",
    "    radar_metrics.append('action_diversity')\n",
    "\n",
    "if len(radar_metrics) >= 2:  # Need at least 2 metrics for a radar chart\n",
    "    # Function to create radar chart (from original code)\n",
    "    def create_radar_chart(categories, metrics, data, title):\n",
    "        # Prepare data for radar chart\n",
    "        radar_data = []\n",
    "        for category in categories:\n",
    "            for dataset in ['Train', 'Test', 'eICU']:\n",
    "                values = {}\n",
    "                for metric in metrics:\n",
    "                    subset = data[(data['Dataset'] == dataset) & (data['Category'] == category)]\n",
    "                    if not subset.empty and metric in subset.columns:\n",
    "                        values[metric] = subset[metric].values[0]\n",
    "                \n",
    "                if len(values) == len(metrics):  # Only add if all metrics are present\n",
    "                    entry = {'Category': category, 'Dataset': dataset}\n",
    "                    entry.update(values)\n",
    "                    radar_data.append(entry)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        radar_df = pd.DataFrame(radar_data)\n",
    "        \n",
    "        # Normalize data for radar chart (different scales)\n",
    "        for metric in metrics:\n",
    "            min_val = radar_df[metric].min()\n",
    "            max_val = radar_df[metric].max()\n",
    "            if max_val > min_val:  # Avoid division by zero\n",
    "                radar_df[metric] = (radar_df[metric] - min_val) / (max_val - min_val)\n",
    "        \n",
    "        # Create the radar chart\n",
    "        num_metrics = len(metrics)\n",
    "        angles = np.linspace(0, 2*np.pi, num_metrics, endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Close the polygon\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "        \n",
    "        # Add metrics labels\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(metrics)\n",
    "        \n",
    "        # Add data for each category and dataset\n",
    "        for dataset in ['Train', 'Test', 'eICU']:\n",
    "            for category in categories:\n",
    "                subset = radar_df[(radar_df['Dataset'] == dataset) & (radar_df['Category'] == category)]\n",
    "                if not subset.empty:\n",
    "                    values = [subset[metric].values[0] for metric in metrics]\n",
    "                    values += values[:1]  # Close the polygon\n",
    "                    \n",
    "                    ax.plot(angles, values, linewidth=2, label=f\"{dataset} - {category}\")\n",
    "                    ax.fill(angles, values, alpha=0.1)\n",
    "        \n",
    "        ax.set_title(title, fontsize=15)\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    # Define categories for radar chart\n",
    "    radar_categories = combined_rewards['Category'].unique().tolist()[:3]  # Limit to 3 categories for clarity\n",
    "    \n",
    "    # Create radar chart\n",
    "    radar_fig = create_radar_chart(\n",
    "        radar_categories, \n",
    "        radar_metrics,\n",
    "        combined_rewards,\n",
    "        'Naive Agent: Normalized Comparison of Categories Across Datasets'\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('naive_agent_radar_chart.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Display the summary statistics for each dataset\n",
    "print(\"\\nNaive Agent Summary Statistics:\")\n",
    "for dataset in ['Train', 'Test', 'eICU']:\n",
    "    subset = combined_rewards[combined_rewards['Dataset'] == dataset]\n",
    "    print(f\"\\n{dataset} Dataset:\")\n",
    "    print(f\"Average Mean Reward: {subset['mean_reward'].mean():.2f}\")\n",
    "    print(f\"Average Std Reward: {subset['std_reward'].mean():.2f}\")\n",
    "    if 'mean_steps_to_weaning' in subset.columns:\n",
    "        print(f\"Average Steps to Weaning: {subset['mean_steps_to_weaning'].mean():.2f}\")\n",
    "    print(f\"Best Category: {subset.loc[subset['mean_reward'].idxmax(), 'Category']} ({subset['mean_reward'].max():.2f})\")\n",
    "    print(f\"Worst Category: {subset.loc[subset['mean_reward'].idxmin(), 'Category']} ({subset['mean_reward'].min():.2f})\")\n",
    "\n",
    "print(\"Visualizations created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Training Progress of the Reward and action diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = \"../models/training_log/rewards_summary_DiscreteBCQ_eICU.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filter data for specific algorithm and data source\n",
    "algo = \"DiscreteBCQ\"\n",
    "data_source = \"eICU\"\n",
    "filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "# Extract unique categories\n",
    "unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "# Plot the mean reward and standard deviation for each category\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for category in unique_categories:\n",
    "    category_data = filtered_data[filtered_data['category'] == category]\n",
    "    epochs = category_data['epoch']\n",
    "    mean_rewards = category_data['mean_reward']\n",
    "    std_rewards = category_data['std_reward']\n",
    "\n",
    "    # Plot mean reward curve\n",
    "    plt.plot(epochs, mean_rewards, label=f\"{category} (mean)\", marker='o')\n",
    "\n",
    "    # Plot standard deviation as shaded area\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        mean_rewards - std_rewards,\n",
    "        mean_rewards + std_rewards,\n",
    "        alpha=0.2,\n",
    "        label=f\"{category} (std)\"\n",
    "    )\n",
    "\n",
    "# Add title, labels, and legend\n",
    "plt.title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Reward\", fontsize=14)\n",
    "plt.xticks(np.arange(0, 10, 1))\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Filter data for specific algorithm and data source\n",
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Plot the mean reward and standard deviation for each category\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_data[filtered_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            plt.plot(epochs, mean_rewards, label=f\"{category}\", marker='o')\n",
    "            # plt.plot(epochs, mean_rewards, label=f\"{category} (mean+std)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            plt.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "                # label=f\"{category} (std)\"\n",
    "            )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Reward\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Filter data for specific algorithm and data source\n",
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Plot the mean reward and standard deviation for each category\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_data[filtered_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            plt.plot(epochs, mean_rewards, label=f\"{category}\", marker='o')\n",
    "            # plt.plot(epochs, mean_rewards, label=f\"{category} (mean+std)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            plt.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "                # label=f\"{category} (std)\"\n",
    "            )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Reward\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty, With Weaning Reward on Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Filter data for specific algorithm and data source\n",
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Plot the mean reward and standard deviation for each category\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_data[filtered_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            plt.plot(epochs, mean_rewards, label=f\"{category}\", marker='o')\n",
    "            # plt.plot(epochs, mean_rewards, label=f\"{category} (mean+std)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            plt.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "                # label=f\"{category} (std)\"\n",
    "            )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Reward\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty, With Weaning Reward on Agent, default parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Filter data for specific algorithm and data source\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Plot the mean reward and standard deviation for each category\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_data[filtered_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            plt.plot(epochs, mean_rewards, label=f\"{category}\", marker='o')\n",
    "            # plt.plot(epochs, mean_rewards, label=f\"{category} (mean+std)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            plt.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "                # label=f\"{category} (std)\"\n",
    "            )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Reward\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty, With Weaning Reward on Agent, Same parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Filter data for specific algorithm and data source\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Plot the mean reward and standard deviation for each category\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_data[filtered_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            plt.plot(epochs, mean_rewards, label=f\"{category}\", marker='o')\n",
    "            # plt.plot(epochs, mean_rewards, label=f\"{category} (mean+std)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            plt.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "                # label=f\"{category} (std)\"\n",
    "            )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Reward\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Action penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Filter data for specific algorithm and data source\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Plot the mean reward and standard deviation for each category\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_data[filtered_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            plt.plot(epochs, mean_rewards, label=f\"{category}\", marker='o')\n",
    "            # plt.plot(epochs, mean_rewards, label=f\"{category} (mean+std)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            plt.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "                # label=f\"{category} (std)\"\n",
    "            )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Reward\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        # Load reward data\n",
    "        reward_file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            reward_data = pd.read_csv(reward_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Reward file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter reward data for the current algorithm and data source\n",
    "        filtered_reward_data = reward_data[\n",
    "            (reward_data['algo'] == algo) & (reward_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any reward data\n",
    "        if filtered_reward_data.empty:\n",
    "            print(f\"No reward data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Load action diversity data\n",
    "        diversity_file_path = f\"../models/training_log/action_diversity_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            diversity_data = pd.read_csv(diversity_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Action diversity file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter action diversity data for the current algorithm and data source\n",
    "        filtered_diversity_data = diversity_data[\n",
    "            (diversity_data['algo'] == algo) & (diversity_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any action diversity data\n",
    "        if filtered_diversity_data.empty:\n",
    "            print(f\"No action diversity data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories for rewards\n",
    "        unique_categories = filtered_reward_data['category'].unique()\n",
    "\n",
    "        # Plot the mean reward, standard deviation, and action diversity\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_reward_data[filtered_reward_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            plt.plot(epochs, mean_rewards, label=f\"{category} (Reward)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            plt.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "\n",
    "        # Plot action diversity curve\n",
    "        diversity_epochs = filtered_diversity_data['epoch']\n",
    "        action_diversity = filtered_diversity_data['action_diversity']\n",
    "        plt.plot(diversity_epochs, action_diversity, label=\"Action Diversity\", color=\"black\", linestyle=\"--\", marker='x')\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Value\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 10, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training progress with action diversity across different RL algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        # Load reward data\n",
    "        reward_file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            reward_data = pd.read_csv(reward_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Reward file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter reward data for the current algorithm and data source\n",
    "        filtered_reward_data = reward_data[\n",
    "            (reward_data['algo'] == algo) & (reward_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any reward data\n",
    "        if filtered_reward_data.empty:\n",
    "            print(f\"No reward data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Load action diversity data\n",
    "        diversity_file_path = f\"../models/training_log/action_diversity_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            diversity_data = pd.read_csv(diversity_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Action diversity file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter action diversity data for the current algorithm and data source\n",
    "        filtered_diversity_data = diversity_data[\n",
    "            (diversity_data['algo'] == algo) & (diversity_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any action diversity data\n",
    "        if filtered_diversity_data.empty:\n",
    "            print(f\"No action diversity data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories for rewards\n",
    "        unique_categories = filtered_reward_data['category'].unique()\n",
    "\n",
    "        # Create a figure and axis for the plot\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        # Plot the mean reward and standard deviation\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_reward_data[filtered_reward_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            ax1.plot(epochs, mean_rewards, label=f\"{category} (Reward)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            ax1.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "\n",
    "        # Set labels and title for the left y-axis (rewards)\n",
    "        ax1.set_xlabel(\"Epoch\", fontsize=14)\n",
    "        ax1.set_ylabel(\"Reward\", fontsize=14, color=\"blue\")\n",
    "        ax1.tick_params(axis='y', labelcolor=\"blue\")\n",
    "        ax1.set_title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        ax1.grid(alpha=0.3)\n",
    "\n",
    "        # Create a secondary y-axis for action diversity\n",
    "        ax2 = ax1.twinx()\n",
    "        diversity_epochs = filtered_diversity_data['epoch']\n",
    "        action_diversity = filtered_diversity_data['action_diversity']\n",
    "\n",
    "        # Plot action diversity curve\n",
    "        ax2.plot(diversity_epochs, action_diversity, label=\"Action Diversity\", color=\"black\", linestyle=\"--\", marker='x')\n",
    "\n",
    "        # Set labels for the right y-axis (action diversity)\n",
    "        ax2.set_ylabel(\"Action Diversity\", fontsize=14, color=\"black\")\n",
    "        ax2.tick_params(axis='y', labelcolor=\"black\")\n",
    "\n",
    "        # Combine legends from both axes\n",
    "        lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "        lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"upper left\", fontsize=10)\n",
    "\n",
    "        # Adjust layout and show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty, With Weaning Reward on Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        # Load reward data\n",
    "        reward_file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            reward_data = pd.read_csv(reward_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Reward file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter reward data for the current algorithm and data source\n",
    "        filtered_reward_data = reward_data[\n",
    "            (reward_data['algo'] == algo) & (reward_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any reward data\n",
    "        if filtered_reward_data.empty:\n",
    "            print(f\"No reward data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Load action diversity data\n",
    "        diversity_file_path = f\"../models/training_log/action_diversity_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            diversity_data = pd.read_csv(diversity_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Action diversity file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter action diversity data for the current algorithm and data source\n",
    "        filtered_diversity_data = diversity_data[\n",
    "            (diversity_data['algo'] == algo) & (diversity_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any action diversity data\n",
    "        if filtered_diversity_data.empty:\n",
    "            print(f\"No action diversity data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories for rewards\n",
    "        unique_categories = filtered_reward_data['category'].unique()\n",
    "\n",
    "        # Create a figure and axis for the plot\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        # Plot the mean reward and standard deviation\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_reward_data[filtered_reward_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            ax1.plot(epochs, mean_rewards, label=f\"{category} (Reward)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            ax1.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "\n",
    "        # Set labels and title for the left y-axis (rewards)\n",
    "        ax1.set_xlabel(\"Epoch\", fontsize=14)\n",
    "        ax1.set_ylabel(\"Reward\", fontsize=14, color=\"blue\")\n",
    "        ax1.tick_params(axis='y', labelcolor=\"blue\")\n",
    "        ax1.set_title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        ax1.grid(alpha=0.3)\n",
    "\n",
    "        # Create a secondary y-axis for action diversity\n",
    "        ax2 = ax1.twinx()\n",
    "        diversity_epochs = filtered_diversity_data['epoch']\n",
    "        action_diversity = filtered_diversity_data['action_diversity']\n",
    "\n",
    "        # Plot action diversity curve\n",
    "        ax2.plot(diversity_epochs, action_diversity, label=\"Action Diversity\", color=\"black\", linestyle=\"--\", marker='x')\n",
    "\n",
    "        # Set labels for the right y-axis (action diversity)\n",
    "        ax2.set_ylabel(\"Action Diversity\", fontsize=14, color=\"black\")\n",
    "        ax2.tick_params(axis='y', labelcolor=\"black\")\n",
    "\n",
    "        # Combine legends from both axes\n",
    "        lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "        lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"upper left\", fontsize=10)\n",
    "\n",
    "        # Adjust layout and show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty, With Weaning Reward on Agent, Same parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        # Load reward data\n",
    "        reward_file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            reward_data = pd.read_csv(reward_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Reward file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter reward data for the current algorithm and data source\n",
    "        filtered_reward_data = reward_data[\n",
    "            (reward_data['algo'] == algo) & (reward_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any reward data\n",
    "        if filtered_reward_data.empty:\n",
    "            print(f\"No reward data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Load action diversity data\n",
    "        diversity_file_path = f\"../models/training_log/action_diversity_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            diversity_data = pd.read_csv(diversity_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Action diversity file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter action diversity data for the current algorithm and data source\n",
    "        filtered_diversity_data = diversity_data[\n",
    "            (diversity_data['algo'] == algo) & (diversity_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any action diversity data\n",
    "        if filtered_diversity_data.empty:\n",
    "            print(f\"No action diversity data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories for rewards\n",
    "        unique_categories = filtered_reward_data['category'].unique()\n",
    "\n",
    "        # Create a figure and axis for the plot\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        # Plot the mean reward and standard deviation\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_reward_data[filtered_reward_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            ax1.plot(epochs, mean_rewards, label=f\"{category} (Reward)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            ax1.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "\n",
    "        # Set labels and title for the left y-axis (rewards)\n",
    "        ax1.set_xlabel(\"Epoch\", fontsize=14)\n",
    "        ax1.set_ylabel(\"Reward\", fontsize=14, color=\"blue\")\n",
    "        ax1.tick_params(axis='y', labelcolor=\"blue\")\n",
    "        ax1.set_title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        ax1.grid(alpha=0.3)\n",
    "\n",
    "        # Create a secondary y-axis for action diversity\n",
    "        ax2 = ax1.twinx()\n",
    "        diversity_epochs = filtered_diversity_data['epoch']\n",
    "        action_diversity = filtered_diversity_data['action_diversity']\n",
    "\n",
    "        # Plot action diversity curve\n",
    "        ax2.plot(diversity_epochs, action_diversity, label=\"Action Diversity\", color=\"black\", linestyle=\"--\", marker='x')\n",
    "\n",
    "        # Set labels for the right y-axis (action diversity)\n",
    "        ax2.set_ylabel(\"Action Diversity\", fontsize=14, color=\"black\")\n",
    "        ax2.tick_params(axis='y', labelcolor=\"black\")\n",
    "\n",
    "        # Combine legends from both axes\n",
    "        lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "        lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"upper left\", fontsize=10)\n",
    "\n",
    "        # Adjust layout and show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty, With Weaning Reward on Agent, With Same parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        # Load reward data\n",
    "        reward_file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            reward_data = pd.read_csv(reward_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Reward file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter reward data for the current algorithm and data source\n",
    "        filtered_reward_data = reward_data[\n",
    "            (reward_data['algo'] == algo) & (reward_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any reward data\n",
    "        if filtered_reward_data.empty:\n",
    "            print(f\"No reward data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Load action diversity data\n",
    "        diversity_file_path = f\"../models/training_log/action_diversity_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            diversity_data = pd.read_csv(diversity_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Action diversity file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter action diversity data for the current algorithm and data source\n",
    "        filtered_diversity_data = diversity_data[\n",
    "            (diversity_data['algo'] == algo) & (diversity_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any action diversity data\n",
    "        if filtered_diversity_data.empty:\n",
    "            print(f\"No action diversity data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories for rewards\n",
    "        unique_categories = filtered_reward_data['category'].unique()\n",
    "\n",
    "        # Create a figure and axis for the plot\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        # Plot the mean reward and standard deviation\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_reward_data[filtered_reward_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            ax1.plot(epochs, mean_rewards, label=f\"{category} (Reward)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            ax1.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "\n",
    "        # Set labels and title for the left y-axis (rewards)\n",
    "        ax1.set_xlabel(\"Epoch\", fontsize=14)\n",
    "        ax1.set_ylabel(\"Reward\", fontsize=14, color=\"blue\")\n",
    "        ax1.tick_params(axis='y', labelcolor=\"blue\")\n",
    "        ax1.set_title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        ax1.grid(alpha=0.3)\n",
    "\n",
    "        # Create a secondary y-axis for action diversity\n",
    "        ax2 = ax1.twinx()\n",
    "        diversity_epochs = filtered_diversity_data['epoch']\n",
    "        action_diversity = filtered_diversity_data['action_diversity']\n",
    "\n",
    "        # Plot action diversity curve\n",
    "        ax2.plot(diversity_epochs, action_diversity, label=\"Action Diversity\", color=\"black\", linestyle=\"--\", marker='x')\n",
    "\n",
    "        # Set labels for the right y-axis (action diversity)\n",
    "        ax2.set_ylabel(\"Action Diversity\", fontsize=14, color=\"black\")\n",
    "        ax2.tick_params(axis='y', labelcolor=\"black\")\n",
    "\n",
    "        # Combine legends from both axes\n",
    "        lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "        lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"upper left\", fontsize=10)\n",
    "\n",
    "        # Adjust layout and show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Action penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        # Load reward data\n",
    "        reward_file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            reward_data = pd.read_csv(reward_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Reward file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter reward data for the current algorithm and data source\n",
    "        filtered_reward_data = reward_data[\n",
    "            (reward_data['algo'] == algo) & (reward_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any reward data\n",
    "        if filtered_reward_data.empty:\n",
    "            print(f\"No reward data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Load action diversity data\n",
    "        diversity_file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/action_diversity_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            diversity_data = pd.read_csv(diversity_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Action diversity file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter action diversity data for the current algorithm and data source\n",
    "        filtered_diversity_data = diversity_data[\n",
    "            (diversity_data['algo'] == algo) & (diversity_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any action diversity data\n",
    "        if filtered_diversity_data.empty:\n",
    "            print(f\"No action diversity data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories for rewards\n",
    "        unique_categories = filtered_reward_data['category'].unique()\n",
    "\n",
    "        # Create a figure and axis for the plot\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        # Plot the mean reward and standard deviation\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_reward_data[filtered_reward_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            ax1.plot(epochs, mean_rewards, label=f\"{category} (Reward)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            ax1.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "\n",
    "        # Set labels and title for the left y-axis (rewards)\n",
    "        ax1.set_xlabel(\"Epoch\", fontsize=14)\n",
    "        ax1.set_ylabel(\"Reward\", fontsize=14, color=\"blue\")\n",
    "        ax1.tick_params(axis='y', labelcolor=\"blue\")\n",
    "        ax1.set_title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        ax1.grid(alpha=0.3)\n",
    "\n",
    "        # Create a secondary y-axis for action diversity\n",
    "        ax2 = ax1.twinx()\n",
    "        diversity_epochs = filtered_diversity_data['epoch']\n",
    "        action_diversity = filtered_diversity_data['action_diversity']\n",
    "\n",
    "        # Plot action diversity curve\n",
    "        ax2.plot(diversity_epochs, action_diversity, label=\"Action Diversity\", color=\"black\", linestyle=\"--\", marker='x')\n",
    "\n",
    "        # Set labels for the right y-axis (action diversity)\n",
    "        ax2.set_ylabel(\"Action Diversity\", fontsize=14, color=\"black\")\n",
    "        ax2.tick_params(axis='y', labelcolor=\"black\")\n",
    "\n",
    "        # Combine legends from both axes\n",
    "        lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "        lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"upper left\", fontsize=10)\n",
    "\n",
    "        # Adjust layout and show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        # Load reward data\n",
    "        reward_file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            reward_data = pd.read_csv(reward_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Reward file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter reward data for the current algorithm and data source\n",
    "        filtered_reward_data = reward_data[\n",
    "            (reward_data['algo'] == algo) & (reward_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any reward data\n",
    "        if filtered_reward_data.empty:\n",
    "            print(f\"No reward data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Load action diversity data\n",
    "        diversity_file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/action_diversity_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            diversity_data = pd.read_csv(diversity_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Action diversity file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter action diversity data for the current algorithm and data source\n",
    "        filtered_diversity_data = diversity_data[\n",
    "            (diversity_data['algo'] == algo) & (diversity_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any action diversity data\n",
    "        if filtered_diversity_data.empty:\n",
    "            print(f\"No action diversity data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories for rewards\n",
    "        unique_categories = filtered_reward_data['category'].unique()\n",
    "\n",
    "        # Create a figure and axis for the plot\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        # Plot the mean reward and standard deviation\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_reward_data[filtered_reward_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            ax1.plot(epochs, mean_rewards, label=f\"{category} (Reward)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            ax1.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "\n",
    "        # Set labels and title for the left y-axis (rewards)\n",
    "        ax1.set_xlabel(\"Epoch\", fontsize=14)\n",
    "        ax1.set_ylabel(\"Reward\", fontsize=14, color=\"blue\")\n",
    "        ax1.tick_params(axis='y', labelcolor=\"blue\")\n",
    "        ax1.set_title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        ax1.grid(alpha=0.3)\n",
    "\n",
    "        # Create a secondary y-axis for action diversity\n",
    "        ax2 = ax1.twinx()\n",
    "        diversity_epochs = filtered_diversity_data['epoch']\n",
    "        action_diversity = filtered_diversity_data['action_diversity']\n",
    "\n",
    "        # Plot action diversity curve\n",
    "        ax2.plot(diversity_epochs, action_diversity, label=\"Action Diversity\", color=\"black\", linestyle=\"--\", marker='x')\n",
    "\n",
    "        # Set labels for the right y-axis (action diversity)\n",
    "        ax2.set_ylabel(\"Action Diversity\", fontsize=14, color=\"black\")\n",
    "        ax2.tick_params(axis='y', labelcolor=\"black\")\n",
    "\n",
    "        # Combine legends from both axes\n",
    "        lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "        lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"upper left\", fontsize=10)\n",
    "\n",
    "        # Adjust layout and show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training progress with action diversity and trajectory length across different RL algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        # Load reward data\n",
    "        reward_file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            reward_data = pd.read_csv(reward_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Reward file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter reward data for the current algorithm and data source\n",
    "        filtered_reward_data = reward_data[\n",
    "            (reward_data['algo'] == algo) & (reward_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any reward data\n",
    "        if filtered_reward_data.empty:\n",
    "            print(f\"No reward data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Load action diversity data\n",
    "        diversity_file_path = f\"../models/training_log/action_diversity_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            diversity_data = pd.read_csv(diversity_file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Action diversity file not found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Filter action diversity data for the current algorithm and data source\n",
    "        filtered_diversity_data = diversity_data[\n",
    "            (diversity_data['algo'] == algo) & (diversity_data['data_source'] == data_source)\n",
    "        ]\n",
    "\n",
    "        # Check if there is any action diversity data\n",
    "        if filtered_diversity_data.empty:\n",
    "            print(f\"No action diversity data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories for rewards\n",
    "        unique_categories = filtered_reward_data['category'].unique()\n",
    "\n",
    "        # Create a figure and axis for the plot\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        # Plot the mean reward and standard deviation\n",
    "        for category in unique_categories:\n",
    "            category_data = filtered_reward_data[filtered_reward_data['category'] == category]\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            ax1.plot(epochs, mean_rewards, label=f\"{category} (Reward)\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            ax1.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "\n",
    "        # Set labels and title for the left y-axis (rewards)\n",
    "        ax1.set_xlabel(\"Epoch\", fontsize=14)\n",
    "        ax1.set_ylabel(\"Reward\", fontsize=14, color=\"blue\")\n",
    "        ax1.tick_params(axis='y', labelcolor=\"blue\")\n",
    "        ax1.set_title(f\"Training Progress for {algo} on {data_source}\", fontsize=16)\n",
    "        ax1.grid(alpha=0.3)\n",
    "\n",
    "        # Create a secondary y-axis for action diversity\n",
    "        ax2 = ax1.twinx()\n",
    "        diversity_epochs = filtered_diversity_data['epoch']\n",
    "        action_diversity = filtered_diversity_data['action_diversity']\n",
    "\n",
    "        # Plot action diversity curve\n",
    "        ax2.plot(diversity_epochs, action_diversity, label=\"Action Diversity\", color=\"black\", linestyle=\"--\", marker='x')\n",
    "\n",
    "        # Set labels for the right y-axis (action diversity)\n",
    "        ax2.set_ylabel(\"Action Diversity\", fontsize=14, color=\"black\")\n",
    "        ax2.tick_params(axis='y', labelcolor=\"black\")\n",
    "\n",
    "        # Plot mean trajectory length and standard deviation\n",
    "        traj_epochs = filtered_reward_data['epoch']\n",
    "        mean_traj_len = filtered_reward_data['mean_traj_len']\n",
    "        std_traj_len = filtered_reward_data['std_traj_len']\n",
    "\n",
    "        # Use the same axis as ax1 but with a different color/style\n",
    "        ax3 = ax1.twinx()\n",
    "        ax3.spines['right'].set_position(('outward', 60))  # Offset the third axis\n",
    "        ax3.plot(traj_epochs, mean_traj_len, label=\"Mean Trajectory Length\", color=\"green\", linestyle=\"-.\", marker='s')\n",
    "        ax3.fill_between(\n",
    "            traj_epochs,\n",
    "            mean_traj_len - std_traj_len,\n",
    "            mean_traj_len + std_traj_len,\n",
    "            color=\"green\",\n",
    "            alpha=0.2,\n",
    "        )\n",
    "        ax3.set_ylabel(\"Trajectory Length\", fontsize=14, color=\"green\")\n",
    "        ax3.tick_params(axis='y', labelcolor=\"green\")\n",
    "\n",
    "        # Combine legends from all axes\n",
    "        lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "        lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "        lines_3, labels_3 = ax3.get_legend_handles_labels()\n",
    "        ax1.legend(lines_1 + lines_2 + lines_3, labels_1 + labels_2 + labels_3, loc=\"upper left\", fontsize=10)\n",
    "\n",
    "        # Adjust layout and show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Progress Curve: Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "# Iterate over each data source\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Plot data for each category\n",
    "    for category, algo_data_list in category_data_dict.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for algo, category_data in algo_data_list:\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            plt.plot(epochs, mean_rewards, label=f\"{algo}\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            plt.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2\n",
    "            )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Training Progress for {category} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Reward\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Progress Curve: Reward with Naive Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "# Load naive_agent data\n",
    "naive_agent_train_rewards = pd.read_csv('../models/training_log/rewards_summary_naive_agent_train.csv')\n",
    "naive_agent_test_rewards = pd.read_csv('../models/training_log/rewards_summary_naive_agent_test.csv')\n",
    "naive_agent_eICU_rewards = pd.read_csv('../models/training_log/rewards_summary_naive_agent_eICU.csv')\n",
    "\n",
    "# Map data sources to naive_agent data\n",
    "naive_agent_data_map = {\n",
    "    \"train\": naive_agent_train_rewards,\n",
    "    \"test\": naive_agent_test_rewards,\n",
    "    \"eICU\": naive_agent_eICU_rewards\n",
    "}\n",
    "\n",
    "# Iterate over each data source\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Get naive_agent data for the current data source\n",
    "    naive_agent_data = naive_agent_data_map[data_source]\n",
    "\n",
    "    # Plot data for each category\n",
    "    for category, algo_data_list in category_data_dict.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        # Plot data for each algorithm\n",
    "        for algo, category_data in algo_data_list:\n",
    "            epochs = category_data['epoch']\n",
    "            mean_rewards = category_data['mean_reward']\n",
    "            std_rewards = category_data['std_reward']\n",
    "\n",
    "            # Plot mean reward curve\n",
    "            plt.plot(epochs, mean_rewards, label=f\"{algo}\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            plt.fill_between(\n",
    "                epochs,\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0.2\n",
    "            )\n",
    "\n",
    "        # Add naive_agent constant curve\n",
    "        if category in naive_agent_data['category'].values:\n",
    "            naive_mean_reward = naive_agent_data[naive_agent_data['category'] == category]['mean_reward'].values[0]\n",
    "            plt.axhline(y=naive_mean_reward, color='r', linestyle='--', label='Physician Policy')\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Training Progress for {category} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Reward\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Progress Curve: Length of trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "# Iterate over each data source\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Plot data for each category\n",
    "    for category, algo_data_list in category_data_dict.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for algo, category_data in algo_data_list:\n",
    "            epochs = category_data['epoch']\n",
    "            mean_traj_len = category_data['mean_traj_len_without_nonsense_actions']\n",
    "            std_traj_len = category_data['std_traj_len_without_nonsense_actions']\n",
    "\n",
    "            # Plot mean trajectory length curve\n",
    "            plt.plot(epochs, mean_traj_len, label=f\"{algo}\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            plt.fill_between(\n",
    "                epochs,\n",
    "                mean_traj_len - std_traj_len,\n",
    "                mean_traj_len + std_traj_len,\n",
    "                alpha=0.2\n",
    "            )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Trajectory Length Progress for {category} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Mean Trajectory Length (without nonsense actions)\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty, With Weaning Reward on Agent, With Same parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "# Iterate over each data source\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Plot data for each category\n",
    "    for category, algo_data_list in category_data_dict.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for algo, category_data in algo_data_list:\n",
    "            epochs = category_data['epoch']\n",
    "            mean_traj_len = category_data['mean_traj_len_without_nonsense_actions']\n",
    "            std_traj_len = category_data['std_traj_len_without_nonsense_actions']\n",
    "\n",
    "            # Plot mean trajectory length curve\n",
    "            plt.plot(epochs, mean_traj_len, label=f\"{algo}\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            plt.fill_between(\n",
    "                epochs,\n",
    "                mean_traj_len - std_traj_len,\n",
    "                mean_traj_len + std_traj_len,\n",
    "                alpha=0.2\n",
    "            )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Trajectory Length Progress for {category} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Mean Trajectory Length (without nonsense actions)\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Action penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "# Iterate over each data source\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Plot data for each category\n",
    "    for category, algo_data_list in category_data_dict.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for algo, category_data in algo_data_list:\n",
    "            epochs = category_data['epoch']\n",
    "            mean_traj_len = category_data['mean_traj_len_without_nonsense_actions']\n",
    "            std_traj_len = category_data['std_traj_len_without_nonsense_actions']\n",
    "\n",
    "            # Plot mean trajectory length curve\n",
    "            plt.plot(epochs, mean_traj_len, label=f\"{algo}\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            plt.fill_between(\n",
    "                epochs,\n",
    "                mean_traj_len - std_traj_len,\n",
    "                mean_traj_len + std_traj_len,\n",
    "                alpha=0.2\n",
    "            )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Trajectory Length Progress for {category} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Mean Trajectory Length (without nonsense actions)\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Progress Curve: Nonsense Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "# Iterate over each data source\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/{EXP_FOLDER_PREFIX}/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Plot data for each category\n",
    "    for category, algo_data_list in category_data_dict.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for algo, category_data in algo_data_list:\n",
    "            epochs = category_data['epoch']\n",
    "            mean_nonsense_actions = category_data['mean_nonsense_actions']\n",
    "            # std_nonsense_actions = category_data['std_nonsense_actions']\n",
    "\n",
    "            # Plot mean nonsense actions curve\n",
    "            plt.plot(epochs, mean_nonsense_actions, label=f\"{algo}\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            # plt.fill_between(\n",
    "            #     epochs,\n",
    "            #     mean_nonsense_actions - std_nonsense_actions,\n",
    "            #     mean_nonsense_actions + std_nonsense_actions,\n",
    "            #     alpha=0.2\n",
    "            # )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Mean Anomaly Actions for {category} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Mean Anomaly Actions\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty, With Weaning Reward on Agent, With Same parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "# Iterate over each data source\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Plot data for each category\n",
    "    for category, algo_data_list in category_data_dict.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for algo, category_data in algo_data_list:\n",
    "            epochs = category_data['epoch']\n",
    "            mean_nonsense_actions = category_data['mean_nonsense_actions']\n",
    "            # std_nonsense_actions = category_data['std_nonsense_actions']\n",
    "\n",
    "            # Plot mean nonsense actions curve\n",
    "            plt.plot(epochs, mean_nonsense_actions, label=f\"{algo}\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            # plt.fill_between(\n",
    "            #     epochs,\n",
    "            #     mean_nonsense_actions - std_nonsense_actions,\n",
    "            #     mean_nonsense_actions + std_nonsense_actions,\n",
    "            #     alpha=0.2\n",
    "            # )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Mean Anomaly Actions for {category} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Mean Anomaly Actions\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Action penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of algorithms and data sources\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "\n",
    "# Iterate over each data source\n",
    "for data_source in data_source_list:\n",
    "    # Initialize a dictionary to store data for each category\n",
    "    category_data_dict = {}\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algo in algo_list:\n",
    "        file_path = f\"../models/training_log/rewards_summary_{algo}_{data_source}.csv\"\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Filter data for the current algorithm and data source\n",
    "        filtered_data = data[(data['algo'] == algo) & (data['data_source'] == data_source)]\n",
    "\n",
    "        # Check if there is any data for the current algorithm and data source\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for {algo} on {data_source}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique categories\n",
    "        unique_categories = filtered_data['category'].unique()\n",
    "\n",
    "        # Organize data by category\n",
    "        for category in unique_categories:\n",
    "            if category not in category_data_dict:\n",
    "                category_data_dict[category] = []\n",
    "            category_data_dict[category].append((algo, filtered_data[filtered_data['category'] == category]))\n",
    "\n",
    "    # Plot data for each category\n",
    "    for category, algo_data_list in category_data_dict.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for algo, category_data in algo_data_list:\n",
    "            epochs = category_data['epoch']\n",
    "            mean_nonsense_actions = category_data['mean_nonsense_actions']\n",
    "            # std_nonsense_actions = category_data['std_nonsense_actions']\n",
    "\n",
    "            # Plot mean nonsense actions curve\n",
    "            plt.plot(epochs, mean_nonsense_actions, label=f\"{algo}\", marker='o')\n",
    "\n",
    "            # Plot standard deviation as shaded area\n",
    "            # plt.fill_between(\n",
    "            #     epochs,\n",
    "            #     mean_nonsense_actions - std_nonsense_actions,\n",
    "            #     mean_nonsense_actions + std_nonsense_actions,\n",
    "            #     alpha=0.2\n",
    "            # )\n",
    "\n",
    "        # Add title, labels, and legend\n",
    "        plt.title(f\"Mean Anomaly Actions for {category} on {data_source}\", fontsize=16)\n",
    "        plt.xlabel(\"Epoch\", fontsize=14)\n",
    "        plt.ylabel(\"Mean Anomaly Actions\", fontsize=14)\n",
    "        plt.xticks(np.arange(0, 20, 1))\n",
    "        plt.legend(loc=\"upper left\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define the lists\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "categories = [\"small_reward\", \"median_reward\", \"large_reward\"]\n",
    "\n",
    "# Load all data into a single DataFrame\n",
    "data_frames = []\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        file_path = f'../models/training_log/rewards_summary_{algo}_{data_source}.csv'\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['algo'] = algo\n",
    "            df['data_source'] = data_source\n",
    "            data_frames.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Step 1: Find the best epoch for \"small_reward\" category\n",
    "best_epochs = {}\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        subset = data[(data['algo'] == algo) & (data['data_source'] == data_source) & (data['category'] == 'small_reward')]\n",
    "        if not subset.empty:\n",
    "            best_epoch = subset.loc[subset['mean_reward'].idxmax()]\n",
    "            best_epochs[(algo, data_source)] = best_epoch\n",
    "\n",
    "# Step 2: Build tables for each data_source and category\n",
    "tables = {}\n",
    "for data_source in data_source_list:\n",
    "    for category in categories:\n",
    "        table_rows = []\n",
    "        for algo in algo_list:\n",
    "            subset = data[(data['algo'] == algo) & (data['data_source'] == data_source) & (data['category'] == category)]\n",
    "            if not subset.empty:\n",
    "                # Calculate averages and standard deviations\n",
    "                avg_reward = subset['mean_reward'].mean()\n",
    "                std_reward = subset['std_reward'].mean()\n",
    "                avg_traj_len = subset['mean_traj_len_to_meet_weaning'].mean()\n",
    "                std_traj_len = subset['std_traj_len_without_nonsense_actions'].mean()\n",
    "                avg_meet_weaning = subset['meet_weaning_percentage'].mean()\n",
    "\n",
    "                # Append row to the table\n",
    "                table_rows.append({\n",
    "                    \"RL Algo\": algo,\n",
    "                    \"Avg Total Reward (Std)\": f\"{avg_reward:.2f} ({std_reward:.2f})\",\n",
    "                    \"Avg Length Meet Extubation\": f\"{avg_traj_len:.2f} ({std_traj_len:.2f})\",\n",
    "                    \"Percentage of Meet Extubation\": f\"{avg_meet_weaning:.2f}\"\n",
    "                })\n",
    "\n",
    "        # Create a DataFrame for the table\n",
    "        table = pd.DataFrame(table_rows)\n",
    "        tables[(data_source, category)] = table\n",
    "\n",
    "# Step 3: Display the tables\n",
    "# for (data_source, category), table in tables.items():\n",
    "#     print(f\"Table for Data Source: {data_source}, Category: {category}\")\n",
    "#     print(table)\n",
    "#     print(\"\\n\")\n",
    "for (data_source, category), table in tables.items():\n",
    "    print(f\"Table for Data Source: {data_source}, Category: {category}\")\n",
    "    print(tabulate(table, headers='keys', tablefmt='grid'))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty, With Weaning Reward on Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define the lists\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "categories = [\"small_reward\", \"median_reward\", \"large_reward\"]\n",
    "\n",
    "# Load all data into a single DataFrame\n",
    "data_frames = []\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        file_path = f'../models/training_log/rewards_summary_{algo}_{data_source}.csv'\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['algo'] = algo\n",
    "            df['data_source'] = data_source\n",
    "            data_frames.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Step 1: Find the best epoch for \"small_reward\" category\n",
    "best_epochs = {}\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        subset = data[(data['algo'] == algo) & (data['data_source'] == data_source) & (data['category'] == 'small_reward')]\n",
    "        if not subset.empty:\n",
    "            best_epoch = subset.loc[subset['mean_reward'].idxmax()]\n",
    "            best_epochs[(algo, data_source)] = best_epoch\n",
    "\n",
    "# Step 2: Build tables for each data_source and category\n",
    "tables = {}\n",
    "for data_source in data_source_list:\n",
    "    for category in categories:\n",
    "        table_rows = []\n",
    "        for algo in algo_list:\n",
    "            subset = data[(data['algo'] == algo) & (data['data_source'] == data_source) & (data['category'] == category)]\n",
    "            if not subset.empty:\n",
    "                # Calculate averages and standard deviations\n",
    "                avg_reward = subset['mean_reward'].mean()\n",
    "                std_reward = subset['std_reward'].mean()\n",
    "                avg_traj_len = subset['mean_traj_len_to_meet_weaning'].mean()\n",
    "                std_traj_len = subset['std_traj_len_without_nonsense_actions'].mean()\n",
    "                avg_meet_weaning = subset['meet_weaning_percentage'].mean()\n",
    "\n",
    "                # Append row to the table\n",
    "                table_rows.append({\n",
    "                    \"RL Algo\": algo,\n",
    "                    \"Avg Total Reward (Std)\": f\"{avg_reward:.2f} ({std_reward:.2f})\",\n",
    "                    \"Avg Length Meet Extubation\": f\"{avg_traj_len:.2f} ({std_traj_len:.2f})\",\n",
    "                    \"Percentage of Meet Extubation\": f\"{avg_meet_weaning:.2f}\"\n",
    "                })\n",
    "\n",
    "        # Create a DataFrame for the table\n",
    "        table = pd.DataFrame(table_rows)\n",
    "        tables[(data_source, category)] = table\n",
    "\n",
    "# Step 3: Display the tables\n",
    "# for (data_source, category), table in tables.items():\n",
    "#     print(f\"Table for Data Source: {data_source}, Category: {category}\")\n",
    "#     print(table)\n",
    "#     print(\"\\n\")\n",
    "for (data_source, category), table in tables.items():\n",
    "    print(f\"Table for Data Source: {data_source}, Category: {category}\")\n",
    "    print(tabulate(table, headers='keys', tablefmt='grid'))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Action penalty, With Weaning Reward on Agent, With Same parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define the lists\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "categories = [\"small_reward\", \"median_reward\", \"large_reward\"]\n",
    "\n",
    "# Load all data into a single DataFrame\n",
    "data_frames = []\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        file_path = f'../models/training_log/rewards_summary_{algo}_{data_source}.csv'\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['algo'] = algo\n",
    "            df['data_source'] = data_source\n",
    "            data_frames.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Step 1: Find the best epoch for \"small_reward\" category\n",
    "best_epochs = {}\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        subset = data[(data['algo'] == algo) & (data['data_source'] == data_source) & (data['category'] == 'small_reward')]\n",
    "        if not subset.empty:\n",
    "            best_epoch = subset.loc[subset['mean_reward'].idxmax()]\n",
    "            best_epochs[(algo, data_source)] = best_epoch\n",
    "\n",
    "# Step 2: Build tables for each data_source and category\n",
    "tables = {}\n",
    "for data_source in data_source_list:\n",
    "    for category in categories:\n",
    "        table_rows = []\n",
    "        for algo in algo_list:\n",
    "            subset = data[(data['algo'] == algo) & (data['data_source'] == data_source) & (data['category'] == category)]\n",
    "            if not subset.empty:\n",
    "                # Calculate averages and standard deviations\n",
    "                avg_reward = subset['mean_reward'].mean()\n",
    "                std_reward = subset['std_reward'].mean()\n",
    "                avg_traj_len = subset['mean_traj_len_to_meet_weaning'].mean()\n",
    "                std_traj_len = subset['std_traj_len_without_nonsense_actions'].mean()\n",
    "                avg_meet_weaning = subset['meet_weaning_percentage'].mean()\n",
    "\n",
    "                # Append row to the table\n",
    "                table_rows.append({\n",
    "                    \"RL Algo\": algo,\n",
    "                    \"Avg Total Reward (Std)\": f\"{avg_reward:.2f} ({std_reward:.2f})\",\n",
    "                    \"Avg Length Meet Extubation\": f\"{avg_traj_len:.2f} ({std_traj_len:.2f})\",\n",
    "                    \"Percentage of Meet Extubation\": f\"{avg_meet_weaning:.2f}\"\n",
    "                })\n",
    "\n",
    "        # Create a DataFrame for the table\n",
    "        table = pd.DataFrame(table_rows)\n",
    "        tables[(data_source, category)] = table\n",
    "\n",
    "# Step 3: Display the tables\n",
    "# for (data_source, category), table in tables.items():\n",
    "#     print(f\"Table for Data Source: {data_source}, Category: {category}\")\n",
    "#     print(table)\n",
    "#     print(\"\\n\")\n",
    "for (data_source, category), table in tables.items():\n",
    "    print(f\"Table for Data Source: {data_source}, Category: {category}\")\n",
    "    print(tabulate(table, headers='keys', tablefmt='grid'))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Action penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define the lists\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "categories = [\"small_reward\", \"median_reward\", \"large_reward\"]\n",
    "\n",
    "# Load all data into a single DataFrame\n",
    "data_frames = []\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        file_path = f'../models/training_log/rewards_summary_{algo}_{data_source}.csv'\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['algo'] = algo\n",
    "            df['data_source'] = data_source\n",
    "            data_frames.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Step 1: Find the best epoch for \"small_reward\" category\n",
    "best_epochs = {}\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        subset = data[(data['algo'] == algo) & (data['data_source'] == data_source) & (data['category'] == 'small_reward')]\n",
    "        if not subset.empty:\n",
    "            best_epoch = subset.loc[subset['mean_reward'].idxmax()]\n",
    "            best_epochs[(algo, data_source)] = best_epoch\n",
    "\n",
    "# Step 2: Build tables for each data_source and category\n",
    "tables = {}\n",
    "for data_source in data_source_list:\n",
    "    for category in categories:\n",
    "        table_rows = []\n",
    "        for algo in algo_list:\n",
    "            subset = data[(data['algo'] == algo) & (data['data_source'] == data_source) & (data['category'] == category)]\n",
    "            if not subset.empty:\n",
    "                # Calculate averages and standard deviations\n",
    "                avg_reward = subset['mean_reward'].mean()\n",
    "                std_reward = subset['std_reward'].mean()\n",
    "                avg_traj_len = subset['mean_traj_len_without_nonsense_actions'].mean()\n",
    "                std_traj_len = subset['std_traj_len_without_nonsense_actions'].mean()\n",
    "                avg_meet_weaning = subset['meet_weaning_percentage'].mean()\n",
    "\n",
    "                # Append row to the table\n",
    "                table_rows.append({\n",
    "                    \"RL Algo\": algo,\n",
    "                    \"Avg Total Reward (Std)\": f\"{avg_reward:.2f} ({std_reward:.2f})\",\n",
    "                    \"Avg Length Meet Extubation\": f\"{avg_traj_len:.2f} ({std_traj_len:.2f})\",\n",
    "                    \"Percentage of Meet Extubation\": f\"{avg_meet_weaning:.2f}\"\n",
    "                })\n",
    "\n",
    "        # Create a DataFrame for the table\n",
    "        table = pd.DataFrame(table_rows)\n",
    "        tables[(data_source, category)] = table\n",
    "\n",
    "# Step 3: Display the tables\n",
    "for (data_source, category), table in tables.items():\n",
    "    print(f\"Table for Data Source: {data_source}, Category: {category}\")\n",
    "    print(table)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming `tables` is a dictionary with keys as (data_source, category) and values as DataFrames\n",
    "for (data_source, category), table in tables.items():\n",
    "    print(f\"Table for Data Source: {data_source}, Category: {category}\")\n",
    "    print(tabulate(table, headers='keys', tablefmt='grid'))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of Action Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Define the lists\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "categories = [\"small_reward\", \"median_reward\", \"large_reward\"]\n",
    "\n",
    "# Initialize an empty DataFrame to store all data\n",
    "all_data = []\n",
    "\n",
    "# Load data for each algorithm and data source\n",
    "for algo in algo_list:\n",
    "    for data_source in data_source_list:\n",
    "        file_path = f'../models/training_log/rewards_summary_{algo}_{data_source}.csv'\n",
    "        if os.path.exists(file_path):\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['algo'] = algo\n",
    "            df['data_source'] = data_source\n",
    "            all_data.append(df)\n",
    "\n",
    "# Concatenate all data into a single DataFrame\n",
    "if all_data:\n",
    "    data = pd.concat(all_data, ignore_index=True)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No data files found in the specified paths.\")\n",
    "\n",
    "# Plotting\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(len(data_source_list), len(categories), figsize=(18, 12), sharex=True, sharey=True)\n",
    "\n",
    "for i, data_source in enumerate(data_source_list):\n",
    "    for j, category in enumerate(categories):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        # Filter data for the current data_source and category\n",
    "        subset = data[(data['data_source'] == data_source) & (data['category'] == category)]\n",
    "        \n",
    "        # Plot action diversity for each algorithm\n",
    "        for algo in algo_list:\n",
    "            algo_data = subset[subset['algo'] == algo]\n",
    "            ax.plot(algo_data['epoch'], algo_data['action_diversity'], label=algo, marker='o')\n",
    "        \n",
    "        # Set plot title and labels\n",
    "        ax.set_title(f\"{data_source} - {category}\", fontsize=12)\n",
    "        ax.set_xlabel(\"Epoch\", fontsize=10)\n",
    "        ax.set_ylabel(\"Action Diversity\", fontsize=10)\n",
    "        ax.legend(fontsize=8, loc='upper right')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark by input trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trajectory_metrics_for_benchmark(df, state_cols, action_cols, baseline_cols, \n",
    "                               min_max_values_guidelines_bin_dict,\n",
    "                               obs_hrs=3, stable_hr=2, stable_threshold=2, \n",
    "                               timestamp_reward_step=1, max_timestamp=48):\n",
    "    \"\"\"\n",
    "    Compute trajectory metrics per stay:\n",
    "      - Uses the same penalty logic as the environment.\n",
    "      - Clips trajectory when weaning (6 consecutive zero-penalty steps) occurs.\n",
    "      - Calculates total reward, steps to weaning for each stay.\n",
    "      - Prints summary: reward distribution, average steps, and proportion not weaned.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    metrics_list = []\n",
    "    total_rewards = []\n",
    "    steps_list = []\n",
    "    steps_meet_weaning_list = []\n",
    "    num_stays = 0\n",
    "    num_not_weaned = 0\n",
    "    single_agent_action_diversity_set = set()\n",
    "    \n",
    "    for stay_id, group in df.groupby('stay_id'):\n",
    "        group = group.sort_values('hours_in')\n",
    "        if len(group) < obs_hrs + 1:\n",
    "            continue\n",
    "        \n",
    "        rows = group.to_dict('records')\n",
    "        # Seed history\n",
    "        history = []\n",
    "        for i in range(obs_hrs - 1):\n",
    "            history.append((tuple(rows[i][col] for col in state_cols),\n",
    "                            tuple(rows[i][col] for col in action_cols)))\n",
    "        \n",
    "        current_state = tuple(rows[obs_hrs-1][col] for col in state_cols)\n",
    "        previous_action = tuple(rows[obs_hrs-2][col] for col in action_cols) # TODO: [update reward]\n",
    "        current_action = tuple(rows[obs_hrs-2][col] for col in action_cols) # TODO: [update reward]\n",
    "        total_reward = 0\n",
    "        weaning_count = 0\n",
    "        weaning_met = False\n",
    "        steps_to_weaning = None\n",
    "        action_diversity_set = set()\n",
    "        anomaly_action = False\n",
    "        \n",
    "        # TODO: [update reward Part 2]\n",
    "        # Iterate through trajectory\n",
    "        for t in range(obs_hrs - 1, len(rows) - 1):\n",
    "            if weaning_met:\n",
    "                break\n",
    "            step = t - (obs_hrs - 1)  # zero-index step\n",
    "            previous_action = current_action\n",
    "            current_action = tuple(rows[t][col] for col in action_cols)\n",
    "            next_state = tuple(rows[t+1][col] for col in state_cols)\n",
    "\n",
    "            action_diversity_set.add(current_action)\n",
    "            single_agent_action_diversity_set.add(current_action)\n",
    "            \n",
    "            # State penalty, This is used to check, not to calculate reward\n",
    "            state_penalty = -1 if any(\n",
    "                not (min_max_values_guidelines_bin_dict[col]['min'] <= next_state[i] <= \n",
    "                     min_max_values_guidelines_bin_dict[col]['max'])\n",
    "                for i, col in enumerate(state_cols)\n",
    "            ) else 0\n",
    "\n",
    "            state_reward = sum(\n",
    "                1 if min_max_values_guidelines_bin_dict[col]['min'] <= next_state[i] <= min_max_values_guidelines_bin_dict[col]['max'] else 0\n",
    "                for i, col in enumerate(state_cols)\n",
    "            ) / len(state_cols)\n",
    "            \n",
    "            # Action penalty\n",
    "            # action_penalty = -1 if any(\n",
    "            #     not (min_max_values_guidelines_bin_dict[col]['min'] <= a <= \n",
    "            #          min_max_values_guidelines_bin_dict[col]['max'])\n",
    "            #     for a, col in zip(current_action, action_cols)\n",
    "            # ) else 0\n",
    "            action_penalty = sum(\n",
    "                -1 if current_action[i] < min_max_values_guidelines_bin_dict[col]['min'] or current_action[i] > min_max_values_guidelines_bin_dict[col]['max'] else 0\n",
    "                for i, col in enumerate(action_cols)\n",
    "            ) / len(action_cols)\n",
    "            \n",
    "            # Stable penalty\n",
    "            # stable_penalty = 0\n",
    "            # if len(history) >= stable_hr:\n",
    "            #     stable = True\n",
    "            #     for i in range(-stable_hr + 1, 0):\n",
    "            #         for j in range(len(state_cols)):\n",
    "            #             diff = abs(history[i][0][j] - history[i-1][0][j])\n",
    "            #             if diff >= stable_threshold:\n",
    "            #                 stable = False\n",
    "            #                 break\n",
    "            #         if not stable:\n",
    "            #             break\n",
    "            #     if stable:\n",
    "            #         last_state = history[-1][0]\n",
    "            #         for j in range(len(state_cols)):\n",
    "            #             diff = abs(current_state[j] - last_state[j])\n",
    "            #             if diff >= stable_threshold:\n",
    "            #                 stable = False\n",
    "            #                 break\n",
    "            #     stable_penalty = -1 if not stable else 0\n",
    "\n",
    "            state_stable_penalty = sum(\n",
    "                -1 if abs(next_state[i] - current_state[i]) >= stable_threshold else 0\n",
    "                for i, col in enumerate(state_cols)\n",
    "            ) / len(state_cols)\n",
    "            \n",
    "            action_stable_penalty = sum(\n",
    "                -1 if abs(previous_action[i] - current_action[i]) >= stable_threshold * 2 else 0\n",
    "                for i, col in enumerate(action_cols)\n",
    "            ) / len(action_cols)\n",
    "\n",
    "            # Timestamp penalty\n",
    "            timestamp_penalty = -1 if (step + 1) % timestamp_reward_step == 0 else 0\n",
    "            \n",
    "            # Accumulate reward\n",
    "            if ACTION_PENALTY_FLAG:\n",
    "                reward = state_reward + action_penalty + state_stable_penalty + action_stable_penalty + timestamp_penalty\n",
    "            else:\n",
    "                reward = state_reward + state_stable_penalty + action_stable_penalty + timestamp_penalty\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Check weaning condition\n",
    "            # if state_penalty == 0 and action_penalty == 0 and state_stable_penalty == 0:\n",
    "            if state_penalty == 0 and action_penalty == 0 and state_stable_penalty == 0 and action_stable_penalty == 0: # TODO: try add action_stable_penalty\n",
    "                weaning_count += 1\n",
    "            else:\n",
    "                weaning_count = 0\n",
    "            \n",
    "            # if weaning_count >= 6: # TODO: 6 is set by my self\n",
    "            # if weaning_count >= 6 and step >= 23: # TODO: 6 is set by my self # TODO: [update reward]\n",
    "            if weaning_count >= 6: # TODO: 6 is set by my self\n",
    "                # total_reward += 10 # TODO: [update reward]\n",
    "                if WEANING_REWARD_FLAG:\n",
    "                    total_reward += 10\n",
    "                weaning_met = True\n",
    "                steps_to_weaning = step + 1\n",
    "                break\n",
    "            # [update reward] TODO: elif last step and weaning_cout >= 2\n",
    "            # elif step == len(rows) - obs_hrs - 1 and weaning_count >= 2:\n",
    "            #     total_reward += 100\n",
    "            #     weaning_met = True\n",
    "            #     steps_to_weaning = step + 1\n",
    "\n",
    "            # Update history and state for next iteration\n",
    "            history = history[1:] + [(current_state, current_action)]\n",
    "            current_state = next_state\n",
    "        \n",
    "        # If not weaned, mark steps_to_weaning as total steps\n",
    "        if not weaning_met:\n",
    "            steps_to_weaning = len(rows) - obs_hrs\n",
    "            num_not_weaned += 1\n",
    "            if steps_to_weaning < max_timestamp:\n",
    "                anomaly_action = True\n",
    "            total_reward -= 10  # TODO: [update reward] penalty for not weaning\n",
    "        \n",
    "        metrics_list.append({\n",
    "            'stay_id': stay_id,\n",
    "            'total_reward': total_reward,\n",
    "            'meet_weaning': weaning_met,\n",
    "            'steps_to_weaning': steps_to_weaning,\n",
    "            'anomaly_action': anomaly_action,\n",
    "            'action_diversity': len(action_diversity_set),\n",
    "            'action': action_diversity_set\n",
    "        })\n",
    "        total_rewards.append(total_reward)\n",
    "        steps_list.append(steps_to_weaning)\n",
    "        if weaning_met:\n",
    "            steps_meet_weaning_list.append(steps_to_weaning)\n",
    "        num_stays += 1\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_list) # every trajectory is a row\n",
    "    \n",
    "    # Summary statistics\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    std_reward = np.std(total_rewards)\n",
    "    prop_weaned = metrics_df['meet_weaning'].mean() if num_stays > 0 else None\n",
    "    avg_steps = np.mean(steps_list)\n",
    "    std_steps = np.std(steps_list)\n",
    "    avg_steps_meet_weaning = np.mean(steps_meet_weaning_list) if steps_meet_weaning_list else None\n",
    "    std_steps_meet_weaning = np.std(steps_meet_weaning_list) if steps_meet_weaning_list else None\n",
    "    action_diversity = len(single_agent_action_diversity_set)\n",
    "    anomaly_action = len(metrics_df[metrics_df['anomaly_action']]) / num_stays if num_stays > 0 else None\n",
    "\n",
    "    prop_not_weaned = num_not_weaned / num_stays if num_stays > 0 else None\n",
    "    \n",
    "    # print(f\"Total stays evaluated: {num_stays}\")\n",
    "    # print(f\"Reward (mean  std): {avg_reward:.2f}  {std_reward:.2f}\")\n",
    "    # print(f\"Proportion weaned: {prop_weaned:.2%}\")\n",
    "    # print(f\"Average steps (mean  std): {avg_steps:.2f}  {std_steps:.2f}\")\n",
    "    # print(f\"Average steps to weaning (mean  std): {avg_steps_meet_weaning:.2f}  {std_steps_meet_weaning:.2f}\")\n",
    "    # print(f\"Action diversity: {action_diversity}\")\n",
    "    # print(f\"Proportion of anomaly actions: {anomaly_action:.2%}\")\n",
    "\n",
    "    benchmark_summary = {\n",
    "        'total_stays': num_stays,\n",
    "        'avg_reward': avg_reward,\n",
    "        'std_reward': std_reward,\n",
    "        'prop_weaned': prop_weaned,\n",
    "        'avg_steps': avg_steps,\n",
    "        'std_steps': std_steps,\n",
    "        'avg_steps_meet_weaning': avg_steps_meet_weaning,\n",
    "        'std_steps_meet_weaning': std_steps_meet_weaning,\n",
    "        'action_diversity': action_diversity,\n",
    "        'anomaly_action_proportion': anomaly_action,\n",
    "    }\n",
    "    \n",
    "    return benchmark_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_df = compute_trajectory_metrics(train_df, state_cols, action_cols, baseline_cols, min_max_values_guidelines_bin_dict)\n",
    "# metrics_train_df = compute_trajectory_metrics(train_df, state_cols, action_cols, baseline_cols, min_max_values_guidelines_bin_dict)\n",
    "# metrics_test_df = compute_trajectory_metrics(test_df, state_cols, action_cols, baseline_cols, min_max_values_guidelines_bin_dict)\n",
    "# metrics_eICU_df = compute_trajectory_metrics(eICU_disc, state_cols, action_cols, baseline_cols, min_max_values_guidelines_bin_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_traj_path_folder_list = [\n",
    "    \"2025_06_08_with_action_penalty_training_log\",      # tune hyperparameters, with action penalty, without weaning reward\n",
    "    \"2025_06_09_without_action_penalty_training_log\",   # tune hyperparameters, without action penalty, without weaning reward\n",
    "    \"2025_06_09_without_action_penalty_with_weaning_reward_training_log\", # tune hyperparameters, without action penalty, with weaning reward\n",
    "    \"2025_06_09_without_action_penalty_with_weaning_reward_same_parameter_settings_v1_training_log\",    # same hyperparameters, without action penalty, with weaning reward\n",
    "    \"2025_06_10_without_action_penalty_with_weaning_reward_default_parameter_settings_v1_training_log\"  # default hyperparameters, without action penalty, with weaning reward\n",
    "    ]\n",
    "# algo_list = [\"DQN\", \"DoubleDQN\"]\n",
    "algo_list = [\"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\", \"DiscreteBC\", \"NFQ\"]\n",
    "# data_source_list = [\"train\", \"test\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "# train_results = {}\n",
    "# test_results = {}\n",
    "# eICU_results = {}\n",
    "# small_reward_benchmark_results = {}\n",
    "benchmark_results_df = pd.DataFrame()\n",
    "for gen_traj_path_folder in gen_traj_path_folder_list:\n",
    "    for data_source in data_source_list:\n",
    "        for algo in algo_list:\n",
    "            file_path = f\"../models/{gen_traj_path_folder}/eval_trajectories_{algo}_{data_source}_epoch19.csv\"\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "            # get three sub df, as 19_small_reward_{id}, 19_median_reward_{id}, 19_large_reward_{id}\n",
    "            # if stay_id include \"small_reward\", \"median_reward\", \"large_reward\"\n",
    "            small_reward_df = df[df['stay_id'].str.contains(\"small_reward\")]\n",
    "            median_reward_df = df[df['stay_id'].str.contains(\"median_reward\")]\n",
    "            large_reward_df = df[df['stay_id'].str.contains(\"large_reward\")]\n",
    "            # Compute metrics for each category\n",
    "            # print(f\"Metrics for {algo} on {data_source} in {gen_traj_path_folder} - Small Reward Category:\")\n",
    "            metrics_small_reward_df = compute_trajectory_metrics_for_benchmark(\n",
    "                small_reward_df, state_cols, action_cols, baseline_cols, \n",
    "                min_max_values_guidelines_bin_dict\n",
    "            )\n",
    "            # print(f\"Metrics for {algo} on {data_source} in {gen_traj_path_folder} - Median Reward Category:\")\n",
    "            metrics_median_reward_df = compute_trajectory_metrics_for_benchmark(\n",
    "                median_reward_df, state_cols, action_cols, baseline_cols, \n",
    "                min_max_values_guidelines_bin_dict\n",
    "            )\n",
    "            # print(f\"Metrics for {algo} on {data_source} in {gen_traj_path_folder} - Large Reward Category:\")\n",
    "            metrics_large_reward_df = compute_trajectory_metrics_for_benchmark(\n",
    "                large_reward_df, state_cols, action_cols, baseline_cols, \n",
    "                min_max_values_guidelines_bin_dict\n",
    "            )\n",
    "            metrics_small_reward_df['algo'] = algo\n",
    "            metrics_small_reward_df['data_source'] = data_source\n",
    "            metrics_small_reward_df['category'] = 'small_reward'\n",
    "            metrics_small_reward_df['train_settings'] = gen_traj_path_folder\n",
    "            metrics_median_reward_df['algo'] = algo\n",
    "            metrics_median_reward_df['data_source'] = data_source\n",
    "            metrics_median_reward_df['category'] = 'median_reward'\n",
    "            metrics_median_reward_df['train_settings'] = gen_traj_path_folder\n",
    "            metrics_large_reward_df['algo'] = algo\n",
    "            metrics_large_reward_df['data_source'] = data_source\n",
    "            metrics_large_reward_df['category'] = 'large_reward'\n",
    "            metrics_large_reward_df['train_settings'] = gen_traj_path_folder\n",
    "            benchmark_results_df = pd.concat([\n",
    "                benchmark_results_df, \n",
    "                pd.DataFrame([metrics_small_reward_df]),\n",
    "                pd.DataFrame([metrics_median_reward_df]),\n",
    "                pd.DataFrame([metrics_large_reward_df])\n",
    "            ], ignore_index=True)\n",
    "benchmark_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results_df[(benchmark_results_df[\"category\"] == \"small_reward\") & (benchmark_results_df[\"data_source\"] == \"test\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results_df.to_csv(\"../models/benchmark_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark_results_df[(benchmark_results_df[\"category\"] == \"small_reward\") & (benchmark_results_df[\"data_source\"] == \"train\")]\n",
    "# benchmark_results_df[(benchmark_results_df[\"category\"] == \"large_reward\") & (benchmark_results_df[\"data_source\"] == \"train\")]\n",
    "# benchmark_results_df[(benchmark_results_df[\"category\"] == \"small_reward\") & (benchmark_results_df[\"data_source\"] == \"test\")]\n",
    "# benchmark_results_df[(benchmark_results_df[\"category\"] == \"median_reward\") & (benchmark_results_df[\"data_source\"] == \"test\")]\n",
    "# benchmark_results_df[(benchmark_results_df[\"category\"] == \"large_reward\") & (benchmark_results_df[\"data_source\"] == \"test\")]\n",
    "# benchmark_results_df[(benchmark_results_df[\"category\"] == \"median_reward\") & (benchmark_results_df[\"data_source\"] == \"eICU\")]\n",
    "# benchmark_results_df[(benchmark_results_df[\"category\"] == \"small_reward\") & (benchmark_results_df[\"data_source\"] == \"train\") & (benchmark_results_df[\"algo\"] == \"DiscreteCQL\")]\n",
    "# benchmark_results_df[(benchmark_results_df[\"category\"] == \"median_reward\") & (benchmark_results_df[\"data_source\"] == \"test\") & (benchmark_results_df[\"algo\"] == \"DiscreteCQL\")]\n",
    "benchmark_results_df[(benchmark_results_df[\"category\"] == \"small_reward\") & (benchmark_results_df[\"data_source\"] == \"test\") & (benchmark_results_df[\"algo\"] == \"DiscreteBC\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark_results_df[(benchmark_results_df[\"category\"] == \"small_reward\") & (benchmark_results_df[\"data_source\"] == \"test\") & (benchmark_results_df[\"algo\"] == \"DQN\")]\n",
    "benchmark_results_df[(benchmark_results_df[\"category\"] == \"small_reward\") & (benchmark_results_df[\"data_source\"] == \"test\") & (benchmark_results_df[\"algo\"] == \"DQN\")]\n",
    "# benchmark_results_df[(benchmark_results_df[\"category\"] == \"small_reward\") & (benchmark_results_df[\"data_source\"] == \"eICU\") & (benchmark_results_df[\"algo\"] == \"DiscreteBC\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results_df[(benchmark_results_df[\"category\"] == \"small_reward\") & (benchmark_results_df[\"data_source\"] == \"test\") & (benchmark_results_df[\"algo\"] == \"DiscreteSAC\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results_df[(benchmark_results_df[\"category\"] == \"large_reward\") & (benchmark_results_df[\"data_source\"] == \"eICU\") & (benchmark_results_df[\"train_settings\"] == \"2025_06_09_without_action_penalty_with_weaning_reward_same_parameter_settings_v1_training_log\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_large_reward_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train_df_v1 = compute_trajectory_metrics_for_benchmark(train_df, state_cols, action_cols, baseline_cols, min_max_values_guidelines_bin_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_formatted_results_table(benchmark_results_df, category, data_source):\n",
    "    # Filter the dataframe\n",
    "    filtered_df = benchmark_results_df[\n",
    "        (benchmark_results_df[\"category\"] == category) & \n",
    "        (benchmark_results_df[\"data_source\"] == data_source)\n",
    "    ]\n",
    "    \n",
    "    # Define the desired order of algorithms\n",
    "    algo_order = ['DiscreteBC', 'NFQ', 'DQN', 'DoubleDQN', 'DiscreteSAC', 'DiscreteBCQ', 'DiscreteCQL']\n",
    "    algo_mapping = {\n",
    "        'DiscreteBC': 'BC',\n",
    "        'NFQ': 'NFQ', \n",
    "        'DQN': 'DQN',\n",
    "        'DoubleDQN': 'DDQN',\n",
    "        'DiscreteSAC': 'SAC',\n",
    "        'DiscreteBCQ': 'BCQ',\n",
    "        'DiscreteCQL': 'CQL'\n",
    "    }\n",
    "    \n",
    "    # Create the formatted dataframe\n",
    "    formatted_data = []\n",
    "    \n",
    "    for algo in algo_order:\n",
    "        if algo in filtered_df['algo'].values:\n",
    "            row_data = filtered_df[filtered_df['algo'] == algo].iloc[0]\n",
    "            \n",
    "            # Format each column according to your specifications\n",
    "            total_reward = f\"{row_data['avg_reward']:.2f} ({row_data['std_reward']:.2f})\"\n",
    "            \n",
    "            meet_extubation = f\"{row_data['prop_weaned']*100:.0f}%\"\n",
    "            \n",
    "            avg_hours = f\"{row_data['avg_steps']:.2f} ({row_data['std_steps']:.2f})\"\n",
    "            \n",
    "            # Handle NaN values for avg_steps_meet_weaning\n",
    "            if pd.isna(row_data['avg_steps_meet_weaning']) or pd.isna(row_data['std_steps_meet_weaning']):\n",
    "                avg_hours_to_meet = \"NaN (NaN)\"\n",
    "            else:\n",
    "                avg_hours_to_meet = f\"{row_data['avg_steps_meet_weaning']:.2f} ({row_data['std_steps_meet_weaning']:.2f})\"\n",
    "            \n",
    "            action_diversity = f\"{row_data['action_diversity']}\"\n",
    "            \n",
    "            anomaly_action = f\"{row_data['anomaly_action_proportion']*100:.0f}%\"\n",
    "            \n",
    "            formatted_data.append({\n",
    "                f\"{data_source} set / {category}\": algo_mapping[algo],\n",
    "                \"Total Reward\": total_reward,\n",
    "                \"Meet Extubation\": meet_extubation,\n",
    "                \"Avg hours\": avg_hours,\n",
    "                \"Avg hours to meet\": avg_hours_to_meet,\n",
    "                \"Action diversity\": action_diversity,\n",
    "                \"Anomaly action\": anomaly_action\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "category = \"small_reward\" # \"large_reward\" # \"median_reward\" # \"small_reward\"\n",
    "data_source = \"eICU\" # \"test\" # \"train\"\n",
    "formatted_table = create_formatted_results_table(benchmark_results_df, category, data_source)\n",
    "formatted_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "category = \"small_reward\" # \"large_reward\" # \"median_reward\" # \"small_reward\"\n",
    "data_source = \"test\" # \"test\" # \"train\"\n",
    "formatted_table = create_formatted_results_table(benchmark_results_df, category, data_source)\n",
    "formatted_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "category = \"small_reward\" # \"large_reward\" # \"median_reward\" # \"small_reward\"\n",
    "data_source = \"eICU\" # \"test\" # \"train\"\n",
    "formatted_table = create_formatted_results_table(benchmark_results_df, category, data_source)\n",
    "formatted_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating with PatientEnvironment and D3RLPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo_train_dict = {\n",
    "#     'DQN_train': agent_dqn_train,\n",
    "#     'DoubleDQN_train': agent_ddqn_train,\n",
    "#     'DiscreteSAC_train': agent_sac_train,\n",
    "#     'DiscreteBCQ_train': agent_bcq_train,\n",
    "#     'DiscreteCQL_train': agent_cql_train\n",
    "# }\n",
    "# algo_test_dict = {\n",
    "#     'DQN_test': agent_dqn_test,\n",
    "#     'DoubleDQN_test': agent_ddqn_test,\n",
    "#     'DiscreteSAC_test': agent_sac_test,\n",
    "#     'DiscreteBCQ_test': agent_bcq_test,\n",
    "#     'DiscreteCQL_test': agent_cql_test\n",
    "# }\n",
    "# algo_eICU_dict = {\n",
    "#     'DQN_eICU': agent_dqn_eICU,\n",
    "#     'DoubleDQN_eICU': agent_ddqn_eICU,\n",
    "#     'DiscreteSAC_eICU': agent_sac_eICU,\n",
    "#     'DiscreteBCQ_eICU': agent_bcq_eICU,\n",
    "#     'DiscreteCQL_eICU': agent_cql_eICU\n",
    "# }\n",
    "algo_train_dict = {\n",
    "    'DQN_train': agent_dqn_train,\n",
    "    'DDQN_train': agent_ddqn_train,\n",
    "    'SAC_train': agent_sac_train,\n",
    "    'BCQ_train': agent_bcq_train,\n",
    "    'CQL_train': agent_cql_train,\n",
    "    'BC_train': agent_bc_train\n",
    "}\n",
    "algo_test_dict = {\n",
    "    'DQN_test': agent_dqn_test,\n",
    "    'DDQN_test': agent_ddqn_test,\n",
    "    'SAC_test': agent_sac_test,\n",
    "    'BCQ_test': agent_bcq_test,\n",
    "    'CQL_test': agent_cql_test,\n",
    "    'BC_test': agent_bc_test\n",
    "}\n",
    "algo_eICU_dict = {\n",
    "    'DQN_eICU': agent_dqn_eICU,\n",
    "    'DDQN_eICU': agent_ddqn_eICU,\n",
    "    'SAC_eICU': agent_sac_eICU,\n",
    "    'BCQ_eICU': agent_bcq_eICU,\n",
    "    'CQL_eICU': agent_cql_eICU,\n",
    "    'BC_eICU': agent_bc_eICU\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_counts = train_df[['fio2', 'respiratory_rate_set']].value_counts()\n",
    "print(action_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PatientEnvironment(train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_format(obs):\n",
    "    \"\"\"Reset the environment to an initial state.\"\"\"\n",
    "    # Helper function to convert np.int32 to plain integers\n",
    "    def to_plain_int(value):\n",
    "        return int(value) if isinstance(value, np.integer) else value\n",
    "\n",
    "    # Convert all elements in obs to plain integers\n",
    "    obs = [to_plain_int(x) for x in obs]\n",
    "\n",
    "    state_len = len(state_cols)\n",
    "    action_len = len(action_cols)\n",
    "    baseline_len = len(baseline_cols)\n",
    "\n",
    "    # Split the data into states, actions, and baselines\n",
    "    num_states = (len(obs) - baseline_len) // (state_len + action_len)\n",
    "    states = []\n",
    "    actions = []\n",
    "    for i in range(num_states):\n",
    "        start_idx = i * (state_len + action_len)\n",
    "        states.append(tuple(obs[start_idx:start_idx + state_len]))\n",
    "        actions.append(tuple(obs[start_idx + state_len:start_idx + state_len + action_len]))\n",
    "    # last state is the current state\n",
    "    current_state = tuple(obs[-(state_len + baseline_len):-baseline_len])\n",
    "    states.append(current_state)\n",
    "    # Extract baselines\n",
    "    baselines = tuple(obs[-baseline_len:])\n",
    "\n",
    "    # Create the final format\n",
    "    result = []\n",
    "    for i in range(num_states + 1):\n",
    "        # Use the last action if the current action is missing\n",
    "        action = actions[i] if i < len(actions) else None\n",
    "        result.append((states[i], action))\n",
    "    \n",
    "    # Add the baselines\n",
    "    result.append(baselines)\n",
    "\n",
    "    obs = tuple(result)\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "print(obs)\n",
    "obs_format(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Actions with different Agent algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    for _ in range(1):\n",
    "        print(f\"obs: {obs_format(obs)}\")\n",
    "        for algo_name, agent in algo_train_dict.items():\n",
    "            action = agent.predict(np.expand_dims(obs, axis=0))[0]  # Add batch dimension\n",
    "            print(f\"{algo_name}: \\t{env.id_to_action[action]}\")\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    for _ in range(1):\n",
    "        print(f\"obs: {obs_format(obs)}\")\n",
    "        for algo_name, agent in algo_test_dict.items():\n",
    "            action = agent.predict(np.expand_dims(obs, axis=0))[0]  # Add batch dimension\n",
    "            print(f\"{algo_name}: \\t{env.id_to_action[action]}\")\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    for _ in range(1):\n",
    "        print(f\"obs: {obs_format(obs)}\")\n",
    "        for algo_name, agent in algo_eICU_dict.items():\n",
    "            action = agent.predict(np.expand_dims(obs, axis=0))[0]  # Add batch dimension\n",
    "            print(f\"{algo_name}: \\t{env.id_to_action[action]}\")\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_init_obs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate agents on special init categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_agents_and_visualize(categories, algo_dict, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs):\n",
    "    \"\"\"\n",
    "    Evaluate all agents across different categories and visualize the results.\n",
    "\n",
    "    Args:\n",
    "        categories (dict): Dictionary of categories with initial observations.\n",
    "        algo_dict (dict): Dictionary of agents to evaluate.\n",
    "        train_transitions: Training transitions for the environment.\n",
    "        state_cols: State columns for the environment.\n",
    "        action_cols: Action columns for the environment.\n",
    "        baseline_cols: Baseline columns for the environment.\n",
    "        num_bins_dict: Dictionary of binning information for the environment.\n",
    "        min_max_values_guidelines_bin_dict: Min-max values for binning.\n",
    "        obs_hrs: Observation hours for the environment.\n",
    "\n",
    "    Returns:\n",
    "        results (dict): Dictionary of results with average rewards for each agent and category.\n",
    "    \"\"\"\n",
    "    results = {agent_name: {} for agent_name in algo_dict.keys()}\n",
    "\n",
    "    for agent_name, agent in algo_dict.items():\n",
    "        print(f\"Evaluating agent: {agent_name}\")\n",
    "        for category, init_obs_list in categories.items():\n",
    "            if not init_obs_list:\n",
    "                print(f\"Skipping {category}: No valid initial observations\")\n",
    "                results[agent_name][category] = None\n",
    "                continue\n",
    "\n",
    "            env = PatientEnvironment(train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs)\n",
    "\n",
    "            # Evaluate the agent\n",
    "            episode_returns = []\n",
    "            for idx in range(min(1000, len(init_obs_list))):\n",
    "                obs = env.reset(init_obs_list[idx])\n",
    "                done = False\n",
    "                total_reward = 0\n",
    "                while not done:\n",
    "                    action = agent.predict(np.expand_dims(obs, axis=0))[0]\n",
    "                    obs, reward, done, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                episode_returns.append(total_reward)\n",
    "\n",
    "            if len(episode_returns) == 0:\n",
    "                print(f\"Skipping {category}: No valid episode returns\")\n",
    "                results[agent_name][category] = None\n",
    "                continue\n",
    "\n",
    "            avg_reward = np.mean(episode_returns)\n",
    "            results[agent_name][category] = avg_reward\n",
    "            print(f\"{agent_name} - {category}: Average Total Reward = {avg_reward}\")\n",
    "\n",
    "    # Visualization\n",
    "    visualize_agents_performance_results(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def visualize_agents_performance_results(results):\n",
    "    \"\"\"\n",
    "    Visualize the results using a heatmap.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Dictionary of results with average rewards for each agent and category.\n",
    "    \"\"\"\n",
    "    # Convert results to a DataFrame for easier visualization\n",
    "    import pandas as pd\n",
    "    results_df = pd.DataFrame(results).T  # Transpose to have agents as rows and categories as columns\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(results_df, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar_kws={'label': 'Average Reward'})\n",
    "    plt.title(\"Agent Performance Across Categories\")\n",
    "    plt.xlabel(\"Categories\")\n",
    "    plt.ylabel(\"Agents\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# categories = {\n",
    "#     'small_reward': small_reward_init_obs,\n",
    "#     'median_reward': median_reward_init_obs,\n",
    "#     'large_reward': large_reward_init_obs,\n",
    "#     'short_to_wean': short_to_wean_init_obs,\n",
    "#     'median_to_wean': median_to_wean_init_obs,\n",
    "#     'long_to_wean': long_to_wean_init_obs,\n",
    "#     'common': common_init_obs\n",
    "# }\n",
    "\n",
    "# Filter out None values from init_obs lists\n",
    "# categories = {k: [obs for obs in v if obs is not None] for k, v in categories.items()}\n",
    "\n",
    "# Combine all agent dictionaries into one for evaluation\n",
    "all_agents = {**algo_train_dict, **algo_test_dict, **algo_eICU_dict}\n",
    "\n",
    "# Evaluate and visualize\n",
    "results = evaluate_agents_and_visualize(eval_init_obs_categories, all_agents, train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the trained agent (example)\n",
    "# def get_newest_json(model_dir):\n",
    "#     json_files = [f for f in os.listdir(model_dir) if f.endswith('.json')]\n",
    "#     return max(json_files, key=lambda x: os.path.getmtime(os.path.join(model_dir, x)))\n",
    "\n",
    "\n",
    "# model_dir = \"../models\"\n",
    "# os.makedirs(model_dir, exist_ok=True)\n",
    "# model_path = os.path.join(model_dir, f\"{algo}_{data_source}.pkl\")\n",
    "\n",
    "\n",
    "# model_dir = \"path_to_your_model\"\n",
    "# params_file = get_newest_json(model_dir)\n",
    "# algo = d3rlpy.algos.DQN.from_json(os.path.join(model_dir, params_file))\n",
    "# algo.load_model(os.path.join(model_dir, \"model.pt\"))\n",
    "\n",
    "\n",
    "# TODO: Load the model by function, given the algo name and data source\n",
    "# algo = agent_dqn\n",
    "# algo = agent_bcq_test\n",
    "# algo = agent_dqn_train\n",
    "algo = agent_bcq_train\n",
    "# algo = naive_agent\n",
    "\n",
    "\n",
    "# # Evaluate per category\n",
    "# categories = {\n",
    "#     'small_reward': small_reward_init_obs,\n",
    "#     'median_reward': median_reward_init_obs,\n",
    "#     'large_reward': large_reward_init_obs,\n",
    "#     'short_to_wean': short_to_wean_init_obs,\n",
    "#     'median_to_wean': median_to_wean_init_obs,\n",
    "#     'long_to_wean': long_to_wean_init_obs,\n",
    "#     'common': common_init_obs\n",
    "# }\n",
    "\n",
    "# results = {}\n",
    "# for category, init_obs_list in categories.items():\n",
    "#     env = FixedInitialObsEnv(initial_obs_list=init_obs_list, base_env_class=PatientEnvironment)\n",
    "#     # Run 1000 episodes or as many as init_obs_list allows\n",
    "#     avg_reward = d3rlpy.metrics.evaluate_on_environment(env, n_trials=min(1000, len(init_obs_list)))(algo)\n",
    "#     results[category] = avg_reward\n",
    "#     print(f\"{category}: Average Total Reward = {avg_reward}\")\n",
    "\n",
    "# # Alternative: Per patient evaluation\n",
    "# for category, init_obs_list in categories.items():\n",
    "#     stay_ids = [metrics_df[metrics_df['stay_id'].isin(patients)]['stay_id'] for patients in [small_reward_patients, median_reward_patients, large_reward_patients, short_to_wean_patients, median_to_wean_patients, long_to_wean_patients]][list(categories.keys()).index(category)]\n",
    "#     rewards = [evaluate_on_patient(PatientEnvironment, sid, algo) for sid in stay_ids]\n",
    "#     avg_reward = np.mean(rewards)\n",
    "#     results[category] = avg_reward\n",
    "#     print(f\"{category}: Average Total Reward = {avg_reward}\")\n",
    "\n",
    "import d3rlpy\n",
    "import numpy as np\n",
    "\n",
    "# Define categories (from previous response)\n",
    "# categories = {\n",
    "#     'small_reward': small_reward_init_obs,\n",
    "#     'median_reward': median_reward_init_obs,\n",
    "#     'large_reward': large_reward_init_obs,\n",
    "#     'short_to_wean': short_to_wean_init_obs,\n",
    "#     'median_to_wean': median_to_wean_init_obs,\n",
    "#     'long_to_wean': long_to_wean_init_obs,\n",
    "#     'common': common_init_obs\n",
    "# }\n",
    "\n",
    "# # Filter out None values from init_obs lists\n",
    "# categories = {k: [obs for obs in v if obs is not None] for k, v in categories.items()}\n",
    "\n",
    "# Evaluation with FixedInitialObsEnv\n",
    "results = {}\n",
    "for category, init_obs_list in eval_init_obs_categories.items():\n",
    "    if not init_obs_list:\n",
    "        print(f\"Skipping {category}: No valid initial observations\")\n",
    "        results[category] = None\n",
    "        continue\n",
    "\n",
    "    env = PatientEnvironment(train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict, obs_hrs)\n",
    "\n",
    "    # Run up to 1000 episodes or as many as init_obs_list allows\n",
    "    # avg_reward = d3rlpy.metrics.scorer.evaluate_on_environment(env, n_trials=min(1000, len(init_obs_list)))(algo)\n",
    "    # implementation of evaluate_on_environment\n",
    "    episode_returns = []\n",
    "    for idx in range(min(1000, len(init_obs_list))):\n",
    "        # print(f\"init state: {init_obs_list[idx]}\")\n",
    "        obs = env.reset(init_obs_list[idx])\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = algo.predict(np.expand_dims(obs, axis=0))[0]\n",
    "            # print(f\"action: {action}\")\n",
    "            # print(f\"action: {env.id_to_action[action]}\")\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        episode_returns.append(total_reward)\n",
    "    avg_reward = np.mean(episode_returns)\n",
    "    if len(episode_returns) == 0:\n",
    "        print(f\"Skipping {category}: No valid episode returns\")\n",
    "        results[category] = None\n",
    "        continue\n",
    "    results[category] = avg_reward\n",
    "    print(f\"{category}: Average Total Reward = {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, init_obs_list in eval_init_obs_categories.items():\n",
    "    print(len(init_obs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_agent performance\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bcq agent performance\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bcq agent test performance\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_counts = eICU_disc[['fio2', 'respiratory_rate_set']].value_counts()\n",
    "print(action_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_counts = train_df[['fio2', 'respiratory_rate_set']].value_counts()\n",
    "print(action_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generates trajectories by interacting with different offline policy doctor agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive agent interact with patient environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def interact_with_environment(env, agent, obs_hrs, num_steps=10, stay_id=None):\n",
    "    \"\"\"\n",
    "    Interact with the PatientEnvironment using the NaiveAgent.\n",
    "\n",
    "    Args:\n",
    "        env (PatientEnvironment): The environment to interact with.\n",
    "        agent (NaiveAgent): The naive agent.\n",
    "        obs_hrs (int): Number of observation hours (historical + current).\n",
    "        num_steps (int): Number of steps to interact with the environment.\n",
    "        stay_id (int, optional): The stay_id for the trajectory. If None, a random stay_id will be generated.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries representing the trajectory.\n",
    "    \"\"\"\n",
    "    trajectory = []\n",
    "    obs = env.reset()\n",
    "    stay_id = stay_id if stay_id is not None else random.randint(10000000, 99999999)  # Generate random stay_id\n",
    "    \n",
    "    # Observation structure: [s_1(3), a_1(2), s_2(3), a_2(2), ..., s_current(3), baseline(2)]\n",
    "    state_size = len(env.state_cols)  # 3\n",
    "    action_size = len(env.action_cols)  # 2\n",
    "    baseline_size = len(env.baseline_cols)  # 2\n",
    "    \n",
    "    # Extract last action (a_(obs_hrs-1)) from obs\n",
    "    last_action_start = (obs_hrs - 1) * (state_size + action_size) - action_size  # Index of last action\n",
    "    last_action = tuple(obs[last_action_start:last_action_start + action_size])  # e.g., obs[8:10] for obs_hrs=3\n",
    "    \n",
    "    # At t=0, include historical states and current state\n",
    "    for h in range(obs_hrs):  # h=0 for s_1, h=1 for s_2, h=2 for s_current\n",
    "        state_start = h * (state_size + action_size)  # 0 for s_1, 5 for s_2, 10 for s_current\n",
    "        state = obs[state_start:state_start + state_size]  # Extract state\n",
    "        # Action: use a_1 for s_1, a_2 for s_2, last_action for s_current\n",
    "        action = tuple(obs[state_start + state_size:state_start + state_size + action_size]) if h < obs_hrs - 1 else last_action\n",
    "        # Baseline variables\n",
    "        baseline_start = obs.size - baseline_size  # Last 2 indices\n",
    "        baseline = obs[baseline_start:baseline_start + baseline_size]  # e.g., obs[13:15]\n",
    "        \n",
    "        trajectory.append({\n",
    "            'stay_id': stay_id,\n",
    "            'hours_in': h,  # Start from 0, e.g., 0, 1, 2 for obs_hrs=3\n",
    "            **{col: state[i] for i, col in enumerate(env.state_cols)},  # State variables\n",
    "            **{col: action[i] for i, col in enumerate(env.action_cols)},  # Action variables\n",
    "            **{col: baseline[i] for i, col in enumerate(env.baseline_cols)}  # Baseline variables\n",
    "        })\n",
    "    \n",
    "    # Continue for remaining steps\n",
    "    for t in range(obs_hrs, num_steps):\n",
    "        # Convert observation to state_with_pre_state_action_baseline key\n",
    "        state_with_pre_state_action_baseline = tuple(obs.tolist())\n",
    "        \n",
    "        # Sample action from the naive agent\n",
    "        action_tuple = tuple(agent.sample_action(state_with_pre_state_action_baseline, last_action))\n",
    "        action = env.action_to_id.get(action_tuple, 0)  # Map action tuple to action ID\n",
    "        \n",
    "        # Step in the environment\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Extract current state and baseline\n",
    "        state_start = (obs_hrs - 1) * (state_size + action_size)  # Index of s_current, e.g., 10 for obs_hrs=3\n",
    "        state = next_obs[state_start:state_start + state_size]  # s_current\n",
    "        baseline_start = next_obs.size - baseline_size  # Last 2 indices\n",
    "        baseline = next_obs[baseline_start:baseline_start + baseline_size]  # baseline\n",
    "        \n",
    "        # Store trajectory\n",
    "        trajectory.append({\n",
    "            'stay_id': stay_id,\n",
    "            'hours_in': t,  # Continue from obs_hrs, e.g., 3, 4, ... for obs_hrs=3\n",
    "            **{col: state[i] for i, col in enumerate(env.state_cols)},  # State variables\n",
    "            **{col: action_tuple[i] for i, col in enumerate(env.action_cols)},  # Action variables\n",
    "            **{col: baseline[i] for i, col in enumerate(env.baseline_cols)}  # Baseline variables\n",
    "        })\n",
    "        \n",
    "        # Update observation and last action\n",
    "        obs = next_obs\n",
    "        last_action = action_tuple\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "def generate_trajectories_as_dataframe(env, agent, num_trajectories=10, num_steps=10):\n",
    "    \"\"\"\n",
    "    Generate a target number of trajectories and format them as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        env (PatientEnvironment): The environment to interact with.\n",
    "        agent (NaiveAgent): The naive agent.\n",
    "        num_trajectories (int): The number of trajectories to generate.\n",
    "        num_steps (int): The number of steps per trajectory.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing all trajectories.\n",
    "    \"\"\"\n",
    "    all_trajectories = []\n",
    "    \n",
    "    for num in range(num_trajectories):\n",
    "        trajectory = interact_with_environment(env, agent, obs_hrs, num_steps, stay_id=num)\n",
    "        all_trajectories.extend(trajectory)\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(all_trajectories)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example initialization\n",
    "naive_agent = NaiveAgent(train_df, obs_hrs, state_cols, action_cols, baseline_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PatientEnvironment(train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interact with the environment\n",
    "trajectory = interact_with_environment(env, naive_agent, obs_hrs, num_steps=120)\n",
    "# for step in trajectory:\n",
    "#     print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "print(obs)\n",
    "print(obs[-7:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_agent.naive_agent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10 trajectories, each with 20 steps\n",
    "df_trajectories = generate_trajectories_as_dataframe(env, naive_agent, num_trajectories=10000, num_steps=120)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_trajectories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trajectories.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trajectories['respiratory_rate_set'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate naive agent total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_episodes(env, agent, obs_hrs, num_episodes=100, num_steps=10):\n",
    "    \"\"\"\n",
    "    Run multiple episodes using the NaiveAgent in the PatientEnvironment and collect total rewards.\n",
    "\n",
    "    Args:\n",
    "        env (PatientEnvironment): The environment to interact with.\n",
    "        agent (NaiveAgent): The naive agent.\n",
    "        obs_hrs (int): Number of observation hours (historical + current).\n",
    "        num_episodes (int): Number of episodes to run.\n",
    "        num_steps (int): Number of steps per episode.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of total rewards for each episode.\n",
    "        list: A list of trajectories for each episode.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    all_trajectories = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Generate a random stay_id for this episode\n",
    "        stay_id = random.randint(10000000, 99999999)\n",
    "        \n",
    "        # Reset the environment for a new episode\n",
    "        obs = env.reset()\n",
    "        trajectory = []\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Get initial state and baseline information\n",
    "        state_size = len(env.state_cols)  # 3\n",
    "        action_size = len(env.action_cols)  # 2\n",
    "        baseline_size = len(env.baseline_cols)  # 2\n",
    "        \n",
    "        # Extract last action from obs\n",
    "        last_action_start = (obs_hrs - 1) * (state_size + action_size) - action_size\n",
    "        last_action = tuple(obs[last_action_start:last_action_start + action_size])\n",
    "        \n",
    "        # Record initial observations (historical + current states)\n",
    "        for h in range(obs_hrs):\n",
    "            state_start = h * (state_size + action_size)\n",
    "            state = obs[state_start:state_start + state_size]\n",
    "            action = tuple(obs[state_start + state_size:state_start + state_size + action_size]) if h < obs_hrs - 1 else last_action\n",
    "            baseline_start = obs.size - baseline_size\n",
    "            baseline = obs[baseline_start:baseline_start + baseline_size]\n",
    "            \n",
    "            trajectory.append({\n",
    "                'stay_id': stay_id,\n",
    "                'hours_in': h,\n",
    "                **{col: state[i] for i, col in enumerate(env.state_cols)},\n",
    "                **{col: action[i] for i, col in enumerate(env.action_cols)},\n",
    "                **{col: baseline[i] for i, col in enumerate(env.baseline_cols)}\n",
    "            })\n",
    "        \n",
    "        # Continue for remaining steps\n",
    "        for t in range(obs_hrs, num_steps):\n",
    "            # Convert observation to state_with_pre_state_action_baseline\n",
    "            state_with_pre_state_action_baseline = tuple(obs.tolist())\n",
    "            \n",
    "            # Sample action from the naive agent\n",
    "            action_tuple = tuple(agent.sample_action(state_with_pre_state_action_baseline, last_action))\n",
    "            action = env.action_to_id.get(action_tuple, 0)\n",
    "            \n",
    "            # Step in the environment\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward  # Accumulate reward\n",
    "            \n",
    "            # Extract current state and baseline\n",
    "            state_start = (obs_hrs - 1) * (state_size + action_size)\n",
    "            state = next_obs[state_start:state_start + state_size]\n",
    "            baseline_start = next_obs.size - baseline_size\n",
    "            baseline = next_obs[baseline_start:baseline_start + baseline_size]\n",
    "            \n",
    "            # Store trajectory\n",
    "            trajectory.append({\n",
    "                'stay_id': stay_id,\n",
    "                'hours_in': t,\n",
    "                **{col: state[i] for i, col in enumerate(env.state_cols)},\n",
    "                **{col: action_tuple[i] for i, col in enumerate(env.action_cols)},\n",
    "                **{col: baseline[i] for i, col in enumerate(env.baseline_cols)}\n",
    "            })\n",
    "            \n",
    "            # Update observation and last action\n",
    "            obs = next_obs\n",
    "            last_action = action_tuple\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Store the total reward and trajectory for this episode\n",
    "        episode_rewards.append(episode_reward)\n",
    "        all_trajectories.append(trajectory)\n",
    "        \n",
    "        # Optional: Print progress\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Completed {episode + 1}/{num_episodes} episodes\")\n",
    "    \n",
    "    return episode_rewards, all_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Initialize your agent and environment\n",
    "naive_agent = NaiveAgent(train_df, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "env = PatientEnvironment(train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict)\n",
    "\n",
    "# Run multiple episodes\n",
    "rewards, trajectories = run_multiple_episodes(env, naive_agent, obs_hrs=3, num_episodes=100, num_steps=120)\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Average reward across episodes: {sum(rewards) / len(rewards):.4f}\")\n",
    "print(f\"Min reward: {min(rewards):.4f}, Max reward: {max(rewards):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Initialize your agent and environment\n",
    "naive_agent = NaiveAgent(train_df, obs_hrs, state_cols, action_cols, baseline_cols)\n",
    "env = PatientEnvironment(train_transitions, state_cols, action_cols, baseline_cols, num_bins_dict, min_max_values_guidelines_bin_dict)\n",
    "\n",
    "# Run multiple episodes\n",
    "rewards, trajectories = run_multiple_episodes(env, naive_agent, obs_hrs=3, num_episodes=100, num_steps=120)\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Average reward across episodes: {sum(rewards) / len(rewards):.4f}\")\n",
    "print(f\"Min reward: {min(rewards):.4f}, Max reward: {max(rewards):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average len of trajectories\n",
    "avg_len = sum(len(traj) for traj in trajectories) / len(trajectories)\n",
    "print(f\"Average length of trajectories: {avg_len:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert generated trajectories from discrete to orignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_discrete_to_original(df, bin_edges_dict, columns):\n",
    "    \"\"\"\n",
    "    Convert discrete values in the DataFrame back to their original ranges.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing discrete values.\n",
    "        bin_edges_dict (dict): Dictionary containing bin edges for each feature.\n",
    "        columns (list): List of columns to convert.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the converted values.\n",
    "    \"\"\"\n",
    "    def map_bin_to_range(bin_index, feature, bin_edges_dict):\n",
    "        \"\"\"\n",
    "        Map a discrete bin index to its original range and return the mean of the range.\n",
    "        Args:\n",
    "            bin_index (int): The bin index.\n",
    "            feature (str): The feature name.\n",
    "            bin_edges_dict (dict): Dictionary containing bin edges for each feature.\n",
    "        Returns:\n",
    "            int: The mean of the range, rounded for better visualization.\n",
    "        \"\"\"\n",
    "        if feature not in bin_edges_dict:\n",
    "            return \"N/A\"\n",
    "        edges = bin_edges_dict[feature]\n",
    "        if bin_index < 0 or bin_index >= len(edges) - 1:\n",
    "            return \"Invalid\"\n",
    "        lower = edges[bin_index]\n",
    "        upper = edges[bin_index + 1]\n",
    "        if feature == \"fio2\":\n",
    "            return int(round(lower))  # For fio2, return the lower bound\n",
    "        else:\n",
    "            return int(round((lower + upper) / 2))  # Return the mean of the range\n",
    "\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_converted = df.copy()\n",
    "\n",
    "    # Apply the mapping to each column\n",
    "    for col in columns:\n",
    "        if col in bin_edges_dict:\n",
    "            df_converted[col] = df[col].apply(lambda x: map_bin_to_range(x, col, bin_edges_dict))\n",
    "\n",
    "    return df_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to convert\n",
    "columns_to_convert = state_cols + action_cols + [\"age\"]\n",
    "# Convert the discrete values back to their original ranges\n",
    "df_converted = convert_discrete_to_original(df_trajectories, bin_edges_dict, columns_to_convert)\n",
    "\n",
    "# Display the converted DataFrame\n",
    "df_converted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trajectories[\"respiratory_rate_set\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_converted[\"respiratory_rate_set\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame to a CSV file\n",
    "df_trajectories.to_csv(f'{mimic_iv_prefix_path}/naive_agent_generated_traj_disc_larger_24_hr_10000_update.csv', index=False)\n",
    "df_converted.to_csv(f'{mimic_iv_prefix_path}/naive_agent_generated_traj_cont_larger_24_hr_10000_update.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_converted[\"stay_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotly: Visualizes trajectories with transition action options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create transitin history for Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Interactive Trajectory Visualization with Dash & Plotly\n",
    "# Updated to use 2 history states + 1 current state\n",
    "# ================================\n",
    "\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to create transition history from the discretized DataFrame\n",
    "# Modified to use 2 history states + 1 current state\n",
    "def create_transition_history(df, n_history, state_cols, action_cols, baseline_cols):\n",
    "    \"\"\"\n",
    "    Create a transition history DataFrame with state, action, and baseline variables.\n",
    "    Now includes current state in the history window.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        n_history (int): Number of historical observations to include (excluding current).\n",
    "        state_cols (list): List of state variable columns.\n",
    "        action_cols (list): List of action variable columns.\n",
    "        baseline_cols (list): List of baseline variable columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Transition history DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame is sorted by stay_id and hours_in\n",
    "    df = df.sort_values(['stay_id', 'hours_in']).reset_index(drop=True)\n",
    "    \n",
    "    # Create history columns for state and action variables (only for n_history previous periods)\n",
    "    for i in range(1, n_history + 1):\n",
    "        for col in state_cols + action_cols:\n",
    "            df[f\"{col}_-{i}\"] = df.groupby('stay_id')[col].shift(i)\n",
    "    \n",
    "    # Drop rows without full history\n",
    "    df = df.dropna(subset=[f\"{state_cols[0]}_-{i}\" for i in range(1, n_history + 1)])\n",
    "    \n",
    "    # Create history tuples for previous state and action\n",
    "    df['S_hist'] = df[[f\"{col}_-{i}\" for col in state_cols for i in range(1, n_history + 1)]].apply(lambda row: tuple(row), axis=1)\n",
    "    df['A_hist'] = df[[f\"{col}_-{i}\" for col in action_cols for i in range(1, n_history + 1)]].apply(lambda row: tuple(row), axis=1)\n",
    "    \n",
    "    # Current state, action, and next state\n",
    "    df['S_t'] = df[state_cols].apply(lambda row: tuple(row), axis=1)\n",
    "    df['A_t'] = df[action_cols].apply(lambda row: tuple(row), axis=1)\n",
    "    df['S_t+1'] = df.groupby('stay_id')['S_t'].shift(-1)\n",
    "    \n",
    "    # Add baseline variables\n",
    "    df['baseline'] = df[baseline_cols].apply(lambda row: tuple(row), axis=1)\n",
    "    \n",
    "    return df.dropna(subset=['S_t+1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the transition history DataFrame using the discretized data\n",
    "# Now using n_history = 2 for 2 previous hours + current hour\n",
    "transition_history_df = create_transition_history(mimic_iv_disc.copy(), obs_hrs-1, state_cols, action_cols, baseline_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_history_mimic_iv_disc_df = transition_history_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = \"DiscreteCQL\" # \"naive_agent\" # \"DiscreteCQL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f'../models/training_log/{EXP_FOLDER_PREFIX}/eval_trajectories_{algo}_train_epoch19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read models/training_log/eval_trajectories_DiscreteSAC_train_epoch0.csv models/training_log/eval_trajectories_DoubleDQN_train_epoch0.csv\n",
    "# transition_history_df = create_transition_history(pd.read_csv(f'../models/training_log/{EXP_FOLDER_PREFIX}/eval_trajectories_{algo}_train_epoch19.csv'), obs_hrs-1, state_cols, action_cols, baseline_cols)\n",
    "if algo == \"naive_agent\":\n",
    "    transition_history_df = create_transition_history(pd.read_csv(f'../models/training_log/naive_agent/eval_trajectories_{algo}_train.csv'), obs_hrs-1, state_cols, action_cols, baseline_cols) # Naive Agent\n",
    "else:\n",
    "    transition_history_df = create_transition_history(pd.read_csv(f'../models/training_log/{EXP_FOLDER_PREFIX}/eval_trajectories_{algo}_train_epoch19.csv'), obs_hrs-1, state_cols, action_cols, baseline_cols)\n",
    "\n",
    "transition_history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iv_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10,5,4,8,5,1,2\n",
    "# 10,6,3,4,6,1,2\n",
    "# mimic_iv_disc[(mimic_iv_disc[\"heart_rate\"] == 10) & (mimic_iv_disc[\"resp_rate\"] == 5) & (mimic_iv_disc[\"spo2\"] == 4) & (mimic_iv_disc[\"fio2\"] == 8) & (mimic_iv_disc[\"respiratory_rate_set\"] == 5) & (mimic_iv_disc[\"gender_M\"] == 1) & (mimic_iv_disc[\"age\"] == 2)]\n",
    "# 0_small_reward_88\n",
    "# 7,5,1,8,4,1,2\n",
    "# 9,3,4,6,5,1,2\n",
    "# mimic_iv_disc[(mimic_iv_disc[\"heart_rate\"] == 7) & (mimic_iv_disc[\"resp_rate\"] == 5) & (mimic_iv_disc[\"spo2\"] == 1) & (mimic_iv_disc[\"fio2\"] == 8) & (mimic_iv_disc[\"respiratory_rate_set\"] == 4) & (mimic_iv_disc[\"gender_M\"] == 1) & (mimic_iv_disc[\"age\"] == 2)]\n",
    "mimic_iv_disc[(mimic_iv_disc[\"heart_rate\"] == 6) & (mimic_iv_disc[\"resp_rate\"] == 5) & (mimic_iv_disc[\"spo2\"] == 1) & (mimic_iv_disc[\"fio2\"] == 8) & (mimic_iv_disc[\"respiratory_rate_set\"] == 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mimic_iv_disc[mimic_iv_disc[\"stay_id\"] == 38297623] # this is small_reward_0\n",
    "# mimic_iv_disc[mimic_iv_disc[\"stay_id\"] == 31726457] # this is small_reward_88\n",
    "mimic_iv_disc[mimic_iv_disc[\"stay_id\"] == 37768645] # this is small_reward_66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iv_disc[mimic_iv_disc[\"stay_id\"] == 30139485]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add background for guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Helper functions for guideline checking\n",
    "def is_within_guidelines(row, state_cols, action_cols, guideline_dict):\n",
    "    state_dict = dict(zip(state_cols, row['S_t']))\n",
    "    action_dict = dict(zip(action_cols, row['A_t']))\n",
    "    all_vars = state_cols + action_cols\n",
    "    for var in all_vars:\n",
    "        if var in guideline_dict:\n",
    "            limits = guideline_dict[var]\n",
    "            if limits['min'] == 0 and limits['max'] == 0:\n",
    "                continue\n",
    "            if limits['max'] <= 15:  # Assuming bin indices for max <= 15\n",
    "                value = state_dict[var] if var in state_cols else action_dict[var]\n",
    "                if not (limits['min'] <= value <= limits['max']):\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "def count_not_within_guidelines(tup, cols, guideline_dict):\n",
    "    tup_dict = dict(zip(cols, tup))\n",
    "    count = 0\n",
    "    for var in cols:\n",
    "        if var in guideline_dict:\n",
    "            limits = guideline_dict[var]\n",
    "            if limits['min'] == 0 and limits['max'] == 0:\n",
    "                continue\n",
    "            if limits['max'] <= 15:\n",
    "                if not (limits['min'] <= tup_dict[var] <= limits['max']):\n",
    "                    count += 1\n",
    "    return count\n",
    "\n",
    "# Functions for actions and outcomes\n",
    "def get_possible_actions(history_state, history_action, current_state):\n",
    "    similar_states = transition_history_df[(transition_history_df['S_hist'] == history_state) &\n",
    "                                           (transition_history_df['A_hist'] == history_action) &\n",
    "                                           (transition_history_df['S_t'] == current_state)]\n",
    "    if similar_states.empty:\n",
    "        return []\n",
    "    action_counts = similar_states.groupby('A_t').size().reset_index(name='count')\n",
    "    action_patients = similar_states.groupby(['A_t', 'stay_id']).size().reset_index()\n",
    "    distinct_patients = action_patients.groupby('A_t').size().reset_index(name='patient_count')\n",
    "    action_stats = pd.merge(action_counts, distinct_patients, on='A_t')\n",
    "    return [{'action': row['A_t'], 'count': row['count'], 'patient_count': row['patient_count']}\n",
    "            for _, row in action_stats.iterrows()]\n",
    "\n",
    "def get_action_outcomes(history_state, history_action, current_state, selected_action, top_n=5):\n",
    "    similar_states = transition_history_df[(transition_history_df['S_hist'] == history_state) &\n",
    "                                           (transition_history_df['A_hist'] == history_action) &\n",
    "                                           (transition_history_df['S_t'] == current_state) &\n",
    "                                           (transition_history_df['A_t'] == selected_action)]\n",
    "    if similar_states.empty:\n",
    "        return []\n",
    "    total_occurrences = similar_states.groupby('S_t+1').size().reset_index(name='total_count')\n",
    "    distinct_patients = similar_states.groupby('S_t+1')['stay_id'].nunique().reset_index(name='distinct_patients')\n",
    "    outcomes = pd.merge(total_occurrences, distinct_patients, on='S_t+1')\n",
    "    outcomes = outcomes.sort_values('total_count', ascending=False).head(top_n)\n",
    "    return [{'S_t+1': row['S_t+1'], 'total_count': row['total_count'], 'distinct_patients': row['distinct_patients']}\n",
    "            for _, row in outcomes.iterrows()]\n",
    "\n",
    "# Formatting functions\n",
    "def format_tuple(t):\n",
    "    return str(tuple(int(x) for x in t))\n",
    "\n",
    "def format_tuple_grouped(data, group_size):\n",
    "    if len(data) % group_size != 0:\n",
    "        raise ValueError(\"Data length must be divisible by group size.\")\n",
    "    num_groups = len(data) // group_size\n",
    "    grouped = [tuple(data[i::num_groups]) for i in range(num_groups)]\n",
    "    grouped.reverse()\n",
    "    return \", \".join(str(group) for group in grouped)\n",
    "\n",
    "# Set up Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Patient Trajectory Visualization\"),\n",
    "    dcc.Dropdown(\n",
    "        id='stay-id-dropdown',\n",
    "        options=[{'label': str(sid), 'value': sid} for sid in sorted(transition_history_df['stay_id'].unique())],\n",
    "        value=sorted(transition_history_df['stay_id'].unique())[0],\n",
    "        clearable=False\n",
    "    ),\n",
    "    dcc.Graph(id='trajectory-plot'),\n",
    "    html.Div(\n",
    "        id='action-details',\n",
    "        style={\n",
    "            'margin-top': '20px',\n",
    "            'backgroundColor': 'white',\n",
    "            'padding': '15px',\n",
    "            'border': '1px solid #ccc',\n",
    "            'borderRadius': '5px',\n",
    "            'boxShadow': '0 2px 5px rgba(0,0,0,0.1)',\n",
    "            'fontFamily': 'Arial, sans-serif',\n",
    "            'color': '#333'\n",
    "        }\n",
    "    )\n",
    "], style={'backgroundColor': 'white', 'padding': '20px'})\n",
    "\n",
    "# Callback to update trajectory plot\n",
    "@app.callback(\n",
    "    Output('trajectory-plot', 'figure'),\n",
    "    Input('stay-id-dropdown', 'value')\n",
    ")\n",
    "def update_plot(selected_stay_id):\n",
    "    patient_df = transition_history_df[transition_history_df['stay_id'] == selected_stay_id].copy()\n",
    "    patient_df['within_guidelines'] = patient_df.apply(\n",
    "        lambda row: is_within_guidelines(row, state_cols, action_cols, min_max_values_guidelines_bin_dict),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    fig = px.line(\n",
    "        patient_df,\n",
    "        x='hours_in',\n",
    "        y=state_cols,\n",
    "        title=f'Trajectory for stay_id {selected_stay_id}',\n",
    "        labels={'value': 'Vital Sign Value', 'variable': 'Vital Sign', 'hours_in': 'Time (hours)'}\n",
    "    )\n",
    "    fig.update_traces(mode='lines+markers')\n",
    "    \n",
    "    hover_texts = []\n",
    "    for _, row in patient_df.iterrows():\n",
    "        hover_text = (\n",
    "            f\"<b>Hour:</b> {row['hours_in']}<br>\"\n",
    "            f\"<b>Current State:</b> {format_tuple(row['S_t'])}<br>\"\n",
    "            f\"<b>Current Action:</b> {format_tuple(row['A_t'])}<br>\"\n",
    "        )\n",
    "        hover_texts.append(hover_text)\n",
    "    \n",
    "    for trace in fig.data:\n",
    "        trace.customdata = patient_df[['stay_id', 'S_hist', 'A_hist', 'S_t', 'A_t', 'hours_in']].to_dict('records')\n",
    "        trace.text = hover_texts\n",
    "        trace.hovertemplate = '%{text}<extra></extra>'\n",
    "    \n",
    "    shapes = []\n",
    "    for idx, row in patient_df.iterrows():\n",
    "        t = row['hours_in']\n",
    "        color = 'rgba(144, 238, 144, 0.3)' if row['within_guidelines'] else 'rgba(255, 99, 71, 0.3)'\n",
    "        shapes.append(dict(\n",
    "            type=\"rect\",\n",
    "            x0=t - 0.5,\n",
    "            y0=0,\n",
    "            x1=t + 0.5,\n",
    "            y1=1,\n",
    "            xref=\"x\",\n",
    "            yref=\"paper\",\n",
    "            fillcolor=color,\n",
    "            line_width=0,\n",
    "            layer=\"below\"\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        shapes=shapes,\n",
    "        clickmode='event+select',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "# Callback to update action details\n",
    "@app.callback(\n",
    "    Output('action-details', 'children'),\n",
    "    Input('trajectory-plot', 'clickData')\n",
    ")\n",
    "def display_action_details(clickData):\n",
    "    if not clickData:\n",
    "        return html.P(\"Click a point on the plot to see detailed transition outcomes.\")\n",
    "    \n",
    "    point = clickData['points'][0]\n",
    "    data = point['customdata']\n",
    "    history_state = tuple(data['S_hist'])\n",
    "    history_action = tuple(data['A_hist'])\n",
    "    current_state = tuple(data['S_t'])\n",
    "    \n",
    "    state_group_size = len(current_state)\n",
    "    action_group_size = len(data['A_t'])\n",
    "    \n",
    "    details = [\n",
    "        html.H3(f\"Details for stay_id {data['stay_id']} at Hour {data['hours_in']}\"),\n",
    "        html.P(f\"Current State: {format_tuple(current_state)}\"),\n",
    "        html.P(f\"Current Action: {format_tuple(data['A_t'])}\"),\n",
    "        html.P(f\"History States: {format_tuple_grouped(history_state, state_group_size)}\"),\n",
    "        html.P(f\"History Actions: {format_tuple_grouped(history_action, action_group_size)}\")\n",
    "    ]\n",
    "    \n",
    "    possible_actions = get_possible_actions(history_state, history_action, current_state)\n",
    "    if possible_actions:\n",
    "        action_panels = []\n",
    "        for act in possible_actions:\n",
    "            action_tuple = act['action']\n",
    "            count_not_satisfy = count_not_within_guidelines(action_tuple, action_cols, min_max_values_guidelines_bin_dict)\n",
    "            bg_color = 'lightgreen' if count_not_satisfy == 0 else 'lightsalmon'\n",
    "            action_text = html.Span(\n",
    "                f\"Action {format_tuple(action_tuple)}\",\n",
    "                style={'backgroundColor': bg_color, 'padding': '2px 5px', 'borderRadius': '3px'}\n",
    "            )\n",
    "            \n",
    "            outcomes = get_action_outcomes(history_state, history_action, current_state, action_tuple)\n",
    "            outcome_lis = []\n",
    "            for outcome in outcomes:\n",
    "                state_tuple = outcome['S_t+1']\n",
    "                count_not_satisfy = count_not_within_guidelines(state_tuple, state_cols, min_max_values_guidelines_bin_dict)\n",
    "                bg_color = 'lightgreen' if count_not_satisfy == 0 else 'orange'\n",
    "                state_text = html.Span(\n",
    "                    f\"State {format_tuple(state_tuple)}\",\n",
    "                    style={'backgroundColor': bg_color, 'padding': '2px 5px', 'borderRadius': '3px'}\n",
    "                )\n",
    "                outcome_lis.append(\n",
    "                    html.Li([state_text, f\" ({outcome['total_count']} occurrences across {outcome['distinct_patients']} patients)\"])\n",
    "                )\n",
    "            \n",
    "            action_panels.append(\n",
    "                html.Div([\n",
    "                    html.H4([action_text, f\" ({act['count']} occurrences across {act['patient_count']} patients)\"]),\n",
    "                    html.Ul(outcome_lis)\n",
    "                ], style={'border': '1px solid #ddd', 'padding': '10px', 'margin': '5px'})\n",
    "            )\n",
    "        details.append(html.Div(action_panels))\n",
    "    else:\n",
    "        details.append(html.P(\"No available action transitions for this state combination.\"))\n",
    "    \n",
    "    return html.Div(details)\n",
    "\n",
    "# Run the app\n",
    "# app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With baseline bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Patient Trajectory Visualization\"),\n",
    "    dcc.Dropdown(\n",
    "        id='stay-id-dropdown',\n",
    "        options=[{'label': str(sid), 'value': sid} for sid in sorted(transition_history_df['stay_id'].unique())],\n",
    "        value=sorted(transition_history_df['stay_id'].unique())[0],\n",
    "        clearable=False\n",
    "    ),\n",
    "    # Added RadioItems for baseline toggle\n",
    "    dcc.RadioItems(\n",
    "        id='consider-baseline',\n",
    "        options=[\n",
    "            {'label': 'Consider Baseline (Gender, Age)', 'value': 'yes'},\n",
    "            {'label': 'Ignore Baseline', 'value': 'no'}\n",
    "        ],\n",
    "        value='no',  # Default to ignoring baseline\n",
    "        labelStyle={'display': 'inline-block', 'margin-right': '20px'}\n",
    "    ),\n",
    "    dcc.Graph(id='trajectory-plot'),\n",
    "    html.Div(\n",
    "        id='action-details',\n",
    "        style={\n",
    "            'margin-top': '20px',\n",
    "            'backgroundColor': 'white',\n",
    "            'padding': '15px',\n",
    "            'border': '1px solid #ccc',\n",
    "            'borderRadius': '5px',\n",
    "            'boxShadow': '0 2px 5px rgba(0,0,0,0.1)',\n",
    "            'fontFamily': 'Arial, sans-serif',\n",
    "            'color': '#333'\n",
    "        }\n",
    "    )\n",
    "], style={'backgroundColor': 'white', 'padding': '20px'})\n",
    "\n",
    "def get_range_str(feat, bin_index):\n",
    "    if feat not in bin_edges_dict: # TODO: set as corresponding bin edges strategy\n",
    "        return f\"{feat}: N/A\"\n",
    "    edges = bin_edges_dict[feat]\n",
    "    bin_index = int(bin_index)\n",
    "    if bin_index < 0 or bin_index >= len(edges) - 1:\n",
    "        return f\"{feat}: Invalid bin\"\n",
    "    lower = edges[bin_index]\n",
    "    upper = edges[bin_index + 1]\n",
    "    return f\"{feat}: ({lower:.1f} ~ {upper:.1f})\"\n",
    "\n",
    "# Updated update_plot function (only showing the modified part)\n",
    "@app.callback(\n",
    "    Output('trajectory-plot', 'figure'),\n",
    "    Input('stay-id-dropdown', 'value')\n",
    ")\n",
    "def update_plot(selected_stay_id):\n",
    "    patient_df = transition_history_df[transition_history_df['stay_id'] == selected_stay_id].copy()\n",
    "    patient_df['within_guidelines'] = patient_df.apply(\n",
    "        lambda row: is_within_guidelines(row, state_cols, action_cols, min_max_values_guidelines_bin_dict),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    fig = px.line(\n",
    "        patient_df,\n",
    "        x='hours_in',\n",
    "        y=state_cols,\n",
    "        title=f'Trajectory for stay_id {selected_stay_id}',\n",
    "        labels={'value': 'Vital Sign Value', 'variable': 'Vital Sign', 'hours_in': 'Time (hours)'}\n",
    "    )\n",
    "    fig.update_traces(mode='lines+markers')\n",
    "    \n",
    "    # Updated hover text with ranges\n",
    "    hover_texts = []\n",
    "    for _, row in patient_df.iterrows():\n",
    "        state_ranges = [get_range_str(feat, bin_idx) for feat, bin_idx in zip(state_cols, row['S_t'])]\n",
    "        action_ranges = [get_range_str(feat, bin_idx) for feat, bin_idx in zip(action_cols, row['A_t'])]\n",
    "        hover_text = (\n",
    "            f\"<b>Hour:</b> {row['hours_in']}<br>\"\n",
    "            f\"<b>Current State:</b> {format_tuple(row['S_t'])}<br>\"\n",
    "            + \"<br>\".join(state_ranges) + \"<br>\"\n",
    "            f\"<b>Current Action:</b> {format_tuple(row['A_t'])}<br>\"\n",
    "            + \"<br>\".join(action_ranges)\n",
    "        )\n",
    "        hover_texts.append(hover_text)\n",
    "    \n",
    "    for trace in fig.data:\n",
    "        trace.customdata = patient_df[['stay_id', 'S_hist', 'A_hist', 'S_t', 'A_t', 'hours_in', 'baseline']].to_dict('records')\n",
    "        trace.text = hover_texts\n",
    "        trace.hovertemplate = '%{text}<extra></extra>'\n",
    "    \n",
    "    # [Rest of the function remains unchanged: shapes, layout, etc.]\n",
    "    shapes = []\n",
    "    for idx, row in patient_df.iterrows():\n",
    "        t = row['hours_in']\n",
    "        color = 'rgba(144, 238, 144, 0.3)' if row['within_guidelines'] else 'rgba(255, 99, 71, 0.3)'\n",
    "        shapes.append(dict(\n",
    "            type=\"rect\",\n",
    "            x0=t - 0.5,\n",
    "            y0=0,\n",
    "            x1=t + 0.5,\n",
    "            y1=1,\n",
    "            xref=\"x\",\n",
    "            yref=\"paper\",\n",
    "            fillcolor=color,\n",
    "            line_width=0,\n",
    "            layer=\"below\"\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        shapes=shapes,\n",
    "        clickmode='event+select',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def get_possible_actions(history_state, history_action, current_state, baseline=None):\n",
    "    # Base filters\n",
    "    filters = (\n",
    "        (transition_history_df['S_hist'] == history_state) &\n",
    "        (transition_history_df['A_hist'] == history_action) &\n",
    "        (transition_history_df['S_t'] == current_state)\n",
    "    )\n",
    "    # Add baseline filter if provided\n",
    "    if baseline is not None:\n",
    "        filters &= (transition_history_df['baseline'] == baseline)\n",
    "    \n",
    "    similar_states = transition_history_df[filters]\n",
    "    if similar_states.empty:\n",
    "        return []\n",
    "    \n",
    "    action_counts = similar_states.groupby('A_t').size().reset_index(name='count')\n",
    "    action_patients = similar_states.groupby(['A_t', 'stay_id']).size().reset_index()\n",
    "    distinct_patients = action_patients.groupby('A_t').size().reset_index(name='patient_count')\n",
    "    action_stats = pd.merge(action_counts, distinct_patients, on='A_t')\n",
    "    return [{'action': row['A_t'], 'count': row['count'], 'patient_count': row['patient_count']}\n",
    "            for _, row in action_stats.iterrows()]\n",
    "\n",
    "def get_action_outcomes(history_state, history_action, current_state, selected_action, baseline=None, top_n=5):\n",
    "    # Base filters\n",
    "    filters = (\n",
    "        (transition_history_df['S_hist'] == history_state) &\n",
    "        (transition_history_df['A_hist'] == history_action) &\n",
    "        (transition_history_df['S_t'] == current_state) &\n",
    "        (transition_history_df['A_t'] == selected_action)\n",
    "    )\n",
    "    # Add baseline filter if provided\n",
    "    if baseline is not None:\n",
    "        filters &= (transition_history_df['baseline'] == baseline)\n",
    "    \n",
    "    similar_states = transition_history_df[filters]\n",
    "    if similar_states.empty:\n",
    "        return []\n",
    "    \n",
    "    total_occurrences = similar_states.groupby('S_t+1').size().reset_index(name='total_count')\n",
    "    distinct_patients = similar_states.groupby('S_t+1')['stay_id'].nunique().reset_index(name='distinct_patients')\n",
    "    outcomes = pd.merge(total_occurrences, distinct_patients, on='S_t+1')\n",
    "    outcomes = outcomes.sort_values('total_count', ascending=False).head(top_n)\n",
    "    return [{'S_t+1': row['S_t+1'], 'total_count': row['total_count'], 'distinct_patients': row['distinct_patients']}\n",
    "            for _, row in outcomes.iterrows()]\n",
    "\n",
    "@app.callback(\n",
    "    Output('action-details', 'children'),\n",
    "    [Input('trajectory-plot', 'clickData'),\n",
    "     Input('consider-baseline', 'value')]\n",
    ")\n",
    "\n",
    "\n",
    "# order by action occurrences\n",
    "def display_action_details(clickData, consider_baseline_value):\n",
    "    if not clickData:\n",
    "        return html.P(\"Click a point on the plot to see detailed transition outcomes.\")\n",
    "    \n",
    "    point = clickData['points'][0]\n",
    "    data = point['customdata']\n",
    "    history_state = tuple(data['S_hist'])\n",
    "    history_action = tuple(data['A_hist'])\n",
    "    current_state = tuple(data['S_t'])\n",
    "    \n",
    "    state_group_size = len(current_state)\n",
    "    action_group_size = len(data['A_t'])\n",
    "    \n",
    "    # Determine whether to consider baseline\n",
    "    consider_baseline = (consider_baseline_value == 'yes')\n",
    "    baseline = tuple(data['baseline']) if consider_baseline else None\n",
    "    \n",
    "    # Build details with baseline consideration status\n",
    "    details = [\n",
    "        html.H3(f\"Details for stay_id {data['stay_id']} at Hour {data['hours_in']}\"),\n",
    "        html.P(f\"Considering baseline: {'Yes' if consider_baseline else 'No'}\"),\n",
    "        html.P(f\"Current State: {format_tuple(current_state)}\"),\n",
    "        html.P(f\"Current Action: {format_tuple(data['A_t'])}\"),\n",
    "        html.P(f\"History States: {format_tuple_grouped(history_state, state_group_size)}\"),\n",
    "        html.P(f\"History Actions: {format_tuple_grouped(history_action, action_group_size)}\")\n",
    "    ]\n",
    "    \n",
    "    # Get possible actions and outcomes with baseline parameter\n",
    "    possible_actions = get_possible_actions(history_state, history_action, current_state, baseline)\n",
    "    \n",
    "    # Sort possible_actions by 'count' in descending order\n",
    "    possible_actions = sorted(possible_actions, key=lambda x: x['count'], reverse=True)\n",
    "    \n",
    "    if possible_actions:\n",
    "        action_panels = []\n",
    "        for act in possible_actions:\n",
    "            action_tuple = act['action']\n",
    "            count_not_satisfy = count_not_within_guidelines(action_tuple, action_cols, min_max_values_guidelines_bin_dict)\n",
    "            bg_color = 'lightgreen' if count_not_satisfy == 0 else 'lightsalmon'\n",
    "            action_text = html.Span(\n",
    "                f\"Action {format_tuple(action_tuple)}\",\n",
    "                style={'backgroundColor': bg_color, 'padding': '2px 5px', 'borderRadius': '3px'}\n",
    "            )\n",
    "            \n",
    "            outcomes = get_action_outcomes(history_state, history_action, current_state, action_tuple, baseline)\n",
    "            outcome_lis = []\n",
    "            for outcome in outcomes:\n",
    "                state_tuple = outcome['S_t+1']\n",
    "                count_not_satisfy = count_not_within_guidelines(state_tuple, state_cols, min_max_values_guidelines_bin_dict)\n",
    "                bg_color = 'lightgreen' if count_not_satisfy == 0 else 'orange'\n",
    "                state_text = html.Span(\n",
    "                    f\"State {format_tuple(state_tuple)}\",\n",
    "                    style={'backgroundColor': bg_color, 'padding': '2px 5px', 'borderRadius': '3px'}\n",
    "                )\n",
    "                outcome_lis.append(\n",
    "                    html.Li([state_text, f\" ({outcome['total_count']} occurrences across {outcome['distinct_patients']} patients)\"])\n",
    "                )\n",
    "            \n",
    "            action_panels.append(\n",
    "                html.Div([\n",
    "                    html.H4([action_text, f\" ({act['count']} occurrences across {act['patient_count']} patients)\"]),\n",
    "                    html.Ul(outcome_lis)\n",
    "                ], style={'border': '1px solid #ddd', 'padding': '10px', 'margin': '5px'})\n",
    "            )\n",
    "        details.append(html.Div(action_panels))\n",
    "    else:\n",
    "        details.append(html.P(\"No available action transitions for this state combination.\"))\n",
    "    \n",
    "    return html.Div(details)\n",
    "\n",
    "# Run the app\n",
    "app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read models/training_log/eval_trajectories_DiscreteSAC_train_epoch0.csv models/training_log/eval_trajectories_DoubleDQN_train_epoch0.csv\n",
    "transition_history_df = create_transition_history(pd.read_csv(f'../models/training_log_backup/eval_trajectories_DiscreteSAC_train_epoch9.csv'), obs_hrs-1, state_cols, action_cols, baseline_cols)\n",
    "transition_history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Patient Trajectory Visualization\"),\n",
    "    dcc.Dropdown(\n",
    "        id='stay-id-dropdown',\n",
    "        options=[{'label': str(sid), 'value': sid} for sid in sorted(transition_history_df['stay_id'].unique())],\n",
    "        value=sorted(transition_history_df['stay_id'].unique())[0],\n",
    "        clearable=False\n",
    "    ),\n",
    "    # Added RadioItems for baseline toggle\n",
    "    dcc.RadioItems(\n",
    "        id='consider-baseline',\n",
    "        options=[\n",
    "            {'label': 'Consider Baseline (Gender, Age)', 'value': 'yes'},\n",
    "            {'label': 'Ignore Baseline', 'value': 'no'}\n",
    "        ],\n",
    "        value='no',  # Default to ignoring baseline\n",
    "        labelStyle={'display': 'inline-block', 'margin-right': '20px'}\n",
    "    ),\n",
    "    dcc.Graph(id='trajectory-plot'),\n",
    "    html.Div(\n",
    "        id='action-details',\n",
    "        style={\n",
    "            'margin-top': '20px',\n",
    "            'backgroundColor': 'white',\n",
    "            'padding': '15px',\n",
    "            'border': '1px solid #ccc',\n",
    "            'borderRadius': '5px',\n",
    "            'boxShadow': '0 2px 5px rgba(0,0,0,0.1)',\n",
    "            'fontFamily': 'Arial, sans-serif',\n",
    "            'color': '#333'\n",
    "        }\n",
    "    )\n",
    "], style={'backgroundColor': 'white', 'padding': '20px'})\n",
    "\n",
    "# Helper function to map discrete bin values to their original ranges\n",
    "def map_bin_to_range(bin_index, feature, bin_edges_dict):\n",
    "    \"\"\"\n",
    "    Map a discrete bin index to its original range and return the mean of the range.\n",
    "    Args:\n",
    "        bin_index (int): The bin index.\n",
    "        feature (str): The feature name.\n",
    "        bin_edges_dict (dict): Dictionary containing bin edges for each feature.\n",
    "    Returns:\n",
    "        int: The mean of the range, rounded for better visualization.\n",
    "    \"\"\"\n",
    "    if feature not in bin_edges_dict:\n",
    "        return \"N/A\"\n",
    "    edges = bin_edges_dict[feature]\n",
    "    if bin_index < 0 or bin_index >= len(edges) - 1:\n",
    "        return \"Invalid\"\n",
    "    lower = edges[bin_index]\n",
    "    upper = edges[bin_index + 1]\n",
    "    if feature == \"fio2\":\n",
    "        # round it to the nearest 5 or 10\n",
    "        # return int(round(round((lower + upper) / 2) / 5) * 5)\n",
    "        return int(round(lower))\n",
    "    else:\n",
    "        return int(round((lower + upper) / 2))\n",
    "\n",
    "# Updated update_plot function (only showing the modified part)\n",
    "@app.callback(\n",
    "    Output('trajectory-plot', 'figure'),\n",
    "    Input('stay-id-dropdown', 'value')\n",
    ")\n",
    "def update_plot(selected_stay_id):\n",
    "    patient_df = transition_history_df[transition_history_df['stay_id'] == selected_stay_id].copy()\n",
    "    patient_df['within_guidelines'] = patient_df.apply(\n",
    "        lambda row: is_within_guidelines(row, state_cols, action_cols, min_max_values_guidelines_bin_dict),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    fig = px.line(\n",
    "        patient_df,\n",
    "        x='hours_in',\n",
    "        y=state_cols,\n",
    "        title=f'Trajectory for stay_id {selected_stay_id}',\n",
    "        labels={'value': 'Vital Sign Value', 'variable': 'Vital Sign', 'hours_in': 'Time (hours)'}\n",
    "    )\n",
    "    fig.update_traces(mode='lines+markers')\n",
    "    \n",
    "    # Updated hover text with ranges\n",
    "    hover_texts = []\n",
    "    for _, row in patient_df.iterrows():\n",
    "        state_ranges = [get_range_str(feat, bin_idx) for feat, bin_idx in zip(state_cols, row['S_t'])]\n",
    "        action_ranges = [get_range_str(feat, bin_idx) for feat, bin_idx in zip(action_cols, row['A_t'])]\n",
    "        hover_text = (\n",
    "            f\"<b>Hour:</b> {row['hours_in']}<br>\"\n",
    "            f\"<b>Current State:</b> {format_tuple(row['S_t'])}<br>\"\n",
    "            + \"<br>\".join(state_ranges) + \"<br>\"\n",
    "            f\"<b>Current Action:</b> {format_tuple(row['A_t'])}<br>\"\n",
    "            + \"<br>\".join(action_ranges)\n",
    "        )\n",
    "        hover_texts.append(hover_text)\n",
    "    \n",
    "    for trace in fig.data:\n",
    "        trace.customdata = patient_df[['stay_id', 'S_hist', 'A_hist', 'S_t', 'A_t', 'hours_in', 'baseline']].to_dict('records')\n",
    "        trace.text = hover_texts\n",
    "        trace.hovertemplate = '%{text}<extra></extra>'\n",
    "    \n",
    "    # [Rest of the function remains unchanged: shapes, layout, etc.]\n",
    "    shapes = []\n",
    "    for idx, row in patient_df.iterrows():\n",
    "        t = row['hours_in']\n",
    "        color = 'rgba(144, 238, 144, 0.3)' if row['within_guidelines'] else 'rgba(255, 99, 71, 0.3)'\n",
    "        shapes.append(dict(\n",
    "            type=\"rect\",\n",
    "            x0=t - 0.5,\n",
    "            y0=0,\n",
    "            x1=t + 0.5,\n",
    "            y1=1,\n",
    "            xref=\"x\",\n",
    "            yref=\"paper\",\n",
    "            fillcolor=color,\n",
    "            line_width=0,\n",
    "            layer=\"below\"\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        shapes=shapes,\n",
    "        clickmode='event+select',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "@app.callback(\n",
    "    Output('action-details', 'children'),\n",
    "    [Input('trajectory-plot', 'clickData'),\n",
    "     Input('consider-baseline', 'value')]\n",
    ")\n",
    "def display_action_details(clickData, consider_baseline_value):\n",
    "    if not clickData:\n",
    "        return html.P(\"Click a point on the plot to see detailed transition outcomes.\")\n",
    "    \n",
    "    point = clickData['points'][0]\n",
    "    data = point['customdata']\n",
    "    history_state = tuple(data['S_hist'])\n",
    "    history_action = tuple(data['A_hist'])\n",
    "    current_state = tuple(data['S_t'])\n",
    "    current_action = tuple(data['A_t'])\n",
    "    \n",
    "    # Map state and action bins to their original ranges and add units\n",
    "    mapped_current_state = [\n",
    "        f\"{map_bin_to_range(bin_idx, feat, bin_edges_dict)} {unit}\"\n",
    "        for feat, bin_idx, unit in zip(state_cols, current_state, state_units)\n",
    "    ]\n",
    "    mapped_current_action = [\n",
    "        f\"{map_bin_to_range(bin_idx, feat, bin_edges_dict)} {unit}\"\n",
    "        for feat, bin_idx, unit in zip(action_cols, current_action, action_units)\n",
    "    ]\n",
    "    \n",
    "    # Determine whether to consider baseline\n",
    "    consider_baseline = (consider_baseline_value == 'yes')\n",
    "    baseline = tuple(data['baseline']) if consider_baseline else None\n",
    "    \n",
    "    # Build details with baseline consideration status\n",
    "    details = [\n",
    "        html.H3(f\"Details for stay_id {data['stay_id']} at Hour {data['hours_in']}\"),\n",
    "        html.P(f\"Considering baseline: {'Yes' if consider_baseline else 'No'}\"),\n",
    "        html.P(f\"Current State: State ({', '.join(mapped_current_state)})\"),\n",
    "        html.P(f\"Current Action: Action ({', '.join(mapped_current_action)})\")\n",
    "    ]\n",
    "    \n",
    "    # Get possible actions and outcomes with baseline parameter\n",
    "    possible_actions = get_possible_actions(history_state, history_action, current_state, baseline)\n",
    "    \n",
    "    # Sort possible_actions by 'count' in descending order\n",
    "    possible_actions = sorted(possible_actions, key=lambda x: x['count'], reverse=True)\n",
    "    \n",
    "    if possible_actions:\n",
    "        action_panels = []\n",
    "        for act in possible_actions:\n",
    "            action_tuple = act['action']\n",
    "            mapped_action = [\n",
    "                f\"{map_bin_to_range(bin_idx, feat, bin_edges_dict)} {unit}\"\n",
    "                for feat, bin_idx, unit in zip(action_cols, action_tuple, action_units)\n",
    "            ]\n",
    "            count_not_satisfy = count_not_within_guidelines(action_tuple, action_cols, min_max_values_guidelines_bin_dict)\n",
    "            bg_color = 'lightgreen' if count_not_satisfy == 0 else 'lightsalmon'\n",
    "            action_text = html.Span(\n",
    "                f\"Action ({', '.join(mapped_action)})\",\n",
    "                style={'backgroundColor': bg_color, 'padding': '2px 5px', 'borderRadius': '3px'}\n",
    "            )\n",
    "            \n",
    "            outcomes = get_action_outcomes(history_state, history_action, current_state, action_tuple, baseline)\n",
    "            outcome_lis = []\n",
    "            for outcome in outcomes:\n",
    "                state_tuple = outcome['S_t+1']\n",
    "                mapped_state = [\n",
    "                    f\"{map_bin_to_range(bin_idx, feat, bin_edges_dict)} {unit}\"\n",
    "                    for feat, bin_idx, unit in zip(state_cols, state_tuple, state_units)\n",
    "                ]\n",
    "                count_not_satisfy = count_not_within_guidelines(state_tuple, state_cols, min_max_values_guidelines_bin_dict)\n",
    "                bg_color = 'lightgreen' if count_not_satisfy == 0 else 'orange'\n",
    "                state_text = html.Span(\n",
    "                    f\"State ({', '.join(mapped_state)})\",\n",
    "                    style={'backgroundColor': bg_color, 'padding': '2px 5px', 'borderRadius': '3px'}\n",
    "                )\n",
    "                outcome_lis.append(\n",
    "                    html.Li([state_text, f\" ({outcome['total_count']} occurrences across {outcome['distinct_patients']} patients)\"])\n",
    "                )\n",
    "            \n",
    "            action_panels.append(\n",
    "                html.Div([\n",
    "                    html.H4([action_text, f\" ({act['count']} occurrences across {act['patient_count']} patients)\"]),\n",
    "                    html.Ul(outcome_lis)\n",
    "                ], style={'border': '1px solid #ddd', 'padding': '10px', 'margin': '5px'})\n",
    "            )\n",
    "        details.append(html.Div(action_panels))\n",
    "    else:\n",
    "        details.append(html.P(\"No available action transitions for this state combination.\"))\n",
    "    \n",
    "    return html.Div(details)\n",
    "\n",
    "app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotly with Action (change color align as trajectory demo cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Patient Trajectory Visualization\"),\n",
    "    dcc.Dropdown(\n",
    "        id='stay-id-dropdown',\n",
    "        options=[{'label': str(sid), 'value': sid} for sid in sorted(transition_history_df['stay_id'].unique())],\n",
    "        value=sorted(transition_history_df['stay_id'].unique())[0],\n",
    "        clearable=False\n",
    "    ),\n",
    "    # Added RadioItems for baseline toggle\n",
    "    dcc.RadioItems(\n",
    "        id='consider-baseline',\n",
    "        options=[\n",
    "            {'label': 'Consider Baseline (Gender, Age)', 'value': 'yes'},\n",
    "            {'label': 'Ignore Baseline', 'value': 'no'}\n",
    "        ],\n",
    "        value='no',  # Default to ignoring baseline\n",
    "        labelStyle={'display': 'inline-block', 'margin-right': '20px'}\n",
    "    ),\n",
    "    dcc.Graph(id='trajectory-plot'),\n",
    "    html.Div(\n",
    "        id='action-details',\n",
    "        style={\n",
    "            'margin-top': '20px',\n",
    "            'backgroundColor': 'white',\n",
    "            'padding': '15px',\n",
    "            'border': '1px solid #ccc',\n",
    "            'borderRadius': '5px',\n",
    "            'boxShadow': '0 2px 5px rgba(0,0,0,0.1)',\n",
    "            'fontFamily': 'Arial, sans-serif',\n",
    "            'color': '#333'\n",
    "        }\n",
    "    )\n",
    "], style={'backgroundColor': 'white', 'padding': '20px'})\n",
    "\n",
    "# Helper function to map discrete bin values to their original ranges\n",
    "def map_bin_to_range(bin_index, feature, bin_edges_dict):\n",
    "    \"\"\"\n",
    "    Map a discrete bin index to its original range and return the mean of the range.\n",
    "    Args:\n",
    "        bin_index (int): The bin index.\n",
    "        feature (str): The feature name.\n",
    "        bin_edges_dict (dict): Dictionary containing bin edges for each feature.\n",
    "    Returns:\n",
    "        int: The mean of the range, rounded for better visualization.\n",
    "    \"\"\"\n",
    "    if feature not in bin_edges_dict:\n",
    "        return \"N/A\"\n",
    "    edges = bin_edges_dict[feature]\n",
    "    if bin_index < 0 or bin_index >= len(edges) - 1:\n",
    "        return \"Invalid\"\n",
    "    lower = edges[bin_index]\n",
    "    upper = edges[bin_index + 1]\n",
    "    if feature == \"fio2\":\n",
    "        # round it to the nearest 5 or 10\n",
    "        # return int(round(round((lower + upper) / 2) / 5) * 5)\n",
    "        return int(round(lower))\n",
    "    else:\n",
    "        return int(round((lower + upper) / 2))\n",
    "\n",
    "# Updated update_plot function (only showing the modified part)\n",
    "@app.callback(\n",
    "    Output('trajectory-plot', 'figure'),\n",
    "    Input('stay-id-dropdown', 'value')\n",
    ")\n",
    "def update_plot(selected_stay_id):\n",
    "    patient_df = transition_history_df[transition_history_df['stay_id'] == selected_stay_id].copy()\n",
    "    patient_df['within_guidelines'] = patient_df.apply(\n",
    "        lambda row: is_within_guidelines(row, state_cols, action_cols, min_max_values_guidelines_bin_dict),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Combine state and action columns for plotting\n",
    "    combined_cols = state_cols + action_cols\n",
    "    patient_df_melted = patient_df.melt(\n",
    "        id_vars=['hours_in', 'within_guidelines'],\n",
    "        value_vars=combined_cols,\n",
    "        var_name='variable',\n",
    "        value_name='value'\n",
    "    )\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'] # change colors as trajectory demo cases\n",
    "\n",
    "    # Create the line plot\n",
    "    fig = px.line(\n",
    "        patient_df_melted,\n",
    "        x='hours_in',\n",
    "        y='value',\n",
    "        color='variable',\n",
    "        title=f'Trajectory for stay_id {selected_stay_id}',\n",
    "        labels={'value': 'Value', 'variable': 'Feature', 'hours_in': 'Time (hours)'},\n",
    "        color_discrete_sequence=colors # change colors as trajectory demo cases\n",
    "    )\n",
    "    fig.update_traces(mode='lines+markers')\n",
    "    \n",
    "    # Updated hover text with ranges\n",
    "    hover_texts = []\n",
    "    for _, row in patient_df.iterrows():\n",
    "        state_ranges = [get_range_str(feat, bin_idx) for feat, bin_idx in zip(state_cols, row['S_t'])]\n",
    "        action_ranges = [get_range_str(feat, bin_idx) for feat, bin_idx in zip(action_cols, row['A_t'])]\n",
    "        hover_text = (\n",
    "            f\"<b>Hour:</b> {row['hours_in']}<br>\"\n",
    "            f\"<b>Current State:</b> {format_tuple(row['S_t'])}<br>\"\n",
    "            + \"<br>\".join(state_ranges) + \"<br>\"\n",
    "            f\"<b>Current Action:</b> {format_tuple(row['A_t'])}<br>\"\n",
    "            + \"<br>\".join(action_ranges)\n",
    "        )\n",
    "        hover_texts.append(hover_text)\n",
    "    \n",
    "    for trace in fig.data:\n",
    "        trace.customdata = patient_df[['stay_id', 'S_hist', 'A_hist', 'S_t', 'A_t', 'hours_in', 'baseline']].to_dict('records')\n",
    "        trace.text = hover_texts\n",
    "        trace.hovertemplate = '%{text}<extra></extra>'\n",
    "    \n",
    "    # Add background shapes to indicate guideline adherence\n",
    "    shapes = []\n",
    "    for idx, row in patient_df.iterrows():\n",
    "        t = row['hours_in']\n",
    "        color = 'rgba(144, 238, 144, 0.3)' if row['within_guidelines'] else 'rgba(255, 99, 71, 0.3)'\n",
    "        shapes.append(dict(\n",
    "            type=\"rect\",\n",
    "            x0=t - 0.5,\n",
    "            y0=0,\n",
    "            x1=t + 0.5,\n",
    "            y1=1,\n",
    "            xref=\"x\",\n",
    "            yref=\"paper\",\n",
    "            fillcolor=color,\n",
    "            line_width=0,\n",
    "            layer=\"below\"\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        shapes=shapes,\n",
    "        clickmode='event+select',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "@app.callback(\n",
    "    Output('action-details', 'children'),\n",
    "    [Input('trajectory-plot', 'clickData'),\n",
    "     Input('consider-baseline', 'value')]\n",
    ")\n",
    "def display_action_details(clickData, consider_baseline_value):\n",
    "    if not clickData:\n",
    "        return html.P(\"Click a point on the plot to see detailed transition outcomes.\")\n",
    "    \n",
    "    point = clickData['points'][0]\n",
    "    data = point['customdata']\n",
    "    history_state = tuple(data['S_hist'])\n",
    "    history_action = tuple(data['A_hist'])\n",
    "    current_state = tuple(data['S_t'])\n",
    "    current_action = tuple(data['A_t'])\n",
    "    \n",
    "    # Map state and action bins to their original ranges and add units\n",
    "    mapped_current_state = [\n",
    "        f\"{map_bin_to_range(bin_idx, feat, bin_edges_dict)} {unit}\"\n",
    "        for feat, bin_idx, unit in zip(state_cols, current_state, state_units)\n",
    "    ]\n",
    "    mapped_current_action = [\n",
    "        f\"{map_bin_to_range(bin_idx, feat, bin_edges_dict)} {unit}\"\n",
    "        for feat, bin_idx, unit in zip(action_cols, current_action, action_units)\n",
    "    ]\n",
    "    \n",
    "    # Determine whether to consider baseline\n",
    "    consider_baseline = (consider_baseline_value == 'yes')\n",
    "    baseline = tuple(data['baseline']) if consider_baseline else None\n",
    "    \n",
    "    # Build details with baseline consideration status\n",
    "    details = [\n",
    "        html.H3(f\"Details for stay_id {data['stay_id']} at Hour {data['hours_in']}\"),\n",
    "        html.P(f\"Considering baseline: {'Yes' if consider_baseline else 'No'}\"),\n",
    "        html.P(f\"Current State: State ({', '.join(mapped_current_state)})\"),\n",
    "        html.P(f\"Current Action: Action ({', '.join(mapped_current_action)})\")\n",
    "    ]\n",
    "    \n",
    "    # Get possible actions and outcomes with baseline parameter\n",
    "    possible_actions = get_possible_actions(history_state, history_action, current_state, baseline)\n",
    "    \n",
    "    # Sort possible_actions by 'count' in descending order\n",
    "    possible_actions = sorted(possible_actions, key=lambda x: x['count'], reverse=True)\n",
    "    \n",
    "    if possible_actions:\n",
    "        action_panels = []\n",
    "        for act in possible_actions:\n",
    "            action_tuple = act['action']\n",
    "            mapped_action = [\n",
    "                f\"{map_bin_to_range(bin_idx, feat, bin_edges_dict)} {unit}\"\n",
    "                for feat, bin_idx, unit in zip(action_cols, action_tuple, action_units)\n",
    "            ]\n",
    "            count_not_satisfy = count_not_within_guidelines(action_tuple, action_cols, min_max_values_guidelines_bin_dict)\n",
    "            bg_color = 'lightgreen' if count_not_satisfy == 0 else 'lightsalmon'\n",
    "            action_text = html.Span(\n",
    "                f\"Action ({', '.join(mapped_action)})\",\n",
    "                style={'backgroundColor': bg_color, 'padding': '2px 5px', 'borderRadius': '3px'}\n",
    "            )\n",
    "            \n",
    "            outcomes = get_action_outcomes(history_state, history_action, current_state, action_tuple, baseline)\n",
    "            outcome_lis = []\n",
    "            for outcome in outcomes:\n",
    "                state_tuple = outcome['S_t+1']\n",
    "                mapped_state = [\n",
    "                    f\"{map_bin_to_range(bin_idx, feat, bin_edges_dict)} {unit}\"\n",
    "                    for feat, bin_idx, unit in zip(state_cols, state_tuple, state_units)\n",
    "                ]\n",
    "                count_not_satisfy = count_not_within_guidelines(state_tuple, state_cols, min_max_values_guidelines_bin_dict)\n",
    "                bg_color = 'lightgreen' if count_not_satisfy == 0 else 'orange'\n",
    "                state_text = html.Span(\n",
    "                    f\"State ({', '.join(mapped_state)})\",\n",
    "                    style={'backgroundColor': bg_color, 'padding': '2px 5px', 'borderRadius': '3px'}\n",
    "                )\n",
    "                outcome_lis.append(\n",
    "                    html.Li([state_text, f\" ({outcome['total_count']} occurrences across {outcome['distinct_patients']} patients)\"])\n",
    "                )\n",
    "            \n",
    "            action_panels.append(\n",
    "                html.Div([\n",
    "                    html.H4([action_text, f\" ({act['count']} occurrences across {act['patient_count']} patients)\"]),\n",
    "                    html.Ul(outcome_lis)\n",
    "                ], style={'border': '1px solid #ddd', 'padding': '10px', 'margin': '5px'})\n",
    "            )\n",
    "        details.append(html.Div(action_panels))\n",
    "    else:\n",
    "        details.append(html.P(\"No available action transitions for this state combination.\"))\n",
    "    \n",
    "    return html.Div(details)\n",
    "\n",
    "app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition_history_df = create_transition_history(pd.read_csv(f'../models/training_log_backup/eval_trajectories_DiscreteSAC_train_epoch0.csv'), obs_hrs-1, state_cols, action_cols, baseline_cols)\n",
    "transition_history_df = create_transition_history(pd.read_csv(f'../models/training_log_backup/eval_trajectories_DQN_train_epoch0.csv'), obs_hrs-1, state_cols, action_cols, baseline_cols)\n",
    "# transition_history_df = create_transition_history(pd.read_csv(f'../models/training_log_backup/eval_trajectories_DoubleDQN_train_epoch0.csv'), obs_hrs-1, state_cols, action_cols, baseline_cols)\n",
    "# transition_history_df = create_transition_history(pd.read_csv(f'../models/training_log_backup/eval_trajectories_DiscreteSAC_train_epoch0.csv'), obs_hrs-1, state_cols, action_cols, baseline_cols)\n",
    "# transition_history_df = create_transition_history(pd.read_csv(f'../models/training_log_backup/eval_trajectories_DiscreteBCQ_train_epoch9.csv'), obs_hrs-1, state_cols, action_cols, baseline_cols)\n",
    "# transition_history_df = create_transition_history(pd.read_csv(f'../models/training_log_backup/eval_trajectories_DiscreteCQL_train_epoch9.csv'), obs_hrs-1, state_cols, action_cols, baseline_cols)\n",
    "# transition_history_df = create_transition_history(pd.read_csv(f'../models/training_log_backup/eval_trajectories_DiscreteBC_train_epoch0.csv'), obs_hrs-1, state_cols, action_cols, baseline_cols)\n",
    "# transition_history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges_dict_s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_values_guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize from hours 0, without click for possible action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_list = [\"DiscreteBC\", \"NFQ\", \"DQN\", \"DoubleDQN\", \"DiscreteSAC\", \"DiscreteBCQ\", \"DiscreteCQL\"]\n",
    "data_source_list = [\"train\", \"test\", \"eICU\"]\n",
    "data_source = \"test\"\n",
    "epoch = 1\n",
    "gen_stay_id_num = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate different algorithms' transition history dataframes, and also update stay_id by concate f\"_{algo}\" at the end of original stay_id\n",
    "merged_rl_algo_df = pd.DataFrame()\n",
    "for algo in algo_list:\n",
    "    rl_algo_df = pd.read_csv(f'../models/training_log/{EXP_FOLDER_PREFIX}/eval_trajectories_{algo}_{data_source}_epoch{epoch}.csv')\n",
    "    rl_algo_df['stay_id'] = rl_algo_df['stay_id'].astype(str) + f'_{algo}'\n",
    "    merged_rl_algo_df = pd.concat([\n",
    "        merged_rl_algo_df,\n",
    "        rl_algo_df\n",
    "    ])\n",
    "merged_rl_algo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate different algorithms' transition history dataframes, and also update stay_id by concate f\"_{algo}\" at the end of original stay_id\n",
    "# for all epoch merge into one dataframe, with given same init e.g., f\"{epoch}_small_reward_66\"\n",
    "merged_rl_algo_df = pd.DataFrame()\n",
    "for epoch in range(20):\n",
    "    for algo in algo_list:\n",
    "        rl_algo_df = pd.read_csv(f'../models/training_log/{EXP_FOLDER_PREFIX}/eval_trajectories_{algo}_{data_source}_epoch{epoch}.csv')\n",
    "        # \"stay_id\" == f\"{epoch}_small_reward_66\"\n",
    "        rl_algo_df = rl_algo_df[rl_algo_df[\"stay_id\"] == f\"{epoch}_small_reward_{gen_stay_id_num}\"]\n",
    "        rl_algo_df['stay_id'] = rl_algo_df['stay_id'].astype(str) + f'_{algo}'\n",
    "        merged_rl_algo_df = pd.concat([\n",
    "            merged_rl_algo_df,\n",
    "            rl_algo_df\n",
    "        ])\n",
    "merged_rl_algo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_rl_algo_with_physician_policy_df\n",
    "# algo = \"naive_agent\"\n",
    "# physician_policy_df = pd.read_csv(f'../models/training_log/naive_agent/eval_trajectories_{algo}_{data_source}.csv')\n",
    "# physician_policy_df[\"stay_id\"] = f\"{epoch}_\" + physician_policy_df[\"stay_id\"].astype(str) + \"_physician_policy\"\n",
    "# physician_policy_df\n",
    "\n",
    "# merged_rl_algo_with_physician_policy_df\n",
    "algo = \"naive_agent\"\n",
    "physician_policy_df = pd.read_csv(f'../models/training_log/naive_agent/eval_trajectories_{algo}_{data_source}.csv')\n",
    "physician_policy_df = physician_policy_df[physician_policy_df[\"stay_id\"] == f\"small_reward_{gen_stay_id_num}\"]\n",
    "physician_policy_df[\"stay_id\"] = f\"{epoch}_\" + physician_policy_df[\"stay_id\"].astype(str) + \"_physician_policy\"\n",
    "physician_policy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rl_algo_with_physician_policy_df = pd.concat([\n",
    "    merged_rl_algo_df,\n",
    "    physician_policy_df\n",
    "])\n",
    "merged_rl_algo_with_physician_policy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Trajectory compared train / test / eICU, with merged_rl_algo_with_physician_policy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc, html, Input, Output\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def is_within_guidelines_updated(current_state, current_action, previous_state, previous_action, \n",
    "                                state_cols, action_cols, min_max_values_guidelines_bin_dict, stable_threshold=2):\n",
    "    \"\"\"\n",
    "    Updated guideline check based on weaning criteria from Part 2.\n",
    "    Returns True if all conditions for weaning_count += 1 are met:\n",
    "    - state_penalty == 0 (all states within guidelines)\n",
    "    - action_penalty == 0 (all actions within guidelines) \n",
    "    - state_stable_penalty == 0 (state changes below threshold)\n",
    "    - action_stable_penalty == 0 (action changes below threshold)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check state guidelines (state_penalty == 0)\n",
    "    state_within_guidelines = all(\n",
    "        min_max_values_guidelines_bin_dict[col]['min'] <= current_state[i] <= min_max_values_guidelines_bin_dict[col]['max']\n",
    "        for i, col in enumerate(state_cols)\n",
    "        if col in min_max_values_guidelines_bin_dict\n",
    "    )\n",
    "    \n",
    "    # Check action guidelines (action_penalty == 0)\n",
    "    action_within_guidelines = all(\n",
    "        min_max_values_guidelines_bin_dict[col]['min'] <= current_action[i] <= min_max_values_guidelines_bin_dict[col]['max']\n",
    "        for i, col in enumerate(action_cols)\n",
    "        if col in min_max_values_guidelines_bin_dict\n",
    "    )\n",
    "    \n",
    "    # For the first timestep, we don't have previous state/action to compare\n",
    "    # so we only check state and action guidelines\n",
    "    if previous_action is None or previous_state is None:\n",
    "        return state_within_guidelines and action_within_guidelines\n",
    "    \n",
    "    # Check state stability (state_stable_penalty == 0)\n",
    "    state_stable = all(\n",
    "        abs(previous_state[i] - current_state[i]) < stable_threshold\n",
    "        for i in range(len(state_cols))\n",
    "    )\n",
    "    \n",
    "    # Check action stability (action_stable_penalty == 0)\n",
    "    action_stable = all(\n",
    "        abs(previous_action[i] - current_action[i]) < stable_threshold * 2\n",
    "        for i in range(len(action_cols))\n",
    "    )\n",
    "    \n",
    "    return state_within_guidelines and action_within_guidelines and state_stable and action_stable\n",
    "\n",
    "def prepare_trajectory_data(df, state_cols, action_cols, min_max_values_guidelines_bin_dict, \n",
    "                           obs_hrs=3, stable_threshold=2):\n",
    "    \"\"\"\n",
    "    Prepare trajectory data with guideline compliance from hour 0.\n",
    "    \"\"\"\n",
    "    trajectory_data = []\n",
    "    \n",
    "    for stay_id, group in df.groupby('stay_id'):\n",
    "        group = group.sort_values('hours_in').reset_index(drop=True)\n",
    "        \n",
    "        # Start from hour 0 instead of obs_hrs-1\n",
    "        for i in range(len(group)):\n",
    "            row = group.iloc[i]\n",
    "            \n",
    "            # Get current state and action\n",
    "            current_state = tuple(row[col] for col in state_cols)\n",
    "            current_action = tuple(row[col] for col in action_cols)\n",
    "            \n",
    "            # Get previous state and action if available\n",
    "            previous_state = None\n",
    "            previous_action = None\n",
    "            if i > 0:\n",
    "                prev_row = group.iloc[i-1]\n",
    "                previous_state = tuple(prev_row[col] for col in state_cols)\n",
    "                previous_action = tuple(prev_row[col] for col in action_cols)\n",
    "            \n",
    "            # Check if within guidelines using updated criteria\n",
    "            within_guidelines = is_within_guidelines_updated(\n",
    "                current_state, current_action, previous_state, previous_action,\n",
    "                state_cols, action_cols, min_max_values_guidelines_bin_dict, stable_threshold\n",
    "            )\n",
    "            \n",
    "            # Add to trajectory data\n",
    "            traj_point = {\n",
    "                'stay_id': stay_id,\n",
    "                'hours_in': row['hours_in'],\n",
    "                'within_guidelines': within_guidelines\n",
    "            }\n",
    "            \n",
    "            # Add state and action values\n",
    "            for j, col in enumerate(state_cols):\n",
    "                traj_point[col] = current_state[j]\n",
    "            for j, col in enumerate(action_cols):\n",
    "                traj_point[col] = current_action[j]\n",
    "                \n",
    "            trajectory_data.append(traj_point)\n",
    "    \n",
    "    return pd.DataFrame(trajectory_data)\n",
    "\n",
    "def map_bin_to_range(bin_index, feature, bin_edges_dict):\n",
    "    \"\"\"\n",
    "    Map a discrete bin index to its original range and return the mean of the range.\n",
    "    \"\"\"\n",
    "    if feature not in bin_edges_dict:\n",
    "        return \"N/A\"\n",
    "    edges = bin_edges_dict[feature]\n",
    "    if bin_index < 0 or bin_index >= len(edges) - 1:\n",
    "        return \"Invalid\"\n",
    "    lower = edges[bin_index]\n",
    "    upper = edges[bin_index + 1]\n",
    "    if feature == \"fio2\":\n",
    "        return int(round(lower))\n",
    "    else:\n",
    "        return int(round((lower + upper) / 2))\n",
    "\n",
    "def create_trajectory_app(train_df, test_df, eICU_disc, rl_algo_df, \n",
    "                         state_cols, action_cols, state_units, action_units,\n",
    "                         min_max_values_guidelines_bin_dict, bin_edges_dict):\n",
    "    \"\"\"\n",
    "    Create Dash app for trajectory visualization.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare data for all datasets\n",
    "    datasets = {\n",
    "        'Train': prepare_trajectory_data(train_df, state_cols, action_cols, min_max_values_guidelines_bin_dict),\n",
    "        'Test': prepare_trajectory_data(test_df, state_cols, action_cols, min_max_values_guidelines_bin_dict),\n",
    "        'eICU': prepare_trajectory_data(eICU_disc, state_cols, action_cols, min_max_values_guidelines_bin_dict),\n",
    "        'RL_Algo': prepare_trajectory_data(rl_algo_df, state_cols, action_cols, min_max_values_guidelines_bin_dict)\n",
    "    }\n",
    "    \n",
    "    # Combine all stay_ids for dropdown\n",
    "    all_stay_ids = []\n",
    "    for dataset_name, df in datasets.items():\n",
    "        for stay_id in df['stay_id'].unique():\n",
    "            all_stay_ids.append(f\"{dataset_name}_{stay_id}\")\n",
    "    \n",
    "    app = dash.Dash(__name__)\n",
    "    \n",
    "    app.layout = html.Div([\n",
    "        html.H1(\"Patient Trajectory Visualization - Multi-Dataset Comparison\"),\n",
    "        \n",
    "        html.Div([\n",
    "            html.Label(\"Select Dataset:\"),\n",
    "            dcc.Dropdown(\n",
    "                id='dataset-dropdown',\n",
    "                options=[\n",
    "                    {'label': 'Train Dataset', 'value': 'Train'},\n",
    "                    {'label': 'Test Dataset', 'value': 'Test'},\n",
    "                    {'label': 'eICU Dataset', 'value': 'eICU'},\n",
    "                    {'label': 'RL Algorithm Results', 'value': 'RL_Algo'}\n",
    "                ],\n",
    "                value='Train',\n",
    "                clearable=False\n",
    "            )\n",
    "        ], style={'width': '48%', 'display': 'inline-block'}),\n",
    "        \n",
    "        html.Div([\n",
    "            html.Label(\"Select Stay ID:\"),\n",
    "            dcc.Dropdown(\n",
    "                id='stay-id-dropdown',\n",
    "                clearable=False\n",
    "            )\n",
    "        ], style={'width': '48%', 'float': 'right', 'display': 'inline-block'}),\n",
    "        \n",
    "        dcc.Graph(id='trajectory-plot'),\n",
    "        \n",
    "        html.Div(id='trajectory-summary', style={\n",
    "            'margin-top': '20px',\n",
    "            'backgroundColor': 'white',\n",
    "            'padding': '15px',\n",
    "            'border': '1px solid #ccc',\n",
    "            'borderRadius': '5px',\n",
    "            'boxShadow': '0 2px 5px rgba(0,0,0,0.1)',\n",
    "            'fontFamily': 'Arial, sans-serif',\n",
    "            'color': '#333'\n",
    "        })\n",
    "        \n",
    "    ], style={'backgroundColor': 'white', 'padding': '20px'})\n",
    "    \n",
    "    @app.callback(\n",
    "        Output('stay-id-dropdown', 'options'),\n",
    "        Output('stay-id-dropdown', 'value'),\n",
    "        Input('dataset-dropdown', 'value')\n",
    "    )\n",
    "    def update_stay_id_options(selected_dataset):\n",
    "        if selected_dataset in datasets:\n",
    "            stay_ids = sorted(datasets[selected_dataset]['stay_id'].unique())\n",
    "            options = [{'label': str(sid), 'value': sid} for sid in stay_ids]\n",
    "            value = stay_ids[0] if stay_ids else None\n",
    "            return options, value\n",
    "        return [], None\n",
    "    \n",
    "    @app.callback(\n",
    "        Output('trajectory-plot', 'figure'),\n",
    "        [Input('dataset-dropdown', 'value'),\n",
    "         Input('stay-id-dropdown', 'value')]\n",
    "    )\n",
    "    def update_plot(selected_dataset, selected_stay_id):\n",
    "        if not selected_dataset or not selected_stay_id or selected_dataset not in datasets:\n",
    "            return go.Figure()\n",
    "        \n",
    "        patient_df = datasets[selected_dataset][\n",
    "            datasets[selected_dataset]['stay_id'] == selected_stay_id\n",
    "        ].copy()\n",
    "        \n",
    "        if patient_df.empty:\n",
    "            return go.Figure()\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        combined_cols = state_cols + action_cols\n",
    "        \n",
    "        # Melt the dataframe for line plotting\n",
    "        patient_df_melted = patient_df.melt(\n",
    "            id_vars=['hours_in', 'within_guidelines', 'stay_id'],\n",
    "            value_vars=combined_cols,\n",
    "            var_name='variable',\n",
    "            value_name='value'\n",
    "        )\n",
    "        \n",
    "        # Create color mapping\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
    "        \n",
    "        # Create the line plot\n",
    "        fig = px.line(\n",
    "            patient_df_melted,\n",
    "            x='hours_in',\n",
    "            y='value',\n",
    "            color='variable',\n",
    "            title=f'Trajectory for {selected_dataset} - Stay ID: {selected_stay_id}',\n",
    "            labels={'value': 'Bin Index', 'variable': 'Feature', 'hours_in': 'Time (hours)'},\n",
    "            color_discrete_sequence=colors\n",
    "        )\n",
    "        \n",
    "        fig.update_traces(mode='lines+markers')\n",
    "        \n",
    "        # Add background shapes to indicate guideline adherence\n",
    "        shapes = []\n",
    "        for _, row in patient_df.iterrows():\n",
    "            t = row['hours_in']\n",
    "            color = 'rgba(144, 238, 144, 0.3)' if row['within_guidelines'] else 'rgba(255, 99, 71, 0.3)'\n",
    "            shapes.append(dict(\n",
    "                type=\"rect\",\n",
    "                x0=t - 0.5,\n",
    "                y0=0,\n",
    "                x1=t + 0.5,\n",
    "                y1=1,\n",
    "                xref=\"x\",\n",
    "                yref=\"paper\",\n",
    "                fillcolor=color,\n",
    "                line_width=0,\n",
    "                layer=\"below\"\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            shapes=shapes,\n",
    "            template='plotly_white',\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    @app.callback(\n",
    "        Output('trajectory-summary', 'children'),\n",
    "        [Input('dataset-dropdown', 'value'),\n",
    "         Input('stay-id-dropdown', 'value')]\n",
    "    )\n",
    "    def update_summary(selected_dataset, selected_stay_id):\n",
    "        if not selected_dataset or not selected_stay_id or selected_dataset not in datasets:\n",
    "            return html.P(\"Select a dataset and stay ID to see trajectory summary.\")\n",
    "        \n",
    "        patient_df = datasets[selected_dataset][\n",
    "            datasets[selected_dataset]['stay_id'] == selected_stay_id\n",
    "        ].copy()\n",
    "        \n",
    "        if patient_df.empty:\n",
    "            return html.P(\"No data available for selected stay ID.\")\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        total_hours = len(patient_df)\n",
    "        hours_within_guidelines = patient_df['within_guidelines'].sum()\n",
    "        percentage_compliant = (hours_within_guidelines / total_hours) * 100\n",
    "        \n",
    "        # Count consecutive weaning periods\n",
    "        weaning_sequences = []\n",
    "        current_sequence = 0\n",
    "        for compliant in patient_df['within_guidelines']:\n",
    "            if compliant:\n",
    "                current_sequence += 1\n",
    "            else:\n",
    "                if current_sequence > 0:\n",
    "                    weaning_sequences.append(current_sequence)\n",
    "                current_sequence = 0\n",
    "        if current_sequence > 0:\n",
    "            weaning_sequences.append(current_sequence)\n",
    "        \n",
    "        max_consecutive_weaning = max(weaning_sequences) if weaning_sequences else 0\n",
    "        weaning_achieved = max_consecutive_weaning >= 6\n",
    "        \n",
    "        # Get final state and action values with units\n",
    "        final_row = patient_df.iloc[-1]\n",
    "        final_state_display = []\n",
    "        final_action_display = []\n",
    "        \n",
    "        for i, col in enumerate(state_cols):\n",
    "            mapped_value = map_bin_to_range(final_row[col], col, bin_edges_dict)\n",
    "            unit = state_units[i] if i < len(state_units) else \"\"\n",
    "            final_state_display.append(f\"{col}: {mapped_value} {unit}\")\n",
    "        \n",
    "        for i, col in enumerate(action_cols):\n",
    "            mapped_value = map_bin_to_range(final_row[col], col, bin_edges_dict)\n",
    "            unit = action_units[i] if i < len(action_units) else \"\"\n",
    "            final_action_display.append(f\"{col}: {mapped_value} {unit}\")\n",
    "        \n",
    "        summary = [\n",
    "            html.H3(f\"Trajectory Summary - {selected_dataset} Dataset\"),\n",
    "            html.P(f\"Stay ID: {selected_stay_id}\"),\n",
    "            html.P(f\"Total Duration: {total_hours} hours\"),\n",
    "            html.P(f\"Hours Within Guidelines: {hours_within_guidelines}/{total_hours} ({percentage_compliant:.1f}%)\"),\n",
    "            html.P(f\"Maximum Consecutive Weaning Hours: {max_consecutive_weaning}\"),\n",
    "            html.P(f\"Weaning Achieved (6 consecutive hours): {'Yes' if weaning_achieved else 'No'}\",\n",
    "                  style={'color': 'green' if weaning_achieved else 'red', 'fontWeight': 'bold'}),\n",
    "            html.Hr(),\n",
    "            html.H4(\"Final State:\"),\n",
    "            html.Ul([html.Li(item) for item in final_state_display]),\n",
    "            html.H4(\"Final Action:\"),\n",
    "            html.Ul([html.Li(item) for item in final_action_display])\n",
    "        ]\n",
    "        \n",
    "        return html.Div(summary)\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Example usage:\n",
    "def run_trajectory_visualization(train_df, test_df, eICU_disc, rl_algo_df,\n",
    "                               state_cols, action_cols, state_units, action_units,\n",
    "                               min_max_values_guidelines_bin_dict, bin_edges_dict):\n",
    "    \"\"\"\n",
    "    Run the trajectory visualization app.\n",
    "    \"\"\"\n",
    "    app = create_trajectory_app(\n",
    "        train_df, test_df, eICU_disc, rl_algo_df,\n",
    "        state_cols, action_cols, state_units, action_units,\n",
    "        min_max_values_guidelines_bin_dict, bin_edges_dict\n",
    "    )\n",
    "    \n",
    "    app.run(debug=True)\n",
    "\n",
    "# To use this, you would call:\n",
    "run_trajectory_visualization(train_df[train_df[\"stay_id\"] == 37768645], test_df[:1000], eICU_disc[:1000], merged_rl_algo_with_physician_policy_df,\n",
    "                           state_cols, action_cols, state_units, action_units,\n",
    "                           min_max_values_guidelines_bin_dict, bin_edges_dict) # 37768645 (small_reward_66), 31726457 (small_reward_88)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ventilation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
